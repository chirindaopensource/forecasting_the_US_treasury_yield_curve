{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DD0JtreYkD-x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`README.md`**\n",
        "\n",
        "# Forecasting the U.S. Treasury Yield Curve: A Distributionally Robust Machine Learning Approach\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2601.04608-b31b1b.svg)](https://arxiv.org/abs/2601.04608)\n",
        "[![Journal](https://img.shields.io/badge/Journal-ArXiv%20Preprint-003366)](https://arxiv.org/abs/2601.04608)\n",
        "[![Year](https://img.shields.io/badge/Year-2026-purple)](https://github.com/chirindaopensource/forecasting_the_US_treasury_yield_curve)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Computational%20Finance%20%7C%20Operations%20Research-00529B)](https://github.com/chirindaopensource/forecasting_the_US_treasury_yield_curve)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-LSEG%20Reuters%20Workspace-lightgrey)](https://www.lseg.com/en/data-analytics)\n",
        "[![Core Method](https://img.shields.io/badge/Method-Distributionally%20Robust%20Optimization-orange)](https://github.com/chirindaopensource/forecasting_the_US_treasury_yield_curve)\n",
        "[![Analysis](https://img.shields.io/badge/Analysis-Factor--Augmented%20Dynamic%20Nelson--Siegel-red)](https://github.com/chirindaopensource/forecasting_the_US_treasury_yield_curve)\n",
        "[![Validation](https://img.shields.io/badge/Validation-Random%20Forest%20Ensemble-green)](https://github.com/chirindaopensource/forecasting_the_US_treasury_yield_curve)\n",
        "[![Robustness](https://img.shields.io/badge/Robustness-Adaptive%20Forecast%20Combination-yellow)](https://github.com/chirindaopensource/forecasting_the_US_treasury_yield_curve)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![Scikit-Learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=flat&logo=scikit-learn&logoColor=white)](https://scikit-learn.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%230C55A5.svg?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![Statsmodels](https://img.shields.io/badge/statsmodels-blue.svg)](https://www.statsmodels.org/)\n",
        "[![SHAP](https://img.shields.io/badge/SHAP-Interpretability-ff69b4)](https://shap.readthedocs.io/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/forecasting_the_US_treasury_yield_curve`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2026 paper entitled **\"Forecasting the U.S. Treasury Yield Curve: A Distributionally Robust Machine Learning Approach\"** by:\n",
        "\n",
        "*   **Jinjun Liu** (Hong Kong Baptist University)\n",
        "*   **Ming-Yen Cheng** (Hong Kong Baptist University)\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's findings. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from the ingestion and cleansing of zero-coupon yields and macroeconomic indicators to the rigorous estimation of Factor-Augmented Dynamic Nelson-Siegel (FADNS) models and high-dimensional Random Forests, culminating in distributionally robust forecast combinations that minimize worst-case expected loss under ambiguity.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: `run_main_study_pipeline_variant`](#key-callable-run_main_study_pipeline_variant)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the analytical framework presented in Liu and Cheng (2026). The core of this repository is the iPython Notebook `forecasting_the_US_treasury_yield_curve_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings. The pipeline recasts yield curve forecasting as an operations research problem, where the objective is to select a decision rule that minimizes worst-case expected loss over an ambiguity set of forecast error distributions.\n",
        "\n",
        "The paper addresses the challenge of forecasting U.S. Treasury yields in an environment of distributional uncertainty and structural instability. This codebase operationalizes the paper's framework, allowing users to:\n",
        "-   Rigorously validate and manage the entire experimental configuration via a single `config.yaml` file.\n",
        "-   Cleanse and normalize zero-coupon yields and high-dimensional macroeconomic panels, enforcing strict publication lags to prevent look-ahead bias.\n",
        "-   Estimate rolling-window Factor-Augmented Dynamic Nelson-Siegel (FADNS) models using principal components extracted from economic indicators.\n",
        "-   Train high-dimensional Random Forest models to capture nonlinear interactions among macro-financial drivers.\n",
        "-   Implement distributionally robust forecast combination schemes (DRO-ES, DRMV) that penalize downside tail risk and stabilize covariance estimation.\n",
        "-   Evaluate performance using rigorous metrics such as Root Mean Squared Forecast Error (RMSFE) across maturities and horizons.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods combine techniques from Financial Econometrics, Machine Learning, and Robust Optimization.\n",
        "\n",
        "**1. Factor-Augmented Dynamic Nelson-Siegel (FADNS):**\n",
        "The yield curve is modeled using the dynamic Nelson-Siegel framework, augmented with latent factors extracted from a large panel of macroeconomic variables via Principal Component Analysis (PCA).\n",
        "-   **Measurement Equation:** $y_t(\\tau) = L(\\tau) \\beta_t + \\varepsilon_t$\n",
        "-   **State Dynamics:** $X_{t+1}^{(k)} = c^{(k)} + \\Phi^{(k)} X_t^{(k)} + \\eta_t$, where $X_t^{(k)} = [\\beta_t, F_t^{(k)}]$.\n",
        "\n",
        "**2. Nonparametric Machine Learning (Random Forest):**\n",
        "To capture nonlinearities, Random Forest models are trained on a high-dimensional feature set including lagged macro indicators and lagged yields.\n",
        "-   **Forecasting:** $\\hat{y}_{t+h|t} = \\hat{g}_{h,\\tau}(W_t)$, where $W_t$ includes lagged predictors.\n",
        "-   **Optimization:** Hyperparameters are tuned via Randomized Cross-Validation respecting the time-series structure.\n",
        "\n",
        "**3. Distributionally Robust Optimization (DRO):**\n",
        "Forecast combination weights are optimized to minimize worst-case expected loss over an ambiguity set, rather than assuming a fixed error distribution.\n",
        "-   **DRO-ES:** Weights are exponentially reweighted based on Expected Shortfall (ES) loss: $w_k \\propto \\exp(\\eta \\text{ES}_\\alpha(e_k))$.\n",
        "-   **Regularized Mean-Variance:** Weights are derived from a ridge-regularized covariance matrix to handle estimation uncertainty.\n",
        "\n",
        "**4. Adaptive Forecast Combination (AFTER):**\n",
        "Weights are updated recursively based on past performance, allowing the ensemble to adapt quickly to regime shifts.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`forecasting_the_US_treasury_yield_curve_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Task Architecture:** The pipeline is decomposed into 40 distinct, modular tasks, each with its own orchestrator function.\n",
        "-   **Configuration-Driven Design:** All study parameters (rolling window sizes, lag structures, optimization penalties) are managed in an external `config.yaml` file.\n",
        "-   **Rigorous Data Validation:** A multi-stage validation process checks schema integrity, unit consistency, and temporal alignment.\n",
        "-   **Deterministic Execution:** Enforces reproducibility through seed control, deterministic sorting, and rigorous logging of all stochastic outputs.\n",
        "-   **Comprehensive Evaluation:** Computes RMSFE tables across 15 maturities and 5 horizons, along with robustness checks for benchmark yields and global markets.\n",
        "-   **Reproducible Artifacts:** Generates structured dictionaries, serializable outputs, and cryptographic manifests for every intermediate result.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Validation & Cleansing (Tasks 1-6):** Ingests raw yields and macro data, validates schemas, aligns indices, cleanses missing values, and interpolates quarterly series.\n",
        "2.  **Structural Break Detection (Tasks 7-8):** Applies CUSUM and PELT algorithms to identify regime shifts in the yield curve.\n",
        "3.  **DNS Modeling (Tasks 9-13):** Constructs Nelson-Siegel loadings, extracts latent factors via cross-sectional regression, estimates rolling VAR dynamics, and generates parametric forecasts.\n",
        "4.  **FADNS Modeling (Tasks 14-20):** Preprocesses macro data (stationarity, standardization), extracts PCA factors, estimates augmented VAR models, and selects the optimal factor dimension $k^*$.\n",
        "5.  **Random Forest Modeling (Tasks 21-25):** Constructs high-dimensional feature sets, normalizes data, trains RF models with randomized CV, and generates nonparametric forecasts.\n",
        "6.  **Forecast Combination (Tasks 26-28):** Builds forecast pools, computes combination weights using 14 different schemes (Classic, Var/Risk, DRO, AFTER), and evaluates combined performance.\n",
        "7.  **Robustness & Extensions (Tasks 30-35):** Performs robustness checks using benchmark coupon-bearing yields, TIC variable augmentation, and extends the analysis to global sovereign bond markets.\n",
        "8.  **Interpretability & Visualization (Tasks 36-39):** Computes SHAP values for model interpretation and prepares data for visualizing weight dynamics and error dynamics.\n",
        "9.  **Packaging (Task 40):** Aggregates all results into a final artifact bundle.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The notebook is structured as a logical pipeline with modular orchestrator functions for each of the 40 major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: `run_main_study_pipeline_variant`\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`run_main_study_pipeline_variant`:** This master orchestrator function runs the entire automated research pipeline from end-to-end. A single call to this function reproduces the entire computational portion of the project, managing data flow between cleansing, modeling, combination, and evaluation modules.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `scikit-learn`, `scipy`, `statsmodels`, `shap`, `ruptures`, `pyyaml`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/forecasting_the_US_treasury_yield_curve.git\n",
        "    cd forecasting_the_US_treasury_yield_curve\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy scikit-learn scipy statsmodels shap ruptures pyyaml\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires six primary DataFrames (mocked in the example usage):\n",
        "1.  **`df_us_yields_raw`**: U.S. zero-coupon equivalent yields (15 maturities).\n",
        "2.  **`df_us_macro_raw`**: U.S. macroeconomic indicators (111 variables).\n",
        "3.  **`df_us_benchmark_yields_raw`**: U.S. benchmark coupon-bearing yields (9 maturities).\n",
        "4.  **`df_us_tic_raw`**: Treasury International Capital (TIC) flow variables.\n",
        "5.  **`df_global_yields_raw`**: Global 10-year sovereign bond yields (7 countries).\n",
        "6.  **`global_macro_panels_raw`**: Dictionary of macroeconomic panels for global countries.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The notebook provides a complete, step-by-step guide. The primary workflow is to execute the final cell, which demonstrates how to use the top-level `run_main_study_pipeline_variant` orchestrator:\n",
        "\n",
        "```python\n",
        "# Final cell of the notebook\n",
        "\n",
        "# This block serves as the main entry point for the entire project.\n",
        "if __name__ == '__main__':\n",
        "    # 1. Load the master configuration from the YAML file.\n",
        "    # (Simulated in the notebook example)\n",
        "    raw_study_config = STUDY_CONFIG\n",
        "    \n",
        "    # 2. Load raw datasets (Example using synthetic generator provided in the notebook)\n",
        "    # In production, load from CSV/Parquet: pd.read_csv(...)\n",
        "    (\n",
        "        df_us_yields_raw,\n",
        "        df_us_macro_raw,\n",
        "        df_us_benchmark_yields_raw,\n",
        "        df_us_tic_raw,\n",
        "        df_global_yields_raw,\n",
        "        global_macro_panels_raw,\n",
        "        study_metadata\n",
        "    ) = generate_synthetic_data()\n",
        "\n",
        "    # 3. Execute the entire replication study.\n",
        "    artifacts = run_main_study_pipeline_variant(\n",
        "        df_us_yields_raw=df_us_yields_raw,\n",
        "        df_us_macro_raw=df_us_macro_raw,\n",
        "        df_us_benchmark_yields_raw=df_us_benchmark_yields_raw,\n",
        "        df_us_tic_raw=df_us_tic_raw,\n",
        "        df_global_yields_raw=df_global_yields_raw,\n",
        "        global_macro_panels_raw=global_macro_panels_raw,\n",
        "        raw_study_config=raw_study_config,\n",
        "        study_metadata=study_metadata\n",
        "    )\n",
        "    \n",
        "    # 4. Access results\n",
        "    tables = artifacts[\"Tables\"]\n",
        "    print(tables[\"Combination_RMSFE_H1\"].head())\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The pipeline returns a dictionary containing:\n",
        "-   **`Tables`**: A dictionary of result DataFrames (e.g., `DNS_RMSFE`, `FADNS_Best_k`, `Combination_RMSFE_H1`).\n",
        "-   **`Figures`**: A dictionary of DataFrames formatted for plotting (e.g., `Weight_Dynamics_Data`, `Global_SHAP_Data`).\n",
        "-   **`Audit`**: A dictionary containing the frozen configuration and its cryptographic hash.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "forecasting_the_US_treasury_yield_curve/\n",
        "│\n",
        "├── forecasting_the_US_treasury_yield_curve_draft.ipynb   # Main implementation notebook\n",
        "├── config.yaml                                           # Master configuration file\n",
        "├── requirements.txt                                      # Python package dependencies\n",
        "│\n",
        "├── LICENSE                                               # MIT Project License File\n",
        "└── README.md                                             # This file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can modify study parameters such as:\n",
        "-   **Global Settings:** `start_date_us`, `forecast_horizons`, `us_zero_maturities`.\n",
        "-   **Model Architectures:** `dns_fadns_window_w`, `rf_training_window_W`, `pca_k_grid`.\n",
        "-   **Forecast Combination:** `fc_weight_window_W`, `fc_min_observations`, `alpha` (ES level), `eta` (robustness).\n",
        "-   **Preprocessing:** `cusum_model_specification`, `rbf_kernel_setting`.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Alternative Machine Learning Models:** Integrating Gradient Boosting Machines (XGBoost, LightGBM) or Neural Networks (LSTM, Transformer) into the ensemble.\n",
        "-   **Dynamic Factor Selection:** Implementing time-varying factor selection for the FADNS model.\n",
        "-   **Real-Time Forecasting:** Connecting the pipeline to live data feeds for real-time yield curve prediction.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{liu2026forecasting,\n",
        "  title={Forecasting the U.S. Treasury Yield Curve: A Distributionally Robust Machine Learning Approach},\n",
        "  author={Liu, Jinjun and Cheng, Ming-Yen},\n",
        "  journal={arXiv preprint arXiv:2601.04608},\n",
        "  year={2026}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). Forecasting U.S. Treasury Yields Replication Pipeline: An Open Source Implementation.\n",
        "GitHub repository: https://github.com/chirindaopensource/forecasting_the_US_treasury_yield_curve\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Jinjun Liu and Ming-Yen Cheng** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, Scikit-Learn, SciPy, Statsmodels, SHAP, and Ruptures**.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of the `forecasting_the_US_treasury_yield_curve_draft.ipynb` notebook and follows best practices for research software documentation.*\n"
      ],
      "metadata": {
        "id": "3ESnxDIaT7L-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Forecasting the U.S. Treasury Yield Curve: A Distributionally Robust Machine Learning Approach*\"\n",
        "\n",
        "Authors: Jinjun Liu, Ming-Yen Cheng\n",
        "\n",
        "E-Journal Submission Date: 8 January 2026\n",
        "\n",
        "Link: https://arxiv.org/abs/2601.04608\n",
        "\n",
        "Journal Reference: The paper is preprint. The paper is currently under review.\n",
        "\n",
        "Abstract:\n",
        "\n",
        "We study U.S. Treasury yield curve forecasting under distributional uncertainty and recast forecasting as an operations research and managerial decision problem. Rather than minimizing average forecast error, the forecaster selects a decision rule that minimizes worst case expected loss over an ambiguity set of forecast error distributions. To this end, we propose a distributionally robust ensemble forecasting framework that integrates parametric factor models with high dimensional nonparametric machine learning models through adaptive forecast combinations. The framework consists of three machine learning components. First, a rolling window Factor Augmented Dynamic Nelson Siegel model captures level, slope, and curvature dynamics using principal components extracted from economic indicators. Second, Random Forest models capture nonlinear interactions among macro financial drivers and lagged Treasury yields. Third, distributionally robust forecast combination schemes aggregate heterogeneous forecasts under moment uncertainty, penalizing downside tail risk via expected shortfall and stabilizing second moment estimation through ridge regularized covariance matrices. The severity of the worst case criterion is adjustable, allowing the forecaster to regulate the trade off between robustness and statistical efficiency. Using monthly data, we evaluate out of sample forecasts across maturities and horizons from one to twelve months ahead. Adaptive combinations deliver superior performance at short horizons, while Random Forest forecasts dominate at longer horizons. Extensions to global sovereign bond yields confirm the stability and generalizability of the proposed framework.\n"
      ],
      "metadata": {
        "id": "g-E2jSFTkI05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### **Executive Summary**\n",
        "This research bridges the gap between **financial econometrics**, **machine learning (ML)**, and **operations research (OR)**. The authors propose a novel ensemble forecasting framework for the U.S. Treasury yield curve that addresses the inherent instability of financial data distributions during regime shifts (e.g., the COVID-19 pandemic, the 2022 monetary tightening).\n",
        "\n",
        "Departing from traditional \"point forecasting\" optimization, the paper recasts yield curve forecasting as a **min–max decision problem**. The core contribution is a **Distributionally Robust Optimization (DRO)** forecast combination scheme. This method selects weights to minimize worst-case expected loss over a set of admissible error distributions, explicitly penalizing downside tail risk (via Expected Shortfall) and stabilizing second-moment estimation.\n",
        "\n",
        "\n",
        "### **Data Engineering and Structural Instability**\n",
        "The study utilizes a high-dimensional dataset to capture the complex macro-finance interface.\n",
        "\n",
        "*   **Target Variable:** Zero-coupon U.S. Treasury yields (LSEG Reuters) for 15 maturities (3 months to 30 years).\n",
        "*   **Predictors:** A panel of **111 macroeconomic and financial indicators** (inflation, labor, real activity, etc.), standardized and lagged to reflect real-time availability.\n",
        "*   **Structural Breaks:** Using CUSUM tests and the Pruned Exact Linear Time (PELT) algorithm, the authors confirm significant structural instability in yield dynamics, identifying breakpoints aligning with the 2008 crisis, the 2011 sovereign debt crisis, COVID-19 (2020), and the 2022 tightening cycle. This instability motivates the need for the proposed robust framework.\n",
        "\n",
        "### **The Constituent Models (The Ensemble)**\n",
        "The framework integrates two distinct modeling philosophies—parametric factor models and non-parametric machine learning—to generate the pool of candidate forecasts.\n",
        "\n",
        "#### **A. Factor-Augmented Dynamic Nelson-Siegel (FADNS)**\n",
        "*   **Structure:** Extends the classic Diebold-Li (2006) framework. It models yields using three latent factors (Level, Slope, Curvature) augmented by principal components (PCs) extracted from the 111 economic indicators.\n",
        "*   **Dynamics:** The joint dynamics of yield factors and economic PCs are modeled via a VAR(1) process.\n",
        "*   **Implementation:** Rolling window estimation ($w=60$ months).\n",
        "*   **Performance:** Strong at very short horizons (1-month) but suffers from error accumulation in recursive multi-step forecasting at longer horizons (6-12 months).\n",
        "\n",
        "#### **B. Random Forest (RF)**\n",
        "*   **Structure:** A non-parametric ensemble of decision trees designed to capture nonlinear interactions between economic drivers and lagged yields without imposing an affine term structure.\n",
        "*   **Implementation:** High-dimensional input space ($d_W \\approx 6,600$ features due to lags). Uses recursive partitioning with data-driven regularization (cross-validated depth and leaf size).\n",
        "*   **Performance:** Exhibits superior stability and dominates FADNS at medium-to-long horizons (3–12 months). It does not suffer the same degradation as the parametric model.\n",
        "\n",
        "\n",
        "### **The Core Innovation – Distributionally Robust Forecast Combination**\n",
        "The paper argues that standard combination methods (e.g., OLS, simple averaging) fail during market stress because they rely on plug-in moment estimates that become unstable. The authors introduce **Distributionally Robust Optimization (DRO)** schemes that enforce robustness at the loss function level.\n",
        "\n",
        "#### **Tail-Robust DRO via Expected Shortfall (FC-DRO-ES)**\n",
        "Instead of minimizing Mean Squared Error (MSE), this scheme minimizes a worst-case loss defined by the **Expected Shortfall (ES)** at the $\\alpha=0.10$ level.\n",
        "*   **Mechanism:** It explicitly penalizes models that produce heavy left-tail errors (large downside risks).\n",
        "*   **Weighting:** Weights are determined via exponential reweighting of the stabilized ES loss.\n",
        "\n",
        "#### *Regularized Mean-Variance Combination (FC-DRMV)**\n",
        "Addresses the sensitivity of minimum-variance portfolios to covariance estimation errors.\n",
        "*   **Mechanism:** Solves a min-max problem where the covariance matrix is subject to uncertainty.\n",
        "*   **Solution:** Results in a closed-form solution involving **ridge regularization** of the forecast error covariance matrix ($\\Sigma + \\tau I$), stabilizing the inversion of ill-conditioned matrices.\n",
        "\n",
        "#### **Hybrid Loss Combination (FC-MIX)**\n",
        "*   **Mechanism:** Optimizes a convex combination of MSE (average accuracy) and ES (tail risk), allowing the forecaster to tune the trade-off between central tendency accuracy and crisis robustness.\n",
        "\n",
        "### **Empirical Results and Dynamics**\n",
        "The framework is evaluated using out-of-sample Root Mean Squared Forecast Error (RMSFE) and analysis of weight dynamics.\n",
        "\n",
        "*   **Hybrid Superiority:** Combining FADNS and RF using DRO schemes yields the best performance. The DRO methods effectively filter out the high-variance FADNS forecasts during stress while leveraging their short-term accuracy during calm periods.\n",
        "*   **Adaptive Weighting:**\n",
        "    *   During \"normal\" times, weights are distributed between RF and FADNS.\n",
        "    *   **Crisis Response:** During the COVID-19 shock and the 2022 tightening, the DRO schemes rapidly reallocated weights toward the more stable Random Forest models. This dynamic adjustment provided significant downside protection compared to static or simple adaptive methods (like AFTER).\n",
        "*   **Horizon Analysis:**\n",
        "    *   **Short-term (1m):** Adaptive combinations outperform individual models.\n",
        "    *   **Long-term (12m):** Random Forest dominates; FADNS errors explode.\n",
        "\n",
        "### **Interpretability and Generalization**\n",
        "To ensure the \"Black Box\" ML models are actionable for policymakers:\n",
        "\n",
        "*   **SHAP (SHapley Additive exPlanations):** The authors use SHAP values to interpret the RF models.\n",
        "    *   *Short Horizon:* Driven by high-frequency real activity indicators.\n",
        "    *   *Long Horizon:* Driven by fundamentals—inflation (price indices), labor market data, and financial conditions.\n",
        "*   **Global Extension:** The framework was tested on 10-year sovereign bonds for Canada, China, Germany, Japan, Malaysia, UK, and US.\n",
        "    *   **Result:** The Random Forest approach generalized well, with low RMSFE in low-rate environments (Japan, China) and robust performance in volatile markets (UK, US).\n",
        "\n",
        "### **Conclusion**\n",
        "This paper represents a significant maturation in financial ML literature. It moves beyond merely applying algorithms to demonstrating how **Operations Research principles (Robust Optimization)** can discipline Machine Learning outputs.\n",
        "\n",
        "**Key Takeaway:** In high-dimensional financial time series, **estimation uncertainty** is as dangerous as model misspecification. By treating forecasting as a decision problem under distributional ambiguity, the proposed DRO framework delivers forecasts that are not only accurate on average but resilient to the \"worst-case\" scenarios that define modern financial crises."
      ],
      "metadata": {
        "id": "4j79huxj2SdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "OerW613l9Kaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  Forecasting the U.S. Treasury Yield Curve: A Distributionally Robust Machine Learning Approach\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"Forecasting the U.S. Treasury Yield Curve:\n",
        "#  A Distributionally Robust Machine Learning Approach\" by Jinjun Liu and Ming-Yen\n",
        "#  Cheng (2026). It delivers a computationally tractable system for robust yield\n",
        "#  curve forecasting under distributional ambiguity, integrating parametric factor\n",
        "#  models with nonparametric machine learning through adaptive, risk-aware ensemble\n",
        "#  methods.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Factor-Augmented Dynamic Nelson-Siegel (FADNS) modeling with rolling PCA factors\n",
        "#  • High-dimensional nonparametric forecasting via Random Forests with asymmetric lags\n",
        "#  • Distributionally Robust Optimization (DRO) for forecast combination weights\n",
        "#  • Adaptive forecast aggregation (AFTER) with recursive variance estimation\n",
        "#  • Structural break detection via CUSUM and PELT algorithms\n",
        "#  • Model-agnostic interpretability using SHAP (SHapley Additive exPlanations)\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Rolling-window estimation with strict prevention of look-ahead bias\n",
        "#  • Randomized Cross-Validation for hyperparameter tuning respecting time order\n",
        "#  • Vectorized cross-sectional regression for latent factor extraction\n",
        "#  • Convex and linear programming solvers for robust weight optimization\n",
        "#  • Comprehensive data cleansing, interpolation, and stationarity filtering pipelines\n",
        "#  • Robustness checks across benchmark yields, TIC variables, and global sovereign bonds\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Liu, J., & Cheng, M.-Y. (2026). Forecasting the U.S. Treasury Yield Curve: A\n",
        "#  Distributionally Robust Machine Learning Approach. arXiv preprint arXiv:2601.04608.\n",
        "#  https://arxiv.org/abs/2601.04608\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================#\n",
        "\n",
        "import copy\n",
        "import hashlib\n",
        "import json\n",
        "import logging\n",
        "from typing import Any, Callable, Dict, List, NamedTuple, Optional, Set, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.tseries.offsets import DateOffset, MonthEnd\n",
        "import ruptures as rpt\n",
        "from scipy.optimize import linprog, minimize\n",
        "from scipy.stats import rankdata\n",
        "import shap\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.diagnostic import breaks_cusumolsresid\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "\n",
        "# Configure logging for the validation process\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "id": "mqGLqTd79PO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "-qbnnChx9SmI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Draft 1\n",
        "\n",
        "## **Discussion of the Inputs-Processes-Outputs (IPO) and Research Role of Key Callables**\n",
        "\n",
        "### 1. `validate_all_input_schemas` (Task 1)\n",
        "*   **Inputs**: Raw DataFrames for U.S. yields, macro indicators, benchmark yields, TIC data, global yields, and global macro panels; configuration dictionary; metadata dictionary.\n",
        "*   **Processes**: Iterates through each DataFrame and verifies strict schema compliance: `DatetimeIndex` type, monotonicity, end-of-month alignment, exact column sets (e.g., 15 canonical maturities for yields), numeric data types, and finiteness.\n",
        "*   **Outputs**: Boolean `True` if all validations pass; raises `ValueError` or `TypeError` otherwise.\n",
        "*   **Research Role**: Implements the **Data Integrity** layer. It ensures the input data $Y_t \\in \\mathbb{R}^{15}$ and $Z_t \\in \\mathbb{R}^{111}$ conform to the dimensions and structures assumed by the Nelson-Siegel and Factor models defined in Section 2.1.\n",
        "\n",
        "### 2. `validate_metadata_bundle` (Task 2)\n",
        "*   **Inputs**: Metadata dictionary; configuration dictionary; list of macro columns.\n",
        "*   **Processes**: Validates the maturity-to-$\\tau$ mapping (ensuring units are months, e.g., $\\tau(\\text{\"3M\"})=3$), checks unit conventions (percent points vs decimals), and verifies the existence of frequency maps for interpolation.\n",
        "*   **Outputs**: Boolean `True` if valid.\n",
        "*   **Research Role**: Validates the **Parametric Configuration**. It ensures the decay parameter $\\lambda=0.0609$ will be applied to the correct $\\tau$ values in the Nelson-Siegel loading equation: $\\ell_2(\\tau) = \\frac{1-e^{-\\lambda\\tau}}{\\lambda\\tau}$.\n",
        "\n",
        "### 3. `resolve_and_freeze_config` (Task 3)\n",
        "*   **Inputs**: Raw configuration dictionary containing placeholders.\n",
        "*   **Processes**: Recursively identifies placeholders (e.g., `\"USER_DEFINED...\"`), replaces them with concrete values defined in the replication protocol (e.g., specific random seeds, CUSUM regression form), and generates a SHA256 hash of the final config.\n",
        "*   **Outputs**: Tuple of (Frozen Configuration Dict, Config Hash String).\n",
        "*   **Research Role**: Enforces **Deterministic Reproducibility**. It locks the hyperparameter space $\\Theta$ and random seeds $s$ used in the Random Forest training (Section 2.3).\n",
        "\n",
        "### 4. `cleanse_and_align_indices` (Task 4)\n",
        "*   **Inputs**: All raw DataFrames; frozen configuration.\n",
        "*   **Processes**: Sorts indices, removes duplicates, normalizes timestamps to calendar month-ends using `MonthEnd(0)`, and trims data to the global study date range.\n",
        "*   **Outputs**: Tuple of cleansed and aligned DataFrames.\n",
        "*   **Research Role**: Implements **Temporal Alignment**. It ensures that $t$ represents the same point in time across all datasets, a prerequisite for the rolling window estimation $t-w+1 \\dots t$.\n",
        "\n",
        "### 5. `cleanse_yield_panel` (Task 5)\n",
        "*   **Inputs**: Aligned U.S. yield DataFrame; configuration; metadata.\n",
        "*   **Processes**: Identifies dates with incomplete cross-sections (fewer than 15 maturities) and applies the specified policy (e.g., `\"drop_date\"`) to remove them.\n",
        "*   **Outputs**: Cleansed yield DataFrame.\n",
        "*   **Research Role**: Ensures **Cross-Sectional Completeness**. It guarantees that the yield vector $y_t$ is fully observed for the cross-sectional regression $\\hat{\\beta}_t = (L^\\top L)^{-1} L^\\top y_t$ in Section 2.2.1.\n",
        "\n",
        "### 6. `interpolate_macro_panels` (Task 6)\n",
        "*   **Inputs**: Aligned macro DataFrames (US and Global); metadata; configuration.\n",
        "*   **Processes**: Identifies quarterly variables using the frequency map and applies linear time-based interpolation to convert them to monthly frequency.\n",
        "*   **Outputs**: Tuple of (Cleansed US Macro, Dictionary of Cleansed Global Macro).\n",
        "*   **Research Role**: Implements **Data Frequency Harmonization**. It transforms quarterly predictors $Z_q$ into monthly series $Z_t$ via $Z_m = Q_1 + \\frac{m - m_1}{m_2 - m_1}(Q_2 - Q_1)$ (Section 2.1.2).\n",
        "\n",
        "### 7. `run_cusum_tests` (Task 7)\n",
        "*   **Inputs**: Cleansed yield DataFrame; configuration.\n",
        "*   **Processes**: Fits an intercept-only OLS model $y_t = \\mu + \\varepsilon_t$ for each maturity and applies the CUSUM test on recursive residuals to detect parameter instability.\n",
        "*   **Outputs**: DataFrame of test statistics and p-values.\n",
        "*   **Research Role**: Implements **Structural Break Detection (Stage 1)**. It verifies the hypothesis of parameter constancy using the cumulative sum of recursive residuals (Brown et al., 1975), as described in Section 2.1.1.\n",
        "\n",
        "### 8. `run_pelt_detection` (Task 8)\n",
        "*   **Inputs**: Cleansed yield DataFrame; configuration.\n",
        "*   **Processes**: Applies the Pruned Exact Linear Time (PELT) algorithm with an RBF cost function and penalty $\\beta=10$ to identify change points in the yield series.\n",
        "*   **Outputs**: Tuple of (Formatted Breakpoints Table, Raw Breakpoints Dictionary).\n",
        "*   **Research Role**: Implements **Structural Break Detection (Stage 2)**. It identifies the specific dates of regime shifts $\\mathcal{T}^*$ by minimizing the penalized cost function $\\sum \\mathcal{C}(y_{t_i:t_{i+1}}) + \\beta K$ (Section 2.1.1).\n",
        "\n",
        "### 9. `construct_dns_loading_matrix` (Task 9)\n",
        "*   **Inputs**: Configuration; metadata.\n",
        "*   **Processes**: Computes the Level, Slope, and Curvature loadings for each canonical maturity $\\tau$ using the fixed decay parameter $\\lambda$.\n",
        "*   **Outputs**: Tuple of (Loading Matrix $L \\in \\mathbb{R}^{15 \\times 3}$, Maturity List).\n",
        "*   **Research Role**: Constructs the **Nelson-Siegel Factor Loadings**. It implements the equations: $\\ell_1(\\tau)=1$, $\\ell_2(\\tau)=\\frac{1-e^{-\\lambda\\tau}}{\\lambda\\tau}$, $\\ell_3(\\tau)=\\frac{1-e^{-\\lambda\\tau}}{\\lambda\\tau}-e^{-\\lambda\\tau}$ (Eq 2.1).\n",
        "\n",
        "### 10. `extract_dns_factors` (Task 10)\n",
        "*   **Inputs**: Cleansed yield DataFrame; Loading Matrix $L$; Maturity List.\n",
        "*   **Processes**: Performs vectorized cross-sectional OLS at each time $t$ to estimate the latent factors $\\beta_t$.\n",
        "*   **Outputs**: DataFrame of factors $\\beta_t$ (Level, Slope, Curvature).\n",
        "*   **Research Role**: Implements **Latent Factor Extraction**. It solves the inverse problem $\\hat{\\beta}_t = (L^\\top L)^{-1} L^\\top y_t$ for the Dynamic Nelson-Siegel model (Section 2.2.1).\n",
        "\n",
        "### 11. `estimate_rolling_dns_var` (Task 11)\n",
        "*   **Inputs**: Factor DataFrame; configuration.\n",
        "*   **Processes**: Iterates through rolling windows of size $w=60$, estimating a VAR(1) model on the factors $\\beta_t$ and checking for stability (eigenvalues < 1).\n",
        "*   **Outputs**: Dictionary mapping origin date to VAR parameters $(c, \\Phi)$.\n",
        "*   **Research Role**: Implements **State Dynamics Estimation**. It fits the transition equation $\\beta_{t+1} = c + \\Phi \\beta_t + \\eta_t$ (Eq 2.2a) over rolling windows.\n",
        "\n",
        "### 12. `generate_dns_forecasts` (Task 12)\n",
        "*   **Inputs**: VAR parameters; Factor DataFrame; Loading Matrix; configuration.\n",
        "*   **Processes**: Generates recursive $h$-step-ahead factor forecasts using the estimated VAR parameters and maps them to yields using the loading matrix.\n",
        "*   **Outputs**: Long-format DataFrame of yield forecasts.\n",
        "*   **Research Role**: Implements **Parametric Forecasting**. It computes $\\hat{\\beta}_{t+h|t} = \\sum_{j=0}^{h-1} \\Phi^j c + \\Phi^h \\beta_t$ and $\\hat{y}_{t+h|t} = L \\hat{\\beta}_{t+h|t}$ (Section 2.2.1).\n",
        "\n",
        "### 13. `compute_dns_rmsfe` (Task 13)\n",
        "*   **Inputs**: Forecast DataFrame; Realized Yields; configuration.\n",
        "*   **Processes**: Aligns forecasts with realized values, computes forecast errors, calculates RMSFE, and converts to basis points.\n",
        "*   **Outputs**: RMSFE Table (Maturity x Horizon).\n",
        "*   **Research Role**: Implements **Performance Evaluation**. It calculates $\\text{RMSFE}_h(\\tau) = \\sqrt{\\frac{1}{N} \\sum (y_{t+h} - \\hat{y}_{t+h|t})^2}$ (Section 2.2.1).\n",
        "\n",
        "### 14. `preprocess_fadns_macro` (Task 14)\n",
        "*   **Inputs**: Cleansed Macro DataFrame; configuration; metadata.\n",
        "*   **Processes**: Extracts rolling windows of macro data for each forecast origin, enforcing a 1-month publication lag.\n",
        "*   **Outputs**: Dictionary mapping origin date to Macro Block DataFrame.\n",
        "*   **Research Role**: Defines the **Information Set**. It constructs $\\mathcal{I}_t = \\{Z_{t-1}, \\dots, Z_{t-w}\\}$ to prevent look-ahead bias (Section 2.2.2).\n",
        "\n",
        "### 15. `apply_rolling_adf_filtering` (Task 15)\n",
        "*   **Inputs**: Macro Blocks; configuration.\n",
        "*   **Processes**: Applies the Augmented Dickey-Fuller test to each variable in each window; differences variables where the null of unit root is not rejected ($p \\ge 0.10$).\n",
        "*   **Outputs**: Dictionary of stationarity-transformed Macro Blocks.\n",
        "*   **Research Role**: Implements **Stationarity Filtering**. It ensures predictors are $I(0)$ before PCA extraction, adapting to local window properties (Section 2.1.2).\n",
        "\n",
        "### 16. `standardize_rolling_macro_blocks` (Task 16)\n",
        "*   **Inputs**: Transformed Macro Blocks; configuration.\n",
        "*   **Processes**: Standardizes each variable within the rolling window to zero mean and unit variance (Z-score).\n",
        "*   **Outputs**: Dictionary of standardized Macro Blocks.\n",
        "*   **Research Role**: Implements **Feature Scaling**. It computes $\\tilde{Z}_{j,s} = (Z_{j,s} - \\bar{Z}_{j,t}) / \\sigma_{j,t}$ to prepare for PCA (Section 2.2.2).\n",
        "\n",
        "### 17. `construct_rolling_pca_factors` (Task 17)\n",
        "*   **Inputs**: Standardized Macro Blocks; configuration.\n",
        "*   **Processes**: Performs PCA on the rolling window, aligns eigenvector signs with the previous window to ensure continuity, and projects the last observation onto the principal components.\n",
        "*   **Outputs**: DataFrame of Macro Factors $F_t^{(k)}$.\n",
        "*   **Research Role**: Implements **Dimension Reduction**. It extracts diffusion indices $F_t^{(k)} = V_k^\\top \\tilde{Z}_{t-1}$ from the high-dimensional macro panel (Section 2.2.2).\n",
        "\n",
        "### 18. `run_fadns_estimation_and_forecast` (Task 18)\n",
        "*   **Inputs**: DNS Factors; Macro Factors; configuration.\n",
        "*   **Processes**: Constructs the augmented state vector $X_t^{(k)} = [\\beta_t, F_t^{(k)}]$, estimates a VAR(1) on $X_t$, and generates recursive forecasts for the beta components.\n",
        "*   **Outputs**: DataFrame of Beta Forecasts for all $k$.\n",
        "*   **Research Role**: Implements **Factor-Augmented Forecasting**. It models the joint dynamics $X_{t+1}^{(k)} = c^{(k)} + \\Phi^{(k)} X_t^{(k)} + \\eta_t$ (Section 2.2.2).\n",
        "\n",
        "### 19. `compute_fadns_rmsfe` (Task 19)\n",
        "*   **Inputs**: Beta Forecasts; Realized Yields; Loading Matrix; configuration.\n",
        "*   **Processes**: Maps beta forecasts to yields, computes errors, and calculates RMSFE for each PCA dimension $k$.\n",
        "*   **Outputs**: Tuple of (RMSFE Tables per Horizon, Raw RMSFE DataFrame).\n",
        "*   **Research Role**: Implements **FADNS Evaluation**. It assesses the predictive accuracy of the augmented model for different factor dimensions $k$ (Section 3.1.2).\n",
        "\n",
        "### 20. `select_best_fadns_k` (Task 20)\n",
        "*   **Inputs**: FADNS RMSFE Tables; configuration.\n",
        "*   **Processes**: Identifies the $k$ that minimizes RMSFE for each maturity and horizon.\n",
        "*   **Outputs**: Tuple of (Best-k Table, Best RMSFE Table).\n",
        "*   **Research Role**: Implements **Model Selection**. It determines the optimal number of macro factors $k^*(\\tau, h) = \\arg\\min_k \\text{RMSFE}_h^{(k)}(\\tau)$ (Section 3.1.2).\n",
        "\n",
        "### 21. `construct_rf_features` (Task 21)\n",
        "*   **Inputs**: Cleansed Macro; Cleansed Yields; configuration.\n",
        "*   **Processes**: Constructs the high-dimensional feature vector $W_t$ by concatenating lagged macro variables and lagged yields according to the specified lag structure.\n",
        "*   **Outputs**: Tuple of (Macro Features DataFrame, Dictionary of Yield Features).\n",
        "*   **Research Role**: Implements **Feature Engineering**. It builds the predictor set $W_t = (Z_{t-\\ell})_{\\ell \\in \\mathcal{L}_Z} \\cup (y_{t-\\ell}(\\tau))_{\\ell \\in \\mathcal{L}_y}$ with $d_W = 6720$ (Section 2.3).\n",
        "\n",
        "### 22. `normalize_rf_data` (Task 22)\n",
        "*   **Inputs**: Training Features; Training Targets.\n",
        "*   **Processes**: Computes min and max values for features and targets within the training window and scales data to $[0, 1]$.\n",
        "*   **Outputs**: Tuple of (Normalized X, Normalized y, Scaler Parameters).\n",
        "*   **Research Role**: Implements **Data Normalization**. It applies min-max scaling $x' = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}$ to facilitate ML training (Section 2.3).\n",
        "\n",
        "### 23. `train_rf_model` (Task 23)\n",
        "*   **Inputs**: Normalized Training Data; configuration; seed.\n",
        "*   **Processes**: Performs Randomized Cross-Validation with Time Series Split to optimize Random Forest hyperparameters, then refits the best model.\n",
        "*   **Outputs**: Fitted RandomForestRegressor.\n",
        "*   **Research Role**: Implements **Model Training**. It solves $\\theta^* = \\arg\\min_{\\theta} \\text{CV-MSE}(\\theta)$ to obtain the optimal nonparametric estimator $\\hat{g}$ (Section 2.3).\n",
        "\n",
        "### 24. `generate_rf_forecasts` (Task 24)\n",
        "*   **Inputs**: Features; Yields; configuration.\n",
        "*   **Processes**: Iterates through rolling windows, normalizes data, trains RF models (calling Task 23), generates forecasts, and inverse-transforms them to the original scale.\n",
        "*   **Outputs**: DataFrame of RF Forecasts.\n",
        "*   **Research Role**: Implements **Nonparametric Forecasting**. It computes $\\hat{y}_{t+h|t} = \\hat{g}_{h,\\tau}(W_t)$ using the ensemble of decision trees (Section 2.3).\n",
        "\n",
        "### 25. `compute_rf_rmsfe_summary` (Task 25)\n",
        "*   **Inputs**: RF Forecasts; Realized Yields; configuration.\n",
        "*   **Processes**: Computes RMSFE for each seed, then aggregates (mean, min, max) across seeds.\n",
        "*   **Outputs**: Tuple of (Summary Table, Numeric Stats).\n",
        "*   **Research Role**: Implements **RF Evaluation**. It reports the stability and accuracy of the RF ensemble across random initializations (Section 3.1.1).\n",
        "\n",
        "### 26. `prepare_forecast_combination_data` (Task 26)\n",
        "*   **Inputs**: FADNS Forecasts; RF Forecasts; Realized Yields; configuration.\n",
        "*   **Processes**: Aligns forecasts from all models into a single pool and computes the rolling error matrix for weight estimation.\n",
        "*   **Outputs**: Tuple of (Forecast Pool DataFrame, Error Matrix DataFrame).\n",
        "*   **Research Role**: Implements **Pool Construction**. It assembles the set of candidate forecasts $\\{\\hat{y}_{m,t}\\}_{m=1}^{20}$ and their historical errors $E_t$ (Section 2.4).\n",
        "\n",
        "### 27. `compute_forecast_weights` (Task 27)\n",
        "*   **Inputs**: Error Matrix; configuration.\n",
        "*   **Processes**: Computes combination weights $w_t$ for each method (Classic, Var/Risk, DRO, AFTER) using the rolling error history.\n",
        "*   **Outputs**: DataFrame of Weights.\n",
        "*   **Research Role**: Implements **Weight Optimization**. It solves for $w_t \\in \\Delta_M$ using schemes like DRO-ES ($w \\propto \\exp(\\eta \\text{ES}_\\alpha(e))$) and Regularized Mean-Variance (Section 2.4).\n",
        "\n",
        "### 28. `compute_combination_results` (Task 28)\n",
        "*   **Inputs**: Forecast Pool; Weights; Realized Yields; configuration.\n",
        "*   **Processes**: Computes the combined forecast as the dot product of weights and model forecasts, then evaluates RMSFE.\n",
        "*   **Outputs**: Tuple of (RMSFE Table, Combined Forecasts).\n",
        "*   **Research Role**: Implements **Ensemble Forecasting**. It computes $\\hat{y}_t^{(c)} = \\sum_{m=1}^M w_{m,t} \\hat{y}_{m,t}$ and evaluates the performance of the distributionally robust framework (Section 3.1.3).\n",
        "\n",
        "### 29. `run_main_study_pipeline` (Task 29)\n",
        "*   **Inputs**: All raw DataFrames; configuration; metadata.\n",
        "*   **Processes**: Orchestrates the execution of Tasks 1 through 28 (Validation, Cleansing, DNS, FADNS, RF, Combination).\n",
        "*   **Outputs**: Dictionary of all main study artifacts.\n",
        "*   **Research Role**: **Main Study Orchestrator**. It manages the workflow for the primary empirical results reported in the manuscript.\n",
        "\n",
        "### 30. `run_benchmark_rf_analysis` (Task 30)\n",
        "*   **Inputs**: Benchmark Yields; Macro Data; configuration.\n",
        "*   **Processes**: Orchestrates the comparison between Multi-Output RF and Single-Maturity RF on the benchmark yield dataset.\n",
        "*   **Outputs**: Dictionary of comparison results.\n",
        "*   **Research Role**: Implements **Robustness Check (Benchmark)**. It evaluates whether modeling the yield curve jointly (Multi-Output) offers advantages over univariate modeling (Section 4.1).\n",
        "\n",
        "### 31. `execute_benchmark_analysis` (Task 31)\n",
        "*   **Inputs**: Benchmark Yields; Macro Data; configuration.\n",
        "*   **Processes**: Executes the benchmark analysis and formats the results into comparison tables.\n",
        "*   **Outputs**: Tuple of (Multi RMSFE, Single RMSFE, Diff Table).\n",
        "*   **Research Role**: **Benchmark Execution**. It produces the specific tables required to validate the robustness of the RF approach on coupon-bearing yields.\n",
        "\n",
        "### 32. `run_tic_robustness_analysis` (Task 32)\n",
        "*   **Inputs**: Benchmark Yields; Macro Data; TIC Data; configuration.\n",
        "*   **Processes**: Orchestrates the evaluation of TIC variable augmentation, ensuring sample alignment between baseline and augmented models.\n",
        "*   **Outputs**: Dictionary of TIC analysis results.\n",
        "*   **Research Role**: Implements **Robustness Check (TIC)**. It assesses the predictive value of Treasury International Capital flow variables (Section 4.1).\n",
        "\n",
        "### 33. `execute_tic_analysis` (Task 33)\n",
        "*   **Inputs**: Benchmark Yields; Macro Data; TIC Data; configuration.\n",
        "*   **Processes**: Runs baseline and augmented RF models and computes the change in RMSFE.\n",
        "*   **Outputs**: Dictionary containing the improvement heatmap data.\n",
        "*   **Research Role**: **TIC Execution**. It quantifies $\\Delta \\text{RMSFE}$ to determine if supply-demand factors improve forecasting accuracy.\n",
        "\n",
        "### 34. `run_global_extension_analysis` (Task 34)\n",
        "*   **Inputs**: Global Yields; Global Macro Panels; configuration.\n",
        "*   **Processes**: Orchestrates the RF forecasting pipeline for each of the 7 countries in the global dataset.\n",
        "*   **Outputs**: Dictionary of global analysis results.\n",
        "*   **Research Role**: Implements **Robustness Check (Global)**. It tests the generalizability of the framework to international sovereign bond markets (Section 4.2).\n",
        "\n",
        "### 35. `execute_global_rf_extension` (Task 35)\n",
        "*   **Inputs**: Global Yields; Global Macro Panels; configuration.\n",
        "*   **Processes**: Executes the per-country RF workflow and formats the global RMSFE table.\n",
        "*   **Outputs**: Dictionary containing the global RMSFE table.\n",
        "*   **Research Role**: **Global Execution**. It produces the cross-country performance metrics to demonstrate the method's stability across different markets.\n",
        "\n",
        "### 36. `compute_shap_analysis` (Task 36)\n",
        "*   **Inputs**: RF Features; Realized Yields; configuration.\n",
        "*   **Processes**: Refits the optimal RF model for the final window using rigorous CV and computes SHAP values for the training set.\n",
        "*   **Outputs**: Dictionary of SHAP values and feature names.\n",
        "*   **Research Role**: Implements **Model Interpretability**. It calculates $\\phi_j(x)$ to attribute predictive power to specific macro-financial drivers (Section 4.1.1).\n",
        "\n",
        "### 37. `aggregate_shap_results` (Task 37)\n",
        "*   **Inputs**: Raw SHAP results.\n",
        "*   **Processes**: Aggregates SHAP values across maturities and seeds to compute GlobalSHAP importance scores.\n",
        "*   **Outputs**: Tuple of (GlobalSHAP Table, Plot Data).\n",
        "*   **Research Role**: **Importance Aggregation**. It computes $\\text{GlobalSHAP}_j = \\mathbb{E}[|\\phi_j|]$ to rank predictors by their overall contribution to yield curve forecasting.\n",
        "\n",
        "### 38. `prepare_weight_dynamics_data` (Task 38)\n",
        "*   **Inputs**: Combination Weights; configuration.\n",
        "*   **Processes**: Aggregates weights into \"RF\" and \"FADNS\" groups for the DRO methods.\n",
        "*   **Outputs**: Dictionary of aggregated weight time series.\n",
        "*   **Research Role**: **Visualization (Weights)**. It prepares the data to visualize how the distributionally robust aggregation shifts weight between parametric and nonparametric models during stress periods (Section 3.3).\n",
        "\n",
        "### 39. `prepare_error_dynamics_data` (Task 39)\n",
        "*   **Inputs**: Combined Forecasts; Realized Yields; configuration.\n",
        "*   **Processes**: Computes forecast errors and categorizes them by method family (DRO, Classic, etc.) for plotting.\n",
        "*   **Outputs**: Dictionary of error time series.\n",
        "*   **Research Role**: **Visualization (Errors)**. It prepares the data to analyze the time-varying performance and stability of different combination schemes (Section 3.2).\n",
        "\n",
        "### 40. `package_reproduction_artifacts` (Task 40)\n",
        "*   **Inputs**: Results from all orchestrators.\n",
        "*   **Processes**: Aggregates all tables, figure data, and audit logs into a single structured dictionary.\n",
        "*   **Outputs**: Final Artifact Bundle.\n",
        "*   **Research Role**: **Reproducibility Packaging**. It organizes the study's outputs into a coherent format for verification and reporting.\n",
        "\n",
        "### 41. `run_main_study_pipeline_variant` (Final Orchestrator)\n",
        "*   **Inputs**: All raw DataFrames; raw configuration; metadata.\n",
        "*   **Processes**: Sequentially executes Tasks 1 through 40, managing data flow and dependencies to produce the complete study results.\n",
        "*   **Outputs**: Final Artifact Bundle.\n",
        "*   **Research Role**: **End-to-End Execution**. It serves as the master controller for the entire research pipeline, ensuring that every step from data validation to final artifact generation is executed in the correct order and with the correct inputs.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## **Usage Examples**\n",
        "\n",
        "Below is a Python script which uses synthentic data to demonstrate how to run the End-to-End Pipeline Accurately:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "import logging\n",
        "from typing import Dict, Any, List, Tuple\n",
        "from faker import Faker\n",
        "from pandas.tseries.offsets import MonthEnd\n",
        "\n",
        "# Import all the Python modules that are required to run the callables and run each callable from the workbook\n",
        "\n",
        "# Configure logging to stdout for demonstration\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. Synthetic Data Generation (Mocking LSEG Reuters Workspace Data)\n",
        "# ==============================================================================\n",
        "\n",
        "def generate_synthetic_data() -> Tuple[\n",
        "    pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame,\n",
        "    pd.DataFrame, Dict[str, pd.DataFrame], Dict[str, Any]\n",
        "]:\n",
        "    \"\"\"\n",
        "    Generates synthetic datasets and metadata for the U.S. Treasury Yield Curve study.\n",
        "    \n",
        "    This function simulates the data structures that would typically be retrieved\n",
        "    from a financial data provider like LSEG Reuters Workspace. It ensures all\n",
        "    dimensions, indices, and column names match the study's canonical schemas.\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "        - df_us_yields_raw\n",
        "        - df_us_macro_raw\n",
        "        - df_us_benchmark_yields_raw\n",
        "        - df_us_tic_raw\n",
        "        - df_global_yields_raw\n",
        "        - global_macro_panels_raw\n",
        "        - study_data_metadata\n",
        "    \"\"\"\n",
        "    fake = Faker()\n",
        "    np.random.seed(42) # Ensure reproducibility of synthetic data\n",
        "\n",
        "    # --- A. Define Date Ranges ---\n",
        "    # Main study range: 2006-01-31 to 2025-08-31\n",
        "    dates_main = pd.date_range(start=\"2006-01-31\", end=\"2025-08-31\", freq=\"ME\")\n",
        "    \n",
        "    # Benchmark/Global range: 2010-01-31 to 2025-08-31\n",
        "    dates_global = pd.date_range(start=\"2010-01-31\", end=\"2025-08-31\", freq=\"ME\")\n",
        "    \n",
        "    # TIC range: 2014-09-30 to 2025-08-31\n",
        "    dates_tic = pd.date_range(start=\"2014-09-30\", end=\"2025-08-31\", freq=\"ME\")\n",
        "\n",
        "    # --- B. Generate US Zero-Coupon Yields ---\n",
        "    # 15 canonical maturities\n",
        "    us_maturities = [\"3M\",\"6M\",\"1Y\",\"2Y\",\"3Y\",\"4Y\",\"5Y\",\"6Y\",\"7Y\",\"8Y\",\"9Y\",\"10Y\",\"15Y\",\"20Y\",\"30Y\"]\n",
        "    # Simulate yields as random walks + mean (starting around 4%)\n",
        "    yield_data = np.cumsum(np.random.normal(0, 0.1, size=(len(dates_main), len(us_maturities))), axis=0) + 4.0\n",
        "    df_us_yields_raw = pd.DataFrame(yield_data, index=dates_main, columns=us_maturities)\n",
        "    df_us_yields_raw.index.name = \"Date\"\n",
        "\n",
        "    # --- C. Generate US Macro Predictors ---\n",
        "    # 111 variables. Mix of Monthly (M) and Quarterly (Q).\n",
        "    # We'll designate every 10th variable as Quarterly to test interpolation.\n",
        "    macro_cols = [f\"Macro_Var_{i+1}\" for i in range(111)]\n",
        "    macro_data = np.random.normal(0, 1, size=(len(dates_main), 111))\n",
        "    df_us_macro_raw = pd.DataFrame(macro_data, index=dates_main, columns=macro_cols)\n",
        "    \n",
        "    # Introduce NaNs for Quarterly variables (non-quarter-end months)\n",
        "    us_freq_map = {}\n",
        "    for i, col in enumerate(macro_cols):\n",
        "        if (i + 1) % 10 == 0: # Every 10th var is Quarterly\n",
        "            us_freq_map[col] = \"Q\"\n",
        "            # Set non-quarter-ends to NaN\n",
        "            is_quarter_end = df_us_macro_raw.index.month.isin([3, 6, 9, 12])\n",
        "            df_us_macro_raw.loc[~is_quarter_end, col] = np.nan\n",
        "        else:\n",
        "            us_freq_map[col] = \"M\"\n",
        "            \n",
        "    df_us_macro_raw.index.name = \"Date\"\n",
        "\n",
        "    # --- D. Generate US Benchmark Yields ---\n",
        "    # 9 maturities\n",
        "    bench_maturities = [\"3M\",\"6M\",\"1Y\",\"2Y\",\"3Y\",\"5Y\",\"7Y\",\"10Y\",\"30Y\"]\n",
        "    bench_data = np.cumsum(np.random.normal(0, 0.1, size=(len(dates_global), len(bench_maturities))), axis=0) + 3.5\n",
        "    df_us_benchmark_yields_raw = pd.DataFrame(bench_data, index=dates_global, columns=bench_maturities)\n",
        "    df_us_benchmark_yields_raw.index.name = \"Date\"\n",
        "\n",
        "    # --- E. Generate US TIC Data ---\n",
        "    # 2 variables\n",
        "    tic_cols = [\"US_TIC_Gross_External_Debt_Position\", \"US_General_Government_Gross_External_Debt_Position\"]\n",
        "    tic_data = np.cumsum(np.random.normal(10, 5, size=(len(dates_tic), 2)), axis=0) + 1000.0\n",
        "    df_us_tic_raw = pd.DataFrame(tic_data, index=dates_tic, columns=tic_cols)\n",
        "    df_us_tic_raw.index.name = \"Date\"\n",
        "\n",
        "    # --- F. Generate Global 10Y Yields ---\n",
        "    # 7 countries\n",
        "    global_cols = [\"US_10Y\",\"UK_10Y\",\"Germany_10Y\",\"Japan_10Y\",\"Canada_10Y\",\"China_10Y\",\"Malaysia_10Y\"]\n",
        "    global_data = np.cumsum(np.random.normal(0, 0.08, size=(len(dates_global), 7)), axis=0) + 3.0\n",
        "    df_global_yields_raw = pd.DataFrame(global_data, index=dates_global, columns=global_cols)\n",
        "    df_global_yields_raw.index.name = \"Date\"\n",
        "\n",
        "    # --- G. Generate Global Macro Panels ---\n",
        "    # Dictionary of DataFrames. US must match df_us_macro_raw.\n",
        "    # Others are synthetic.\n",
        "    global_macro_panels_raw = {\"US\": df_us_macro_raw}\n",
        "    global_freq_maps = {\"US\": us_freq_map}\n",
        "    \n",
        "    countries = [\"UK\", \"Germany\", \"Japan\", \"Canada\", \"China\", \"Malaysia\"]\n",
        "    for country in countries:\n",
        "        # Generate random panel (e.g., 50 vars)\n",
        "        n_vars = 50\n",
        "        c_cols = [f\"{country}_Macro_{i+1}\" for i in range(n_vars)]\n",
        "        c_data = np.random.normal(0, 1, size=(len(dates_global), n_vars))\n",
        "        df_c = pd.DataFrame(c_data, index=dates_global, columns=c_cols)\n",
        "        \n",
        "        # Create freq map (all Monthly for simplicity in synthetic data)\n",
        "        c_map = {col: \"M\" for col in c_cols}\n",
        "        \n",
        "        global_macro_panels_raw[country] = df_c\n",
        "        global_freq_maps[country] = c_map\n",
        "\n",
        "    # --- H. Create Metadata Bundle ---\n",
        "    study_data_metadata = {\n",
        "        \"maturity_to_tau_months\": {\n",
        "            \"3M\": 3, \"6M\": 6, \"1Y\": 12, \"2Y\": 24, \"3Y\": 36, \"4Y\": 48, \"5Y\": 60,\n",
        "            \"6Y\": 72, \"7Y\": 84, \"8Y\": 96, \"9Y\": 108, \"10Y\": 120, \"15Y\": 180,\n",
        "            \"20Y\": 240, \"30Y\": 360\n",
        "        },\n",
        "        \"yield_units\": \"pct_points\",\n",
        "        \"rmsfe_units\": \"bps\",\n",
        "        \"pct_points_to_bps_multiplier\": 100.0,\n",
        "        \"variable_frequency_map\": global_freq_maps,\n",
        "        \"missing_data_policy\": {\n",
        "            \"yields\": \"drop_date\",\n",
        "            \"macro\": \"drop_window\" # or 'impute_linear'\n",
        "        },\n",
        "        \"provenance_ids\": {\n",
        "            \"US_Yields\": \"LSEG_US_ZERO\",\n",
        "            \"US_Macro\": \"LSEG_US_MACRO\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return (\n",
        "        df_us_yields_raw, df_us_macro_raw, df_us_benchmark_yields_raw,\n",
        "        df_us_tic_raw, df_global_yields_raw, global_macro_panels_raw,\n",
        "        study_data_metadata\n",
        "    )\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. Main Execution Script\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --------------------------------------------------------------------------\n",
        "    # Step 1: Load Configuration\n",
        "    # --------------------------------------------------------------------------\n",
        "    logger.info(\"Loading configuration from 'config.yaml'...\")\n",
        "    \n",
        "    # In a real scenario, we read the file:\n",
        "    # with open(\"config.yaml\", \"r\") as f:\n",
        "    #     raw_study_config = yaml.safe_load(f)\n",
        "    \n",
        "    # For this example, we assume the config dictionary structure is available\n",
        "    # (as defined in the prompt's STUDY_CONFIG). We simulate the load.\n",
        "    # Note: This dictionary must match the schema expected by Task 3.\n",
        "    raw_study_config = STUDY_CONFIG # Assumed to be defined in the environment/notebook\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # Step 2: Generate/Load Data\n",
        "    # --------------------------------------------------------------------------\n",
        "    logger.info(\"Loading raw data (Synthetic Generation)...\")\n",
        "    \n",
        "    (\n",
        "        df_us_yields_raw,\n",
        "        df_us_macro_raw,\n",
        "        df_us_benchmark_yields_raw,\n",
        "        df_us_tic_raw,\n",
        "        df_global_yields_raw,\n",
        "        global_macro_panels_raw,\n",
        "        study_metadata\n",
        "    ) = generate_synthetic_data()\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # Step 3: Execute Pipeline\n",
        "    # --------------------------------------------------------------------------\n",
        "    logger.info(\"Executing 'run_main_study_pipeline_variant'...\")\n",
        "    \n",
        "    # This function orchestrates Tasks 1 through 40\n",
        "    artifacts = run_main_study_pipeline_variant(\n",
        "        df_us_yields_raw=df_us_yields_raw,\n",
        "        df_us_macro_raw=df_us_macro_raw,\n",
        "        df_us_benchmark_yields_raw=df_us_benchmark_yields_raw,\n",
        "        df_us_tic_raw=df_us_tic_raw,\n",
        "        df_global_yields_raw=df_global_yields_raw,\n",
        "        global_macro_panels_raw=global_macro_panels_raw,\n",
        "        raw_study_config=raw_study_config,\n",
        "        study_metadata=study_metadata\n",
        "    )\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # Step 4: Inspect Results\n",
        "    # --------------------------------------------------------------------------\n",
        "    logger.info(\"Pipeline execution complete. Inspecting artifacts...\")\n",
        "    \n",
        "    # Access the final packaged bundle (Task 40 output)\n",
        "    tables = artifacts[\"Tables\"]\n",
        "    figures = artifacts[\"Figures\"]\n",
        "    audit = artifacts[\"Audit\"]\n",
        "    \n",
        "    # Display key results\n",
        "    print(\"\\n=== DNS RMSFE (Basis Points) ===\")\n",
        "    print(tables[\"DNS_RMSFE\"].head())\n",
        "    \n",
        "    print(\"\\n=== FADNS Best-k Selection (First 5 Maturities) ===\")\n",
        "    print(tables[\"FADNS_Best_k\"].head())\n",
        "    \n",
        "    print(\"\\n=== Random Forest RMSFE Summary ===\")\n",
        "    print(tables[\"RF_RMSFE_Summary\"].head())\n",
        "    \n",
        "    print(\"\\n=== Forecast Combination RMSFE (Horizon=1) ===\")\n",
        "    print(tables[\"Combination_RMSFE_H1\"].head())\n",
        "    \n",
        "    print(\"\\n=== Global Extension RMSFE ===\")\n",
        "    print(tables[\"Global_RMSFE\"].head())\n",
        "    \n",
        "    print(\"\\n=== Audit Info ===\")\n",
        "    print(f\"Config Hash: {audit['Config_Hash']}\")\n",
        "    \n",
        "    logger.info(\"Example run finished successfully.\")\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Implemented Callables**"
      ],
      "metadata": {
        "id": "10KtC-av9U1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 — Validate the schema compliance of all DataFrame inputs against canonical specifications\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Validate the schema compliance of all DataFrame inputs\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 1: Validate df_us_yields_raw against the U.S. zero-coupon yield schema\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_us_yields_schema(\n",
        "    df: pd.DataFrame,\n",
        "    canonical_columns: List[str],\n",
        "    min_date: str,\n",
        "    max_date: str\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Validates the U.S. zero-coupon yields DataFrame against the strict schema requirements\n",
        "    of the manuscript.\n",
        "\n",
        "    Checks performed:\n",
        "    1. Index is DatetimeIndex.\n",
        "    2. Index is strictly monotonic increasing and unique.\n",
        "    3. Index is strictly End-of-Month (calendar).\n",
        "    4. Columns match the canonical 15 maturities exactly.\n",
        "    5. Data type is float64.\n",
        "    6. Values are finite (no infs).\n",
        "    7. Date range covers the manuscript sample (2006-01-31 to 2025-08-31).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw U.S. zero-coupon yields DataFrame.\n",
        "        canonical_columns (List[str]): List of required column names (e.g., [\"3M\", ..., \"30Y\"]).\n",
        "        min_date (str): Minimum required start date (ISO format).\n",
        "        max_date (str): Minimum required end date (ISO format).\n",
        "\n",
        "    Returns:\n",
        "        bool: True if validation passes.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any schema constraint is violated.\n",
        "        TypeError: If input types are incorrect.\n",
        "    \"\"\"\n",
        "    # 1. Index type check\n",
        "    # Ensure the index is a pandas DatetimeIndex\n",
        "    if not isinstance(df.index, pd.DatetimeIndex):\n",
        "        raise TypeError(f\"df_us_yields_raw index must be DatetimeIndex, got {type(df.index)}\")\n",
        "\n",
        "    # 2. Index monotonicity and uniqueness\n",
        "    # Ensure timestamps are strictly increasing\n",
        "    if not df.index.is_monotonic_increasing:\n",
        "        raise ValueError(\"df_us_yields_raw index must be strictly monotonic increasing.\")\n",
        "    # Ensure no duplicate timestamps exist\n",
        "    if df.index.has_duplicates:\n",
        "        raise ValueError(\"df_us_yields_raw index contains duplicate timestamps.\")\n",
        "\n",
        "    # 3. End-of-month alignment\n",
        "    # Check if every date is a calendar month end\n",
        "    # is_month_end checks if the date is the last day of the month\n",
        "    if not df.index.is_month_end.all():\n",
        "        # Identify offending dates for the error message\n",
        "        offending_dates = df.index[~df.index.is_month_end]\n",
        "        raise ValueError(f\"df_us_yields_raw contains non-month-end dates. First 5 offenders: {offending_dates[:5]}\")\n",
        "\n",
        "    # 4. Column set check\n",
        "    # Verify exact match with canonical maturities\n",
        "    if set(df.columns) != set(canonical_columns):\n",
        "        missing = set(canonical_columns) - set(df.columns)\n",
        "        extra = set(df.columns) - set(canonical_columns)\n",
        "        raise ValueError(f\"df_us_yields_raw columns mismatch. Missing: {missing}, Extra: {extra}\")\n",
        "\n",
        "    # Enforce canonical ordering for downstream linear algebra consistency\n",
        "    # This check ensures the DataFrame columns are sorted exactly as required\n",
        "    if list(df.columns) != canonical_columns:\n",
        "        raise ValueError(f\"df_us_yields_raw columns are not in canonical order. Expected {canonical_columns}, got {list(df.columns)}\")\n",
        "\n",
        "    # 5. Dtype check\n",
        "    # Ensure all data is floating point\n",
        "    # We check if the underlying numpy dtype is a float type\n",
        "    if not np.issubdtype(df.values.dtype, np.floating):\n",
        "        raise TypeError(\"df_us_yields_raw must contain only float data.\")\n",
        "\n",
        "    # 6. Finite value check\n",
        "    # Reject infinite values which would break OLS/NLS\n",
        "    if not np.isfinite(df.values).all():\n",
        "        raise ValueError(\"df_us_yields_raw contains non-finite values (inf or -inf).\")\n",
        "\n",
        "    # 7. Date range check\n",
        "    # Verify the data covers the required study period\n",
        "    required_start = pd.Timestamp(min_date)\n",
        "    required_end = pd.Timestamp(max_date)\n",
        "\n",
        "    if df.index.min() > required_start:\n",
        "        raise ValueError(f\"df_us_yields_raw starts after required date {min_date}. Got {df.index.min().date()}\")\n",
        "    if df.index.max() < required_end:\n",
        "        raise ValueError(f\"df_us_yields_raw ends before required date {max_date}. Got {df.index.max().date()}\")\n",
        "\n",
        "    logger.info(\"df_us_yields_raw passed schema validation.\")\n",
        "    return True\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 2: Validate df_us_macro_raw against the U.S. macro predictor schema\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_us_macro_schema(\n",
        "    df: pd.DataFrame,\n",
        "    frequency_map: Dict[str, str],\n",
        "    expected_columns: int = 111\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Validates the U.S. macro predictor DataFrame against schema requirements.\n",
        "\n",
        "    Checks performed:\n",
        "    1. Index checks (Type, Monotonicity, Uniqueness, EOM) - same as yields.\n",
        "    2. Column count is exactly 111.\n",
        "    3. Dtype homogeneity (numeric).\n",
        "    4. Quarterly variable pattern: Variables flagged as 'Q' in frequency_map must\n",
        "       have NaNs at non-quarter-end months (validating raw mixed-frequency state).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw U.S. macro DataFrame.\n",
        "        frequency_map (Dict[str, str]): Mapping of column names to 'M' or 'Q'.\n",
        "        expected_columns (int): Expected number of columns (111).\n",
        "\n",
        "    Returns:\n",
        "        bool: True if validation passes.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If schema constraints are violated.\n",
        "    \"\"\"\n",
        "    # 1. Index checks (reusing logic pattern for consistency)\n",
        "    if not isinstance(df.index, pd.DatetimeIndex):\n",
        "        raise TypeError(f\"df_us_macro_raw index must be DatetimeIndex, got {type(df.index)}\")\n",
        "    if not df.index.is_monotonic_increasing:\n",
        "        raise ValueError(\"df_us_macro_raw index must be strictly monotonic increasing.\")\n",
        "    if df.index.has_duplicates:\n",
        "        raise ValueError(\"df_us_macro_raw index contains duplicate timestamps.\")\n",
        "    if not df.index.is_month_end.all():\n",
        "        raise ValueError(\"df_us_macro_raw contains non-month-end dates.\")\n",
        "\n",
        "    # 2. Column count\n",
        "    # Verify dimensionality matches the paper's 111 predictors\n",
        "    if df.shape[1] != expected_columns:\n",
        "        raise ValueError(f\"df_us_macro_raw must have {expected_columns} columns, got {df.shape[1]}\")\n",
        "\n",
        "    # 3. Dtype homogeneity\n",
        "    # Ensure all columns are numeric (float or int)\n",
        "    # select_dtypes with include=np.number should match the full shape if all are numeric\n",
        "    if df.select_dtypes(include=[np.number]).shape[1] != df.shape[1]:\n",
        "        non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "        raise TypeError(f\"df_us_macro_raw contains non-numeric columns: {non_numeric_cols}\")\n",
        "\n",
        "    # 4. Quarterly variable pattern\n",
        "    # Validate that 'Q' variables follow the expected missingness pattern before interpolation\n",
        "    # Quarter ends are months 3, 6, 9, 12.\n",
        "    for col, freq in frequency_map.items():\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Frequency map contains column '{col}' not found in DataFrame.\")\n",
        "\n",
        "        if freq == 'Q':\n",
        "            # Identify non-quarter-end months\n",
        "            # Month is 1-based (1=Jan, ..., 12=Dec)\n",
        "            non_q_end_mask = ~df.index.month.isin([3, 6, 9, 12])\n",
        "\n",
        "            # Check if values exist in non-quarter-end months\n",
        "            # We expect NaNs here. If count() > 0, we have data where we shouldn't for raw Q series.\n",
        "            if df.loc[non_q_end_mask, col].count() > 0:\n",
        "                raise ValueError(\n",
        "                    f\"Column '{col}' is flagged as Quarterly but contains values at non-quarter-end months. \"\n",
        "                    \"Ensure input is raw mixed-frequency data.\"\n",
        "                )\n",
        "\n",
        "    logger.info(\"df_us_macro_raw passed schema validation.\")\n",
        "    return True\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 3: Validate remaining DataFrames against their schemas\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_supporting_dataframes(\n",
        "    df_benchmark: pd.DataFrame,\n",
        "    df_tic: pd.DataFrame,\n",
        "    df_global_yields: pd.DataFrame,\n",
        "    global_macro_panels: Dict[str, pd.DataFrame],\n",
        "    df_us_macro_ref: pd.DataFrame,\n",
        "    benchmark_cols: List[str],\n",
        "    tic_cols: List[str],\n",
        "    global_yield_cols: List[str],\n",
        "    required_countries: List[str]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Validates the supporting DataFrames for robustness checks and extensions.\n",
        "\n",
        "    Checks performed:\n",
        "    1. df_us_benchmark_yields_raw: 9 specific columns, start date <= 2010-01-31.\n",
        "    2. df_us_tic_raw: 2 specific columns, start date >= 2014-09-30.\n",
        "    3. df_global_yields_raw: 7 specific columns, start date <= 2010-01-31.\n",
        "    4. global_macro_panels: Required keys present, US panel identity check.\n",
        "\n",
        "    Args:\n",
        "        df_benchmark (pd.DataFrame): Benchmark yields.\n",
        "        df_tic (pd.DataFrame): TIC data.\n",
        "        df_global_yields (pd.DataFrame): Global 10Y yields.\n",
        "        global_macro_panels (Dict[str, pd.DataFrame]): Dictionary of country macro panels.\n",
        "        df_us_macro_ref (pd.DataFrame): Reference US macro DF for identity check.\n",
        "        benchmark_cols (List[str]): Required benchmark columns.\n",
        "        tic_cols (List[str]): Required TIC columns.\n",
        "        global_yield_cols (List[str]): Required global yield columns.\n",
        "        required_countries (List[str]): List of required country keys.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if all validations pass.\n",
        "    \"\"\"\n",
        "    # Helper for generic index validation\n",
        "    def _validate_index(df: pd.DataFrame, name: str):\n",
        "        if not isinstance(df.index, pd.DatetimeIndex):\n",
        "            raise TypeError(f\"{name} index must be DatetimeIndex.\")\n",
        "        if not df.index.is_monotonic_increasing:\n",
        "            raise ValueError(f\"{name} index must be monotonic.\")\n",
        "        if not df.index.is_month_end.all():\n",
        "            raise ValueError(f\"{name} contains non-month-end dates.\")\n",
        "\n",
        "    # 1. Benchmark Yields\n",
        "    _validate_index(df_benchmark, \"df_us_benchmark_yields_raw\")\n",
        "    if set(df_benchmark.columns) != set(benchmark_cols):\n",
        "        raise ValueError(f\"df_us_benchmark_yields_raw columns mismatch.\")\n",
        "    if df_benchmark.index.min() > pd.Timestamp(\"2010-01-31\"):\n",
        "        raise ValueError(\"df_us_benchmark_yields_raw starts after 2010-01-31.\")\n",
        "\n",
        "    # 2. TIC Data\n",
        "    _validate_index(df_tic, \"df_us_tic_raw\")\n",
        "    if set(df_tic.columns) != set(tic_cols):\n",
        "        raise ValueError(\"df_us_tic_raw columns mismatch.\")\n",
        "\n",
        "    # TIC availability constraint: Paper says starts in 2014-09.\n",
        "    # We check that the data DOES NOT start earlier than this (implying availability constraint).\n",
        "    if df_tic.index.min() < pd.Timestamp(\"2014-09-30\"):\n",
        "        raise ValueError(\"df_us_tic_raw starts before 2014-09-30, violating availability constraint.\")\n",
        "\n",
        "    # 3. Global Yields\n",
        "    _validate_index(df_global_yields, \"df_global_yields_raw\")\n",
        "    if set(df_global_yields.columns) != set(global_yield_cols):\n",
        "        raise ValueError(\"df_global_yields_raw columns mismatch.\")\n",
        "    if df_global_yields.index.min() > pd.Timestamp(\"2010-01-31\"):\n",
        "        raise ValueError(\"df_global_yields_raw starts after 2010-01-31.\")\n",
        "\n",
        "    # 4. Global Macro Panels\n",
        "    # Check keys\n",
        "    if not set(required_countries).issubset(set(global_macro_panels.keys())):\n",
        "        missing = set(required_countries) - set(global_macro_panels.keys())\n",
        "        raise ValueError(f\"global_macro_panels missing required countries: {missing}\")\n",
        "\n",
        "    # Check US identity\n",
        "    # We verify that the 'US' panel in the global dict is the exact same object\n",
        "    # (or at least equal) to the main US macro dataframe.\n",
        "    # Using .equals() for value equality is safer than 'is' for data pipelines.\n",
        "    if not global_macro_panels[\"US\"].equals(df_us_macro_ref):\n",
        "        raise ValueError(\"global_macro_panels['US'] is not identical to df_us_macro_raw.\")\n",
        "\n",
        "    # Validate each panel's index\n",
        "    for country, panel in global_macro_panels.items():\n",
        "        _validate_index(panel, f\"global_macro_panels['{country}']\")\n",
        "\n",
        "    logger.info(\"Supporting DataFrames passed schema validation.\")\n",
        "    return True\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_all_input_schemas(\n",
        "    df_us_yields_raw: pd.DataFrame,\n",
        "    df_us_macro_raw: pd.DataFrame,\n",
        "    df_us_benchmark_yields_raw: pd.DataFrame,\n",
        "    df_us_tic_raw: pd.DataFrame,\n",
        "    df_global_yields_raw: pd.DataFrame,\n",
        "    global_macro_panels: Dict[str, pd.DataFrame],\n",
        "    study_config: Dict[str, Any],\n",
        "    study_metadata: Dict[str, Any]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Orchestrator function to execute Task 1: Validate schema compliance of all DataFrame inputs.\n",
        "\n",
        "    Args:\n",
        "        df_us_yields_raw: U.S. zero-coupon yields.\n",
        "        df_us_macro_raw: U.S. macro predictors.\n",
        "        df_us_benchmark_yields_raw: U.S. benchmark yields.\n",
        "        df_us_tic_raw: U.S. TIC data.\n",
        "        df_global_yields_raw: Global 10Y yields.\n",
        "        global_macro_panels: Dictionary of global macro panels.\n",
        "        study_config: The main configuration dictionary.\n",
        "        study_metadata: The metadata dictionary containing mappings.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if all validations pass. Raises Exception otherwise.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 1: Input Schema Validation\")\n",
        "\n",
        "    # Extract parameters from config/metadata for validation\n",
        "    us_zero_cols = study_config[\"Global_Settings\"][\"us_zero_maturities\"]\n",
        "    start_date_us = study_config[\"Global_Settings\"][\"start_date_us\"]\n",
        "    end_date_us = study_config[\"Global_Settings\"][\"end_date_us\"]\n",
        "\n",
        "    us_macro_freq_map = study_metadata[\"variable_frequency_map\"][\"US\"]\n",
        "    us_macro_cols_count = study_config[\"Raw_Data_Schemas\"][\"US_Macro\"][\"n_columns\"]\n",
        "\n",
        "    benchmark_cols = study_config[\"Raw_Data_Schemas\"][\"US_Benchmark_Yields\"][\"columns\"]\n",
        "    tic_cols = study_config[\"Raw_Data_Schemas\"][\"US_TIC\"][\"columns\"]\n",
        "    global_yield_cols = study_config[\"Raw_Data_Schemas\"][\"Global_10Y_Yields\"][\"columns\"]\n",
        "    required_countries = study_config[\"Raw_Data_Schemas\"][\"Global_Macro_Panels\"][\"required_keys\"]\n",
        "\n",
        "    # Step 1: Validate US Yields\n",
        "    validate_us_yields_schema(\n",
        "        df_us_yields_raw,\n",
        "        us_zero_cols,\n",
        "        start_date_us,\n",
        "        end_date_us\n",
        "    )\n",
        "\n",
        "    # Step 2: Validate US Macro\n",
        "    validate_us_macro_schema(\n",
        "        df_us_macro_raw,\n",
        "        us_macro_freq_map,\n",
        "        us_macro_cols_count\n",
        "    )\n",
        "\n",
        "    # Step 3: Validate Supporting DataFrames\n",
        "    validate_supporting_dataframes(\n",
        "        df_us_benchmark_yields_raw,\n",
        "        df_us_tic_raw,\n",
        "        df_global_yields_raw,\n",
        "        global_macro_panels,\n",
        "        df_us_macro_raw,\n",
        "        benchmark_cols,\n",
        "        tic_cols,\n",
        "        global_yield_cols,\n",
        "        required_countries\n",
        "    )\n",
        "\n",
        "    logger.info(\"Task 1 Completed Successfully: All inputs comply with canonical schemas.\")\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "G6H7VG5m9Wxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Validate the metadata bundle contains all required fields for pipeline execution\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2: Validate the metadata bundle contains all required fields\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 1: Validate maturity-to-tau mapping completeness and consistency\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_maturity_tau_mapping(\n",
        "    metadata: Dict[str, Any],\n",
        "    canonical_maturities: List[str]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Validates that the maturity-to-tau mapping in the metadata is complete,\n",
        "    uses the correct keys, and uses integer months as units (critical for Nelson-Siegel).\n",
        "\n",
        "    Equation reference:\n",
        "        Nelson-Siegel loadings use tau in months with lambda=0.0609.\n",
        "        tau(\"3M\") must be 3, not 0.25.\n",
        "\n",
        "    Args:\n",
        "        metadata (Dict[str, Any]): The study metadata dictionary.\n",
        "        canonical_maturities (List[str]): List of required maturity labels (e.g., [\"3M\", ...]).\n",
        "\n",
        "    Returns:\n",
        "        bool: True if validation passes.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If mapping is missing, incomplete, or uses incorrect units.\n",
        "    \"\"\"\n",
        "    if \"maturity_to_tau_months\" not in metadata:\n",
        "        raise ValueError(\"Metadata missing required key: 'maturity_to_tau_months'.\")\n",
        "\n",
        "    mapping = metadata[\"maturity_to_tau_months\"]\n",
        "\n",
        "    # Check completeness of keys\n",
        "    missing_keys = set(canonical_maturities) - set(mapping.keys())\n",
        "    if missing_keys:\n",
        "        raise ValueError(f\"maturity_to_tau_months is missing keys: {missing_keys}\")\n",
        "\n",
        "    # Check correctness of values (Must be months, not years)\n",
        "    # We check a few known anchors to detect unit errors\n",
        "    # 3M -> 3, 1Y -> 12, 10Y -> 120\n",
        "    anchors = {\"3M\": 3, \"1Y\": 12, \"10Y\": 120, \"30Y\": 360}\n",
        "\n",
        "    for label, expected_val in anchors.items():\n",
        "        if label in mapping:\n",
        "            actual_val = mapping[label]\n",
        "            # Allow float 3.0 but reject 0.25\n",
        "            if abs(actual_val - expected_val) > 0.1:\n",
        "                raise ValueError(\n",
        "                    f\"Incorrect tau value for {label}. Expected {expected_val} (months), \"\n",
        "                    f\"got {actual_val}. Ensure units are months.\"\n",
        "                )\n",
        "\n",
        "    # Ensure all values are positive numbers\n",
        "    for k, v in mapping.items():\n",
        "        if not (isinstance(v, (int, float)) and v > 0):\n",
        "             raise ValueError(f\"Invalid tau value for {k}: {v}. Must be positive number.\")\n",
        "\n",
        "    logger.info(\"Maturity-to-tau mapping validated successfully.\")\n",
        "    return True\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 2: Validate unit convention and bps conversion rule\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_unit_conventions(metadata: Dict[str, Any]) -> bool:\n",
        "    \"\"\"\n",
        "    Validates that yield unit conventions and RMSFE reporting units are explicitly defined\n",
        "    and consistent.\n",
        "\n",
        "    Requirements:\n",
        "        - yield_units must be 'pct_points' or 'decimals'.\n",
        "        - rmsfe_units must be 'bps'.\n",
        "        - pct_points_to_bps_multiplier must be consistent (100 for pct_points).\n",
        "\n",
        "    Args:\n",
        "        metadata (Dict[str, Any]): The study metadata dictionary.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if validation passes.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If conventions are missing or inconsistent.\n",
        "    \"\"\"\n",
        "    required_keys = [\"yield_units\", \"rmsfe_units\", \"pct_points_to_bps_multiplier\"]\n",
        "    for key in required_keys:\n",
        "        if key not in metadata:\n",
        "            raise ValueError(f\"Metadata missing required key: '{key}'.\")\n",
        "\n",
        "    yield_units = metadata[\"yield_units\"]\n",
        "    rmsfe_units = metadata[\"rmsfe_units\"]\n",
        "    multiplier = metadata[\"pct_points_to_bps_multiplier\"]\n",
        "\n",
        "    # Validate allowed values\n",
        "    allowed_yield_units = {\"pct_points\", \"decimals\"}\n",
        "    if yield_units not in allowed_yield_units:\n",
        "        raise ValueError(f\"yield_units must be one of {allowed_yield_units}, got '{yield_units}'.\")\n",
        "\n",
        "    if rmsfe_units != \"bps\":\n",
        "        raise ValueError(f\"rmsfe_units must be 'bps', got '{rmsfe_units}'.\")\n",
        "\n",
        "    # Validate consistency\n",
        "    if yield_units == \"pct_points\":\n",
        "        if multiplier != 100.0:\n",
        "            raise ValueError(f\"For yield_units='pct_points', multiplier must be 100.0, got {multiplier}.\")\n",
        "    elif yield_units == \"decimals\":\n",
        "        if multiplier != 10000.0:\n",
        "            raise ValueError(f\"For yield_units='decimals', multiplier must be 10000.0, got {multiplier}.\")\n",
        "\n",
        "    logger.info(f\"Unit conventions validated: Yields in {yield_units}, RMSFE in {rmsfe_units}.\")\n",
        "    return True\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 3: Validate variable frequency maps and missing-data policies\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_policies_and_maps(\n",
        "    metadata: Dict[str, Any],\n",
        "    us_macro_columns: List[str],\n",
        "    global_macro_panels_keys: List[str]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Validates existence and completeness of variable frequency maps and missing data policies.\n",
        "\n",
        "    Checks:\n",
        "        - 'variable_frequency_map' exists.\n",
        "        - 'US' map covers all US macro columns.\n",
        "        - Maps exist for all global panels.\n",
        "        - 'missing_data_policy' exists and defines rules for 'yields' and 'macro'.\n",
        "        - 'provenance_ids' exists.\n",
        "\n",
        "    Args:\n",
        "        metadata (Dict[str, Any]): The study metadata dictionary.\n",
        "        us_macro_columns (List[str]): List of column names in the US macro DataFrame.\n",
        "        global_macro_panels_keys (List[str]): List of country keys for global panels.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if validation passes.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If maps/policies are missing or incomplete.\n",
        "    \"\"\"\n",
        "    # 1. Variable Frequency Maps\n",
        "    if \"variable_frequency_map\" not in metadata:\n",
        "        raise ValueError(\"Metadata missing 'variable_frequency_map'.\")\n",
        "\n",
        "    freq_maps = metadata[\"variable_frequency_map\"]\n",
        "\n",
        "    # Check US map completeness\n",
        "    if \"US\" not in freq_maps:\n",
        "        raise ValueError(\"variable_frequency_map missing 'US' key.\")\n",
        "\n",
        "    us_map = freq_maps[\"US\"]\n",
        "    missing_cols = set(us_macro_columns) - set(us_map.keys())\n",
        "    if missing_cols:\n",
        "        # Don't list all if too many, just first few\n",
        "        missing_list = list(missing_cols)[:5]\n",
        "        raise ValueError(f\"US frequency map is missing columns: {missing_list}...\")\n",
        "\n",
        "    # Check allowed values in US map\n",
        "    allowed_freqs = {\"M\", \"Q\"}\n",
        "    invalid_freqs = set(us_map.values()) - allowed_freqs\n",
        "    if invalid_freqs:\n",
        "        raise ValueError(f\"US frequency map contains invalid frequencies: {invalid_freqs}. Must be 'M' or 'Q'.\")\n",
        "\n",
        "    # Check Global maps existence\n",
        "    # We expect a map for every country in the global panels list\n",
        "    missing_countries = set(global_macro_panels_keys) - set(freq_maps.keys())\n",
        "    if missing_countries:\n",
        "        raise ValueError(f\"variable_frequency_map missing keys for countries: {missing_countries}\")\n",
        "\n",
        "    # 2. Missing Data Policies\n",
        "    if \"missing_data_policy\" not in metadata:\n",
        "        raise ValueError(\"Metadata missing 'missing_data_policy'.\")\n",
        "\n",
        "    policy = metadata[\"missing_data_policy\"]\n",
        "    if \"yields\" not in policy:\n",
        "        raise ValueError(\"missing_data_policy missing 'yields' rule.\")\n",
        "    if \"macro\" not in policy:\n",
        "        raise ValueError(\"missing_data_policy missing 'macro' rule.\")\n",
        "\n",
        "    # 3. Provenance IDs\n",
        "    if \"provenance_ids\" not in metadata:\n",
        "        raise ValueError(\"Metadata missing 'provenance_ids' (required for auditability).\")\n",
        "\n",
        "    logger.info(\"Policies and frequency maps validated successfully.\")\n",
        "    return True\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_metadata_bundle(\n",
        "    study_metadata: Dict[str, Any],\n",
        "    study_config: Dict[str, Any],\n",
        "    us_macro_columns: List[str]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 2: Validate metadata bundle completeness and consistency.\n",
        "\n",
        "    Args:\n",
        "        study_metadata (Dict[str, Any]): The metadata bundle to validate.\n",
        "        study_config (Dict[str, Any]): The main study configuration (for canonical lists).\n",
        "        us_macro_columns (List[str]): List of columns from the actual US macro DataFrame\n",
        "                                      (to validate map coverage).\n",
        "\n",
        "    Returns:\n",
        "        bool: True if all validations pass.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 2: Metadata Bundle Validation\")\n",
        "\n",
        "    canonical_maturities = study_config[\"Global_Settings\"][\"us_zero_maturities\"]\n",
        "    global_panel_keys = study_config[\"Raw_Data_Schemas\"][\"Global_Macro_Panels\"][\"required_keys\"]\n",
        "\n",
        "    # Step 1: Validate Maturity-Tau Mapping\n",
        "    validate_maturity_tau_mapping(study_metadata, canonical_maturities)\n",
        "\n",
        "    # Step 2: Validate Unit Conventions\n",
        "    validate_unit_conventions(study_metadata)\n",
        "\n",
        "    # Step 3: Validate Policies and Maps\n",
        "    validate_policies_and_maps(study_metadata, us_macro_columns, global_panel_keys)\n",
        "\n",
        "    logger.info(\"Task 2 Completed Successfully: Metadata bundle is valid.\")\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "sxT0NprKAQGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Validate STUDY_CONFIG contains no unresolved placeholders that block deterministic replication\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Validate STUDY_CONFIG contains no unresolved placeholders\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 1: Identify all placeholder fields in STUDY_CONFIG\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def find_placeholders(config: Dict[str, Any]) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Recursively traverses the configuration dictionary to identify fields containing\n",
        "    placeholder sentinels that block deterministic replication.\n",
        "\n",
        "    Sentinels checked:\n",
        "        - \"USER_DEFINED_REQUIRED_FOR_EXACT_REPLICATION\"\n",
        "        - \"AUTHOR_PROVIDED_REQUIRED\"\n",
        "        - \"LIBRARY_DEFAULT_OR_USER_DEFINED\"\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[str, str]]: A list of (path, value) tuples for every found placeholder.\n",
        "                               Path is dot-separated (e.g., \"Section.SubSection.Key\").\n",
        "    \"\"\"\n",
        "    placeholders = [\n",
        "        \"USER_DEFINED_REQUIRED_FOR_EXACT_REPLICATION\",\n",
        "        \"AUTHOR_PROVIDED_REQUIRED\",\n",
        "        \"LIBRARY_DEFAULT_OR_USER_DEFINED\"\n",
        "    ]\n",
        "    found = []\n",
        "\n",
        "    def _recurse(obj: Any, path: str):\n",
        "        if isinstance(obj, dict):\n",
        "            for k, v in obj.items():\n",
        "                new_path = f\"{path}.{k}\" if path else k\n",
        "                _recurse(v, new_path)\n",
        "        elif isinstance(obj, list):\n",
        "            for i, item in enumerate(obj):\n",
        "                new_path = f\"{path}[{i}]\"\n",
        "                _recurse(item, new_path)\n",
        "        elif isinstance(obj, str):\n",
        "            if obj in placeholders:\n",
        "                found.append((path, obj))\n",
        "\n",
        "    _recurse(config, \"\")\n",
        "    return found\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 2: Resolve or explicitly lock each placeholder to a concrete value\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def resolve_placeholders(config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Creates a deep copy of the configuration and resolves all known placeholders\n",
        "    to concrete, deterministic values required for the replication protocol.\n",
        "\n",
        "    Resolutions applied:\n",
        "        - CUSUM Specification: \"OLS_residuals_constant_only\"\n",
        "        - PELT Kernel: \"rpt_rbf_default\"\n",
        "        - Interpolation Endpoint: \"nan_at_endpoints\"\n",
        "        - RF Seeds: Fixed list of 10 integers [42, ..., 2718]\n",
        "        - RF CV Iterations: 50\n",
        "        - RF Hyperparameter Space: Concrete grid for n_estimators, max_depth, max_features, min_samples_leaf\n",
        "        - AFTER EWMA Decay: 0.97\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The raw configuration with placeholders.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A new configuration dictionary with all placeholders resolved.\n",
        "    \"\"\"\n",
        "    resolved_config = copy.deepcopy(config)\n",
        "\n",
        "    # Define the resolution map based on the paths identified in the config structure\n",
        "    # Note: We access the dictionary directly to ensure we are modifying the structure correctly.\n",
        "\n",
        "    # 1. Preprocessing - Break Detection\n",
        "    if \"Preprocessing_Params\" in resolved_config:\n",
        "        pp = resolved_config[\"Preprocessing_Params\"]\n",
        "\n",
        "        if \"Break_Detection\" in pp:\n",
        "            bd = pp[\"Break_Detection\"]\n",
        "            if \"stage_1\" in bd:\n",
        "                bd[\"stage_1\"][\"cusum_model_specification\"] = \"OLS_residuals_constant_only\"\n",
        "            if \"stage_2\" in bd:\n",
        "                bd[\"stage_2\"][\"rbf_kernel_setting\"] = \"rpt_rbf_default\"\n",
        "\n",
        "        # 2. Preprocessing - Interpolation\n",
        "        if \"Quarterly_To_Monthly\" in pp:\n",
        "            pp[\"Quarterly_To_Monthly\"][\"endpoint_policy\"] = \"nan_at_endpoints\"\n",
        "\n",
        "    # 3. Model Architectures - Random Forest\n",
        "    if \"Model_Architectures\" in resolved_config:\n",
        "        ma = resolved_config[\"Model_Architectures\"]\n",
        "        if \"Random_Forest\" in ma:\n",
        "            rf = ma[\"Random_Forest\"]\n",
        "\n",
        "            # Seeds\n",
        "            rf[\"rf_seeds_main_required_for_exact_replication\"] = [\n",
        "                42, 101, 999, 1234, 5678, 2023, 8888, 777, 314, 2718\n",
        "            ]\n",
        "\n",
        "            # CV Settings\n",
        "            if \"cv\" in rf:\n",
        "                rf[\"cv\"][\"n_iter\"] = 50\n",
        "                # Define concrete hyperparameter space\n",
        "                rf[\"cv\"][\"hyperparam_space\"] = {\n",
        "                    \"n_estimators\": [100, 200, 500],\n",
        "                    \"max_depth\": [10, 20, 30, None],\n",
        "                    \"max_features\": [\"sqrt\", \"log2\", 0.33],\n",
        "                    \"min_samples_leaf\": [1, 5, 10]\n",
        "                }\n",
        "\n",
        "    # 4. Forecast Combination - AFTER\n",
        "    if \"Forecast_Combination\" in resolved_config:\n",
        "        fc = resolved_config[\"Forecast_Combination\"]\n",
        "        if \"AFTER\" in fc:\n",
        "            fc[\"AFTER\"][\"ewma_decay_lambda\"] = 0.97\n",
        "\n",
        "    # Verify resolutions\n",
        "    remaining = find_placeholders(resolved_config)\n",
        "    if remaining:\n",
        "        raise ValueError(f\"Failed to resolve all placeholders. Remaining: {remaining}\")\n",
        "\n",
        "    logger.info(\"All configuration placeholders resolved to concrete values.\")\n",
        "    return resolved_config\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 3: Freeze the resolved configuration and log it for audit\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def freeze_and_hash_config(config: Dict[str, Any]) -> Tuple[Dict[str, Any], str]:\n",
        "    \"\"\"\n",
        "    'Freezes' the configuration (by convention, treating it as immutable from here on)\n",
        "    and generates a SHA256 hash of its content to serve as a unique Audit ID.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The resolved configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, Any], str]: The config and its hex digest hash.\n",
        "    \"\"\"\n",
        "    # Sort keys to ensure deterministic JSON serialization\n",
        "    config_json = json.dumps(config, sort_keys=True, default=str)\n",
        "    config_hash = hashlib.sha256(config_json.encode('utf-8')).hexdigest()\n",
        "\n",
        "    logger.info(f\"Configuration frozen. Audit Hash ID: {config_hash}\")\n",
        "\n",
        "    # In a strict implementation, we might wrap this in a read-only proxy,\n",
        "    # but for this pipeline, we rely on the orchestrator not modifying it.\n",
        "    return config, config_hash\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def resolve_and_freeze_config(raw_config: Dict[str, Any]) -> Tuple[Dict[str, Any], str]:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 3: Validate, Resolve, and Freeze the study configuration.\n",
        "\n",
        "    Args:\n",
        "        raw_config (Dict[str, Any]): The initial configuration dictionary containing placeholders.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, Any], str]: The resolved, frozen configuration and its unique hash.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 3: Configuration Resolution and Freezing\")\n",
        "\n",
        "    # Step 1: Identify placeholders (Audit step)\n",
        "    placeholders = find_placeholders(raw_config)\n",
        "    if placeholders:\n",
        "        logger.info(f\"Found {len(placeholders)} placeholders to resolve.\")\n",
        "        for p in placeholders:\n",
        "            logger.debug(f\"Placeholder at {p[0]}: {p[1]}\")\n",
        "    else:\n",
        "        logger.info(\"No placeholders found in raw config.\")\n",
        "\n",
        "    # Step 2: Resolve placeholders\n",
        "    resolved_config = resolve_placeholders(raw_config)\n",
        "\n",
        "    # Step 3: Freeze and Hash\n",
        "    frozen_config, config_hash = freeze_and_hash_config(resolved_config)\n",
        "\n",
        "    logger.info(\"Task 3 Completed Successfully.\")\n",
        "    return frozen_config, config_hash\n"
      ],
      "metadata": {
        "id": "oqudtAmHBT28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Cleanse and align all DataFrame indices to a unified monthly end-of-month calendar\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4: Cleanse and align all DataFrame indices\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 1 & 2: Sort, Deduplicate, and Normalize Timestamps\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def normalize_dataframe_index(\n",
        "    df: pd.DataFrame,\n",
        "    name: str,\n",
        "    deduplication_policy: str = \"keep_last\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Sorts, deduplicates, and normalizes the index of a DataFrame to calendar month-end.\n",
        "\n",
        "    Process:\n",
        "    1. Convert index to timezone-naive datetime.\n",
        "    2. Sort index.\n",
        "    3. Shift dates to calendar month-end (e.g., 2023-01-15 -> 2023-01-31).\n",
        "    4. Handle duplicates (potentially created by shifting) using the specified policy.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame.\n",
        "        name (str): Name of the DataFrame for logging.\n",
        "        deduplication_policy (str): Strategy for duplicates ('keep_last', 'keep_first', 'raise').\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The normalized DataFrame.\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        logger.warning(f\"DataFrame {name} is empty. Skipping normalization.\")\n",
        "        return df\n",
        "\n",
        "    # 1. Ensure DatetimeIndex and TZ-naive\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    if df.index.tz is not None:\n",
        "        df.index = df.index.tz_localize(None)\n",
        "        logger.info(f\"[{name}] Converted index to timezone-naive.\")\n",
        "\n",
        "    # 2. Sort Index\n",
        "    if not df.index.is_monotonic_increasing:\n",
        "        df = df.sort_index()\n",
        "        logger.info(f\"[{name}] Sorted index.\")\n",
        "\n",
        "    # 3. Normalize to Month End\n",
        "    # Check if normalization is needed\n",
        "    if not df.index.is_month_end.all():\n",
        "        original_index = df.index.copy()\n",
        "        # MonthEnd(0) moves to the end of the current month if not already there\n",
        "        df.index = df.index + MonthEnd(0)\n",
        "        shifted_count = (df.index != original_index).sum()\n",
        "        logger.info(f\"[{name}] Shifted {shifted_count} dates to calendar month-end.\")\n",
        "\n",
        "    # 4. Deduplicate\n",
        "    # Shifting might have created duplicates (e.g., Jan 15 and Jan 31 both become Jan 31)\n",
        "    if df.index.has_duplicates:\n",
        "        duplicate_count = df.index.duplicated().sum()\n",
        "        logger.warning(f\"[{name}] Found {duplicate_count} duplicate timestamps after normalization.\")\n",
        "\n",
        "        if deduplication_policy == \"keep_last\":\n",
        "            df = df[~df.index.duplicated(keep='last')]\n",
        "            logger.info(f\"[{name}] Dropped duplicates, keeping last observation.\")\n",
        "        elif deduplication_policy == \"keep_first\":\n",
        "            df = df[~df.index.duplicated(keep='first')]\n",
        "            logger.info(f\"[{name}] Dropped duplicates, keeping first observation.\")\n",
        "        elif deduplication_policy == \"raise\":\n",
        "            raise ValueError(f\"[{name}] Duplicate timestamps found and policy is 'raise'.\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown deduplication policy: {deduplication_policy}\")\n",
        "\n",
        "    # Final Validation\n",
        "    if not df.index.is_monotonic_increasing:\n",
        "        raise ValueError(f\"[{name}] Index is not monotonic after normalization.\")\n",
        "    if not df.index.is_month_end.all():\n",
        "        raise ValueError(f\"[{name}] Index contains non-month-end dates after normalization.\")\n",
        "    if df.index.has_duplicates:\n",
        "        raise ValueError(f\"[{name}] Index contains duplicates after deduplication.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 3: Intersect or align calendars across datasets\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def align_datasets_to_study_range(\n",
        "    datasets: Dict[str, pd.DataFrame],\n",
        "    global_start_date: str,\n",
        "    global_end_date: str\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Trims datasets to the global study date range defined in the configuration.\n",
        "    Does NOT force intersection across all datasets (as they have different start dates),\n",
        "    but ensures no data exists outside the study bounds.\n",
        "\n",
        "    Args:\n",
        "        datasets (Dict[str, pd.DataFrame]): Dictionary of normalized DataFrames.\n",
        "        global_start_date (str): Global start date (ISO).\n",
        "        global_end_date (str): Global end date (ISO).\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: Dictionary of trimmed DataFrames.\n",
        "    \"\"\"\n",
        "    aligned_datasets = {}\n",
        "    start_ts = pd.Timestamp(global_start_date)\n",
        "    end_ts = pd.Timestamp(global_end_date)\n",
        "\n",
        "    for name, df in datasets.items():\n",
        "        # Slice to bounds\n",
        "        # We use loc to be inclusive of the bounds if they exist in the index\n",
        "        # Note: The index is already sorted and monotonic from Step 1/2\n",
        "\n",
        "        # Handle case where dataset might start after global start (e.g., TIC)\n",
        "        # We only trim the outer bounds.\n",
        "        mask = (df.index >= start_ts) & (df.index <= end_ts)\n",
        "        trimmed_df = df.loc[mask].copy()\n",
        "\n",
        "        if trimmed_df.empty:\n",
        "            logger.warning(f\"[{name}] Dataset is empty after trimming to {global_start_date}-{global_end_date}.\")\n",
        "        else:\n",
        "            dropped_rows = len(df) - len(trimmed_df)\n",
        "            if dropped_rows > 0:\n",
        "                logger.info(f\"[{name}] Trimmed {dropped_rows} rows outside study bounds.\")\n",
        "\n",
        "        aligned_datasets[name] = trimmed_df\n",
        "\n",
        "    return aligned_datasets\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_and_align_indices(\n",
        "    df_us_yields_raw: pd.DataFrame,\n",
        "    df_us_macro_raw: pd.DataFrame,\n",
        "    df_us_benchmark_yields_raw: pd.DataFrame,\n",
        "    df_us_tic_raw: pd.DataFrame,\n",
        "    df_global_yields_raw: pd.DataFrame,\n",
        "    global_macro_panels: Dict[str, pd.DataFrame],\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, Dict[str, pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 4: Cleanse and align all DataFrame indices.\n",
        "\n",
        "    Args:\n",
        "        df_us_yields_raw: Raw US yields.\n",
        "        df_us_macro_raw: Raw US macro.\n",
        "        df_us_benchmark_yields_raw: Raw US benchmark yields.\n",
        "        df_us_tic_raw: Raw US TIC data.\n",
        "        df_global_yields_raw: Raw Global yields.\n",
        "        global_macro_panels: Raw Global macro panels.\n",
        "        study_config: Frozen configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing cleansed versions of all inputs in the same order.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 4: Index Cleansing and Alignment\")\n",
        "\n",
        "    # Extract settings\n",
        "    # Note: We use a generous start date (2006) for trimming, but individual datasets\n",
        "    # like TIC will naturally start later.\n",
        "    start_date = study_config[\"Global_Settings\"][\"start_date_us\"]\n",
        "    end_date = study_config[\"Global_Settings\"][\"end_date_us\"]\n",
        "\n",
        "    # We assume a default deduplication policy of 'keep_last' as it's standard for\n",
        "    # financial time series (latest revision/value).\n",
        "    dedup_policy = \"keep_last\"\n",
        "\n",
        "    # 1. Normalize individual DataFrames\n",
        "    df_us_yields = normalize_dataframe_index(df_us_yields_raw, \"US_Yields\", dedup_policy)\n",
        "    df_us_macro = normalize_dataframe_index(df_us_macro_raw, \"US_Macro\", dedup_policy)\n",
        "    df_benchmark = normalize_dataframe_index(df_us_benchmark_yields_raw, \"US_Benchmark\", dedup_policy)\n",
        "    df_tic = normalize_dataframe_index(df_us_tic_raw, \"US_TIC\", dedup_policy)\n",
        "    df_global_yields = normalize_dataframe_index(df_global_yields_raw, \"Global_Yields\", dedup_policy)\n",
        "\n",
        "    cleaned_global_panels = {}\n",
        "    for country, df in global_macro_panels.items():\n",
        "        cleaned_global_panels[country] = normalize_dataframe_index(df, f\"Global_Macro_{country}\", dedup_policy)\n",
        "\n",
        "    # 2. Trim to Study Bounds\n",
        "    # We group them to use the helper, then unpack\n",
        "    all_dfs = {\n",
        "        \"US_Yields\": df_us_yields,\n",
        "        \"US_Macro\": df_us_macro,\n",
        "        \"US_Benchmark\": df_benchmark,\n",
        "        \"US_TIC\": df_tic,\n",
        "        \"Global_Yields\": df_global_yields\n",
        "    }\n",
        "    # Add global panels to the batch\n",
        "    for country, df in cleaned_global_panels.items():\n",
        "        all_dfs[f\"Global_Macro_{country}\"] = df\n",
        "\n",
        "    aligned_dfs = align_datasets_to_study_range(all_dfs, \"1990-01-01\", end_date) # Use loose start to allow history, strict end\n",
        "\n",
        "    # Unpack\n",
        "    df_us_yields_final = aligned_dfs[\"US_Yields\"]\n",
        "    df_us_macro_final = aligned_dfs[\"US_Macro\"]\n",
        "    df_benchmark_final = aligned_dfs[\"US_Benchmark\"]\n",
        "    df_tic_final = aligned_dfs[\"US_TIC\"]\n",
        "    df_global_yields_final = aligned_dfs[\"Global_Yields\"]\n",
        "\n",
        "    global_macro_panels_final = {}\n",
        "    for country in global_macro_panels.keys():\n",
        "        global_macro_panels_final[country] = aligned_dfs[f\"Global_Macro_{country}\"]\n",
        "\n",
        "    logger.info(\"Task 4 Completed Successfully: Indices cleansed and aligned.\")\n",
        "\n",
        "    return (\n",
        "        df_us_yields_final,\n",
        "        df_us_macro_final,\n",
        "        df_benchmark_final,\n",
        "        df_tic_final,\n",
        "        df_global_yields_final,\n",
        "        global_macro_panels_final\n",
        "    )\n"
      ],
      "metadata": {
        "id": "GAB0LS4hCoFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Cleanse missingness in the yield panel according to the declared policy\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Cleanse missingness in the yield panel\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 1: Identify dates with incomplete yield cross-sections\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def identify_incomplete_yield_dates(\n",
        "    df: pd.DataFrame,\n",
        "    canonical_maturities: List[str]\n",
        ") -> List[pd.Timestamp]:\n",
        "    \"\"\"\n",
        "    Identifies dates where the yield cross-section is incomplete (missing values).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The yield DataFrame.\n",
        "        canonical_maturities (List[str]): List of required maturity columns.\n",
        "\n",
        "    Returns:\n",
        "        List[pd.Timestamp]: List of dates with missing values.\n",
        "    \"\"\"\n",
        "    # Ensure we are looking at the correct columns\n",
        "    # (Though schema validation Task 1 should have ensured this, we double check subset)\n",
        "    df_subset = df[canonical_maturities]\n",
        "\n",
        "    # Replace infinite values with NaN for counting\n",
        "    df_subset = df_subset.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Count non-NaN values per row\n",
        "    # We require count == len(canonical_maturities)\n",
        "    counts = df_subset.count(axis=1)\n",
        "    required_count = len(canonical_maturities)\n",
        "\n",
        "    incomplete_mask = counts < required_count\n",
        "    incomplete_dates = df.index[incomplete_mask].tolist()\n",
        "\n",
        "    if incomplete_dates:\n",
        "        logger.info(f\"Found {len(incomplete_dates)} dates with incomplete yield cross-sections.\")\n",
        "    else:\n",
        "        logger.info(\"No incomplete yield cross-sections found.\")\n",
        "\n",
        "    return incomplete_dates\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 2: Apply the declared missing-data policy\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def apply_yield_missingness_policy(\n",
        "    df: pd.DataFrame,\n",
        "    policy: str,\n",
        "    incomplete_dates: List[pd.Timestamp]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies the specified missing data policy to the yield DataFrame.\n",
        "\n",
        "    Supported policies:\n",
        "        - 'drop_date': Remove rows with any missing values.\n",
        "        - 'impute_linear': Interpolate missing values linearly over time.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame.\n",
        "        policy (str): Policy string from metadata.\n",
        "        incomplete_dates (List[pd.Timestamp]): List of dates identified as incomplete.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Cleansed DataFrame.\n",
        "    \"\"\"\n",
        "    if not incomplete_dates:\n",
        "        return df.copy()\n",
        "\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # Ensure infs are NaNs before processing\n",
        "    df_clean = df_clean.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    if policy == \"drop_date\":\n",
        "        initial_len = len(df_clean)\n",
        "        df_clean = df_clean.dropna(how='any')\n",
        "        dropped_count = initial_len - len(df_clean)\n",
        "        logger.info(f\"Applied 'drop_date' policy. Dropped {dropped_count} rows.\")\n",
        "\n",
        "    elif policy == \"impute_linear\":\n",
        "        # Time-based interpolation\n",
        "        df_clean = df_clean.interpolate(method='time', limit_direction='both')\n",
        "\n",
        "        # Check if any NaNs remain (e.g., if dataset starts/ends with NaNs and limit_direction didn't catch it)\n",
        "        remaining_nans = df_clean.isna().sum().sum()\n",
        "        if remaining_nans > 0:\n",
        "            logger.warning(f\"Linear imputation left {remaining_nans} NaNs (likely at endpoints). Dropping remaining.\")\n",
        "            df_clean = df_clean.dropna(how='any')\n",
        "\n",
        "        logger.info(\"Applied 'impute_linear' policy.\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported yield missingness policy: '{policy}'. Supported: 'drop_date', 'impute_linear'.\")\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 3: Verify the cleansed yield panel is complete\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def verify_yield_panel_completeness(df: pd.DataFrame) -> bool:\n",
        "    \"\"\"\n",
        "    Verifies that the yield panel contains no missing values and checks index contiguity.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Cleansed DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if valid.\n",
        "    \"\"\"\n",
        "    # 1. Check for NaNs\n",
        "    nan_count = df.isna().sum().sum()\n",
        "    if nan_count > 0:\n",
        "        raise ValueError(f\"Yield panel still contains {nan_count} NaNs after cleansing.\")\n",
        "\n",
        "    # 2. Check for Infs\n",
        "    if not np.isfinite(df.values).all():\n",
        "        raise ValueError(\"Yield panel contains infinite values after cleansing.\")\n",
        "\n",
        "    # 3. Check Contiguity (Advisory)\n",
        "    # We infer frequency. If it returns None, it implies gaps or irregular spacing.\n",
        "    inferred_freq = pd.infer_freq(df.index)\n",
        "\n",
        "    if inferred_freq is None:\n",
        "        # Check explicitly for gaps if we expect monthly data\n",
        "        # Calculate differences between consecutive dates\n",
        "        diffs = df.index.to_series().diff().dropna()\n",
        "        # Assuming monthly data, diffs should be roughly 28-31 days\n",
        "        # We flag if any gap is significantly larger (e.g., > 32 days)\n",
        "        gaps = diffs[diffs > pd.Timedelta(days=32)]\n",
        "\n",
        "        if not gaps.empty:\n",
        "            logger.warning(f\"Yield panel index is NOT contiguous. Found {len(gaps)} gaps > 32 days. \"\n",
        "                           \"This is expected if 'drop_date' was used, but note that RMSFE counts will be affected.\")\n",
        "        else:\n",
        "            logger.info(\"Yield panel index appears contiguous (no large gaps), though strict freq inference failed.\")\n",
        "    else:\n",
        "        logger.info(f\"Yield panel index is contiguous with frequency: {inferred_freq}\")\n",
        "\n",
        "    return True\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_yield_panel(\n",
        "    df_us_yields: pd.DataFrame,\n",
        "    study_config: Dict[str, Any],\n",
        "    study_metadata: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 5: Cleanse missingness in the yield panel.\n",
        "\n",
        "    Args:\n",
        "        df_us_yields: The normalized US yields DataFrame (from Task 4).\n",
        "        study_config: Frozen configuration.\n",
        "        study_metadata: Metadata containing missingness policy.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The cleansed, complete yield panel.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 5: Yield Panel Missingness Cleansing\")\n",
        "\n",
        "    canonical_maturities = study_config[\"Global_Settings\"][\"us_zero_maturities\"]\n",
        "    policy = study_metadata[\"missing_data_policy\"][\"yields\"]\n",
        "\n",
        "    # Step 1: Identify\n",
        "    incomplete_dates = identify_incomplete_yield_dates(df_us_yields, canonical_maturities)\n",
        "\n",
        "    # Step 2: Apply Policy\n",
        "    df_clean = apply_yield_missingness_policy(df_us_yields, policy, incomplete_dates)\n",
        "\n",
        "    # Step 3: Verify\n",
        "    verify_yield_panel_completeness(df_clean)\n",
        "\n",
        "    logger.info(\"Task 5 Completed Successfully: Yield panel is clean.\")\n",
        "    return df_clean\n"
      ],
      "metadata": {
        "id": "BU2Ou2M2EKJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Cleanse macro panels by interpolating quarterly variables to monthly frequency\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6: Cleanse macro panels by interpolating quarterly variables\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 1: Identify quarterly variables using the frequency map\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def identify_variable_frequencies(\n",
        "    df: pd.DataFrame,\n",
        "    freq_map: Dict[str, str],\n",
        "    panel_name: str\n",
        ") -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Partitions DataFrame columns into Monthly and Quarterly lists based on the frequency map.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The macro DataFrame.\n",
        "        freq_map (Dict[str, str]): Mapping of column names to 'M' or 'Q'.\n",
        "        panel_name (str): Name of the panel for logging.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[str], List[str]]: (monthly_cols, quarterly_cols).\n",
        "    \"\"\"\n",
        "    monthly_cols = []\n",
        "    quarterly_cols = []\n",
        "\n",
        "    # Validate coverage first\n",
        "    missing_in_map = set(df.columns) - set(freq_map.keys())\n",
        "    if missing_in_map:\n",
        "        raise ValueError(f\"[{panel_name}] Columns missing from frequency map: {missing_in_map}\")\n",
        "\n",
        "    for col in df.columns:\n",
        "        freq = freq_map[col]\n",
        "        if freq == 'Q':\n",
        "            quarterly_cols.append(col)\n",
        "        elif freq == 'M':\n",
        "            monthly_cols.append(col)\n",
        "        else:\n",
        "            raise ValueError(f\"[{panel_name}] Unknown frequency '{freq}' for column '{col}'. Must be 'M' or 'Q'.\")\n",
        "\n",
        "    logger.info(f\"[{panel_name}] Identified {len(monthly_cols)} Monthly and {len(quarterly_cols)} Quarterly variables.\")\n",
        "    return monthly_cols, quarterly_cols\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 2: Apply linear interpolation to convert quarterly to monthly\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def interpolate_quarterly_variables(\n",
        "    df: pd.DataFrame,\n",
        "    quarterly_cols: List[str],\n",
        "    endpoint_policy: str,\n",
        "    panel_name: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Interpolates quarterly variables to monthly frequency using linear interpolation.\n",
        "\n",
        "    Equation:\n",
        "        Z_m = Q_1 + (m - m_1)/(m_2 - m_1) * (Q_2 - Q_1)\n",
        "        Implemented via pandas interpolate(method='time').\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame with NaNs in quarterly columns.\n",
        "        quarterly_cols (List[str]): List of columns to interpolate.\n",
        "        endpoint_policy (str): Policy for handling start/end NaNs ('nan_at_endpoints', 'forward_fill', etc.).\n",
        "        panel_name (str): Name for logging.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with interpolated values.\n",
        "    \"\"\"\n",
        "    if not quarterly_cols:\n",
        "        return df.copy()\n",
        "\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # Check pre-interpolation NaNs\n",
        "    pre_nans = df_clean[quarterly_cols].isna().sum().sum()\n",
        "\n",
        "    if endpoint_policy == \"nan_at_endpoints\":\n",
        "        df_clean[quarterly_cols] = df_clean[quarterly_cols].interpolate(method='time', limit_area='inside')\n",
        "    elif endpoint_policy == \"forward_fill_endpoints\":\n",
        "        # Interpolate inside first\n",
        "        df_clean[quarterly_cols] = df_clean[quarterly_cols].interpolate(method='time', limit_area='inside')\n",
        "        # Then ffill/bfill\n",
        "        df_clean[quarterly_cols] = df_clean[quarterly_cols].ffill().bfill()\n",
        "    else:\n",
        "        # Default to inside only if policy unknown, but log warning\n",
        "        logger.warning(f\"[{panel_name}] Unknown endpoint policy '{endpoint_policy}'. Defaulting to 'nan_at_endpoints'.\")\n",
        "        df_clean[quarterly_cols] = df_clean[quarterly_cols].interpolate(method='time', limit_area='inside')\n",
        "\n",
        "    post_nans = df_clean[quarterly_cols].isna().sum().sum()\n",
        "    filled_count = pre_nans - post_nans\n",
        "\n",
        "    logger.info(f\"[{panel_name}] Interpolated {filled_count} values across {len(quarterly_cols)} columns.\")\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 3: Verify interpolation completeness\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def verify_interpolation_status(\n",
        "    df: pd.DataFrame,\n",
        "    quarterly_cols: List[str],\n",
        "    panel_name: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Verifies the status of quarterly columns after interpolation.\n",
        "    Logs remaining NaNs (endpoints).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Processed DataFrame.\n",
        "        quarterly_cols (List[str]): List of quarterly columns.\n",
        "        panel_name (str): Name for logging.\n",
        "    \"\"\"\n",
        "    if not quarterly_cols:\n",
        "        return\n",
        "\n",
        "    remaining_nans = df[quarterly_cols].isna().sum()\n",
        "    total_remaining = remaining_nans.sum()\n",
        "\n",
        "    if total_remaining > 0:\n",
        "        # This is expected under 'nan_at_endpoints', but we log it for audit.\n",
        "        # We check if any column is *completely* NaN (which would be an error in data provision)\n",
        "        all_nan_cols = remaining_nans[remaining_nans == len(df)].index.tolist()\n",
        "        if all_nan_cols:\n",
        "            logger.error(f\"[{panel_name}] The following quarterly columns are ALL NaN (no anchors for interpolation): {all_nan_cols}\")\n",
        "            # In a strict pipeline, we might raise here, but we'll let the missingness policy handle it downstream.\n",
        "\n",
        "        logger.info(f\"[{panel_name}] {total_remaining} NaNs remain in quarterly columns (likely endpoints).\")\n",
        "    else:\n",
        "        logger.info(f\"[{panel_name}] No NaNs remain in quarterly columns.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def interpolate_macro_panels(\n",
        "    df_us_macro: pd.DataFrame,\n",
        "    global_macro_panels: Dict[str, pd.DataFrame],\n",
        "    study_metadata: Dict[str, Any],\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 6: Interpolate quarterly variables in all macro panels.\n",
        "\n",
        "    Args:\n",
        "        df_us_macro: US macro DataFrame.\n",
        "        global_macro_panels: Dictionary of global macro DataFrames.\n",
        "        study_metadata: Metadata containing frequency maps.\n",
        "        study_config: Configuration containing endpoint policy.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]: (Cleaned US Macro, Cleaned Global Panels).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 6: Macro Panel Interpolation\")\n",
        "\n",
        "    # Extract policy\n",
        "    # Note: Task 3 resolved placeholders, so this key should exist and be concrete.\n",
        "    endpoint_policy = study_config[\"Preprocessing_Params\"][\"Quarterly_To_Monthly\"][\"endpoint_policy\"]\n",
        "\n",
        "    # 1. Process US Macro\n",
        "    us_freq_map = study_metadata[\"variable_frequency_map\"][\"US\"]\n",
        "    us_m_cols, us_q_cols = identify_variable_frequencies(df_us_macro, us_freq_map, \"US_Macro\")\n",
        "\n",
        "    df_us_macro_clean = interpolate_quarterly_variables(\n",
        "        df_us_macro, us_q_cols, endpoint_policy, \"US_Macro\"\n",
        "    )\n",
        "    verify_interpolation_status(df_us_macro_clean, us_q_cols, \"US_Macro\")\n",
        "\n",
        "    # 2. Process Global Panels\n",
        "    global_panels_clean = {}\n",
        "    for country, df in global_macro_panels.items():\n",
        "        # Skip US in global loop if it's just a reference to the main US df,\n",
        "        # but if it's a separate object in the dict, process it.\n",
        "        # The metadata should have a map for every country key.\n",
        "        if country not in study_metadata[\"variable_frequency_map\"]:\n",
        "             raise ValueError(f\"Missing frequency map for country: {country}\")\n",
        "\n",
        "        freq_map = study_metadata[\"variable_frequency_map\"][country]\n",
        "        m_cols, q_cols = identify_variable_frequencies(df, freq_map, f\"Global_{country}\")\n",
        "\n",
        "        df_clean = interpolate_quarterly_variables(\n",
        "            df, q_cols, endpoint_policy, f\"Global_{country}\"\n",
        "        )\n",
        "        verify_interpolation_status(df_clean, q_cols, f\"Global_{country}\")\n",
        "\n",
        "        global_panels_clean[country] = df_clean\n",
        "\n",
        "    logger.info(\"Task 6 Completed Successfully: All macro panels interpolated.\")\n",
        "    return df_us_macro_clean, global_panels_clean\n"
      ],
      "metadata": {
        "id": "JPvcVA6TGQTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Conduct structural break detection Stage 1: CUSUM test per maturity\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7: Conduct structural break detection Stage 1: CUSUM test\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 1 & 2: Define specification and Execute CUSUM test\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def execute_cusum_test_per_maturity(\n",
        "    df_yields: pd.DataFrame,\n",
        "    significance_level: float = 0.05,\n",
        "    model_spec: str = \"OLS_residuals_constant_only\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Executes the CUSUM test for parameter stability on each maturity's yield series.\n",
        "\n",
        "    Algorithm:\n",
        "    1. Fit OLS model y_t = mu + epsilon_t (constant mean).\n",
        "    2. Compute recursive residuals.\n",
        "    3. Calculate CUSUM statistic and p-value (Brown, Durbin, Evans 1975).\n",
        "\n",
        "    Args:\n",
        "        df_yields (pd.DataFrame): Cleansed yield panel.\n",
        "        significance_level (float): Alpha for rejection (default 0.05).\n",
        "        model_spec (str): Specification of the underlying model (must be 'OLS_residuals_constant_only').\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Results table with columns ['Maturity', 'Statistic', 'P-Value', 'Rejected'].\n",
        "    \"\"\"\n",
        "    if model_spec != \"OLS_residuals_constant_only\":\n",
        "        raise ValueError(f\"Unsupported CUSUM model specification: {model_spec}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Ensure we have a constant term for OLS\n",
        "    # We fit y = c, so X is just a column of ones.\n",
        "    X = np.ones((len(df_yields), 1))\n",
        "\n",
        "    for maturity in df_yields.columns:\n",
        "        y = df_yields[maturity].values\n",
        "\n",
        "        # 1. Fit OLS\n",
        "        # We use statsmodels OLS\n",
        "        model = sm.OLS(y, X)\n",
        "        res = model.fit()\n",
        "\n",
        "        # 2. Execute CUSUM on residuals\n",
        "        # breaks_cusumolsresid returns:\n",
        "        # (cusum_statistic, p_value, critical_values)\n",
        "        # Note: The function expects OLS residuals.\n",
        "        # Reference: statsmodels.stats.diagnostic.breaks_cusumolsresid\n",
        "\n",
        "        # We use ddof=1 for the residual variance calculation in the test if needed,\n",
        "        # but the function handles it.\n",
        "        stat, p_value, crit_vals = breaks_cusumolsresid(res.resid)\n",
        "\n",
        "        # 3. Evaluate\n",
        "        # The test is: Null = parameters are stable.\n",
        "        # Reject if p_value < significance_level.\n",
        "        rejected = p_value < significance_level\n",
        "\n",
        "        results.append({\n",
        "            \"Maturity\": maturity,\n",
        "            \"Statistic\": stat,\n",
        "            \"P-Value\": p_value,\n",
        "            \"Rejected\": rejected\n",
        "        })\n",
        "\n",
        "        # Log highly significant rejections as noted in manuscript (p < 0.001)\n",
        "        if p_value < 0.001:\n",
        "            logger.debug(f\"[{maturity}] Strong CUSUM rejection (p < 0.001). Stat: {stat:.4f}\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 3: Store CUSUM results\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def format_cusum_results(results_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Formats and validates the CUSUM results DataFrame.\n",
        "\n",
        "    Args:\n",
        "        results_df (pd.DataFrame): Raw results.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Formatted results indexed by Maturity.\n",
        "    \"\"\"\n",
        "    df = results_df.set_index(\"Maturity\")\n",
        "\n",
        "    # Validate that we have results for all maturities\n",
        "    # (Implicitly checked by loop, but good for audit)\n",
        "\n",
        "    rejection_rate = df[\"Rejected\"].mean()\n",
        "    logger.info(f\"CUSUM Test Complete. Rejection rate: {rejection_rate:.1%} (Target: ~100% per manuscript).\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_cusum_tests(\n",
        "    df_yields: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 7: CUSUM structural break tests.\n",
        "\n",
        "    Args:\n",
        "        df_yields: Cleansed yield DataFrame.\n",
        "        study_config: Frozen configuration.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: CUSUM results table.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 7: CUSUM Structural Break Detection\")\n",
        "\n",
        "    # Extract parameters\n",
        "    # Note: Task 3 resolved placeholders.\n",
        "    params = study_config[\"Preprocessing_Params\"][\"Break_Detection\"][\"stage_1\"]\n",
        "    model_spec = params[\"cusum_model_specification\"]\n",
        "    alpha = params[\"significance_level\"]\n",
        "\n",
        "    # Execute\n",
        "    raw_results = execute_cusum_test_per_maturity(df_yields, alpha, model_spec)\n",
        "\n",
        "    # Format\n",
        "    final_results = format_cusum_results(raw_results)\n",
        "\n",
        "    logger.info(\"Task 7 Completed Successfully.\")\n",
        "    return final_results\n"
      ],
      "metadata": {
        "id": "5ugIs95QHqLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Conduct structural break detection Stage 2: PELT change point detection per maturity\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: Conduct structural break detection Stage 2: PELT\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 1 & 2: Define configuration and Execute PELT\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def execute_pelt_detection_per_maturity(\n",
        "    df_yields: pd.DataFrame,\n",
        "    penalty: float = 10.0,\n",
        "    kernel_setting: str = \"rpt_rbf_default\"\n",
        ") -> Dict[str, List[pd.Timestamp]]:\n",
        "    \"\"\"\n",
        "    Executes the PELT change point detection algorithm with RBF cost on each maturity.\n",
        "\n",
        "    Algorithm:\n",
        "    1. Use ruptures.Pelt with model=\"rbf\".\n",
        "    2. Fit on univariate yield series.\n",
        "    3. Predict breakpoints with penalty beta=10.\n",
        "\n",
        "    Args:\n",
        "        df_yields (pd.DataFrame): Cleansed yield panel.\n",
        "        penalty (float): Penalty parameter beta (default 10).\n",
        "        kernel_setting (str): Configuration for RBF kernel (default 'rpt_rbf_default').\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, List[pd.Timestamp]]: Dictionary mapping maturity to list of break dates.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # Validate kernel setting (placeholder resolution check)\n",
        "    if kernel_setting != \"rpt_rbf_default\":\n",
        "        # If specific params were required, we'd parse them here.\n",
        "        # For now, we rely on ruptures default heuristics which are standard.\n",
        "        pass\n",
        "\n",
        "    for maturity in df_yields.columns:\n",
        "        # Convert to numpy array (n_samples, n_dims=1)\n",
        "        signal = df_yields[maturity].values.reshape(-1, 1)\n",
        "\n",
        "        # 1. Instantiate PELT with RBF cost\n",
        "        algo = rpt.Pelt(model=\"rbf\").fit(signal)\n",
        "\n",
        "        # 2. Predict\n",
        "        # pen=10 matches the manuscript's beta=10\n",
        "        bkps_indices = algo.predict(pen=penalty)\n",
        "\n",
        "        # 3. Convert indices to dates\n",
        "        # ruptures returns the index of the *end* of a segment.\n",
        "        # e.g., if break is at index k, it means the segment is [..., k-1].\n",
        "        # The change point is effectively at k.\n",
        "        # The last element of bkps_indices is always n_samples (end of signal).\n",
        "\n",
        "        break_dates = []\n",
        "        for idx in bkps_indices:\n",
        "            if idx < len(df_yields):\n",
        "                # Map index to date\n",
        "                date = df_yields.index[idx]\n",
        "                break_dates.append(date)\n",
        "\n",
        "        results[maturity] = break_dates\n",
        "\n",
        "        logger.debug(f\"[{maturity}] Found {len(break_dates)} breaks.\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 3: Produce breakpoints table\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def format_breakpoints_table(\n",
        "    raw_breaks: Dict[str, List[pd.Timestamp]],\n",
        "    max_columns: int = 6\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Formats the raw breakpoints into a structured table matching the manuscript appendix.\n",
        "\n",
        "    Args:\n",
        "        raw_breaks (Dict): Dictionary of break dates.\n",
        "        max_columns (int): Number of break columns to produce (default 6).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Table with columns 'Break 1' ... 'Break 6'.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    maturities = list(raw_breaks.keys())\n",
        "\n",
        "    for maturity in maturities:\n",
        "        dates = raw_breaks[maturity]\n",
        "        # Format dates as strings (YYYY-MM-DD) or keep as Timestamp\n",
        "        # We'll use strings for the table display\n",
        "        date_strs = [d.strftime('%Y-%m-%d') for d in dates]\n",
        "\n",
        "        # Pad or truncate to max_columns\n",
        "        if len(date_strs) < max_columns:\n",
        "            padded = date_strs + [\"--\"] * (max_columns - len(date_strs))\n",
        "        else:\n",
        "            padded = date_strs[:max_columns]\n",
        "\n",
        "        row = {\"Maturity\": maturity}\n",
        "        for i, d in enumerate(padded):\n",
        "            row[f\"Break {i+1}\"] = d\n",
        "\n",
        "        data.append(row)\n",
        "\n",
        "    df = pd.DataFrame(data).set_index(\"Maturity\")\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_pelt_detection(\n",
        "    df_yields: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, List[pd.Timestamp]]]:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 8: PELT change point detection.\n",
        "\n",
        "    Args:\n",
        "        df_yields: Cleansed yield DataFrame.\n",
        "        study_config: Frozen configuration.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict]: (Formatted Table, Raw Breakpoints Dict).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 8: PELT Change Point Detection\")\n",
        "\n",
        "    # Extract parameters\n",
        "    params = study_config[\"Preprocessing_Params\"][\"Break_Detection\"][\"stage_2\"]\n",
        "    penalty = params[\"penalty_beta\"]\n",
        "    kernel_setting = params[\"rbf_kernel_setting\"]\n",
        "\n",
        "    # Execute\n",
        "    raw_breaks = execute_pelt_detection_per_maturity(df_yields, penalty, kernel_setting)\n",
        "\n",
        "    # Format\n",
        "    table = format_breakpoints_table(raw_breaks)\n",
        "\n",
        "    logger.info(\"Task 8 Completed Successfully.\")\n",
        "    return table, raw_breaks\n"
      ],
      "metadata": {
        "id": "fmbtzCpeI73_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Construct the Nelson–Siegel loading matrix using λ=0.0609 and τ in months\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9: Construct the Nelson–Siegel loading matrix\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 1: Define the Nelson–Siegel loading functions\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def nelson_siegel_loadings(tau: float, lam: float = 0.0609) -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Computes the Level, Slope, and Curvature factor loadings for a given maturity tau.\n",
        "\n",
        "    Equations:\n",
        "        L1(tau) = 1\n",
        "        L2(tau) = (1 - exp(-lambda * tau)) / (lambda * tau)\n",
        "        L3(tau) = ((1 - exp(-lambda * tau)) / (lambda * tau)) - exp(-lambda * tau)\n",
        "\n",
        "    Args:\n",
        "        tau (float): Maturity in months.\n",
        "        lam (float): Decay parameter lambda (default 0.0609).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float, float]: (L1, L2, L3) loadings.\n",
        "    \"\"\"\n",
        "    if tau <= 0:\n",
        "        raise ValueError(f\"Maturity tau must be positive. Got {tau}.\")\n",
        "\n",
        "    # L1: Level\n",
        "    l1 = 1.0\n",
        "\n",
        "    # Precompute common term exp(-lambda * tau)\n",
        "    exp_term = np.exp(-lam * tau)\n",
        "\n",
        "    # Precompute term (1 - exp) / (lambda * tau)\n",
        "    # Note: For very small x, (1-exp(-x))/x -> 1.\n",
        "    # With tau >= 3 months, lambda*tau >= 0.18, so standard float arithmetic is stable.\n",
        "    # We use standard formula.\n",
        "    slope_term = (1.0 - exp_term) / (lam * tau)\n",
        "\n",
        "    # L2: Slope\n",
        "    l2 = slope_term\n",
        "\n",
        "    # L3: Curvature\n",
        "    l3 = slope_term - exp_term\n",
        "\n",
        "    return l1, l2, l3\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 2: Build the N x 3 loading matrix L\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def build_loading_matrix(\n",
        "    canonical_maturities: List[str],\n",
        "    maturity_to_tau_map: Dict[str, int],\n",
        "    lam: float = 0.0609\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Constructs the N x 3 Nelson-Siegel loading matrix L.\n",
        "\n",
        "    Rows correspond to the canonical maturities in order.\n",
        "    Columns correspond to Level, Slope, Curvature factors.\n",
        "\n",
        "    Args:\n",
        "        canonical_maturities (List[str]): Ordered list of maturity labels.\n",
        "        maturity_to_tau_map (Dict[str, int]): Mapping from label to tau (months).\n",
        "        lam (float): Decay parameter.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The (N, 3) loading matrix.\n",
        "    \"\"\"\n",
        "    n_maturities = len(canonical_maturities)\n",
        "    L = np.zeros((n_maturities, 3))\n",
        "\n",
        "    for i, mat_label in enumerate(canonical_maturities):\n",
        "        if mat_label not in maturity_to_tau_map:\n",
        "            raise ValueError(f\"Maturity label '{mat_label}' not found in tau mapping.\")\n",
        "\n",
        "        tau = maturity_to_tau_map[mat_label]\n",
        "        l1, l2, l3 = nelson_siegel_loadings(tau, lam)\n",
        "\n",
        "        L[i, 0] = l1\n",
        "        L[i, 1] = l2\n",
        "        L[i, 2] = l3\n",
        "\n",
        "    return L\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 3: Verify qualitative properties\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def verify_loading_matrix_properties(L: np.ndarray, taus: List[int]) -> bool:\n",
        "    \"\"\"\n",
        "    Verifies the mathematical properties of the Nelson-Siegel loading matrix.\n",
        "\n",
        "    Checks:\n",
        "        1. Column 0 (Level) is all 1s.\n",
        "        2. Column 1 (Slope) is monotonically decreasing (since tau increases).\n",
        "        3. Column 2 (Curvature) exhibits a hump shape (increases then decreases)\n",
        "           or at least is positive for relevant maturities.\n",
        "\n",
        "    Args:\n",
        "        L (np.ndarray): The loading matrix.\n",
        "        taus (List[int]): The list of tau values corresponding to rows.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if valid.\n",
        "    \"\"\"\n",
        "    # 1. Level check\n",
        "    if not np.allclose(L[:, 0], 1.0):\n",
        "        raise ValueError(\"Loading matrix column 0 (Level) must be all 1s.\")\n",
        "\n",
        "    # 2. Slope check (L2)\n",
        "    # L2 = (1-exp(-x))/x is a decreasing function of x for x > 0.\n",
        "    # Since our rows are ordered by increasing tau, L2 should be decreasing.\n",
        "    l2 = L[:, 1]\n",
        "    if not np.all(np.diff(l2) < 0):\n",
        "        # Allow for small numerical noise if strictly monotonic is too harsh,\n",
        "        # but analytically it is strictly decreasing.\n",
        "        # We'll raise if it increases.\n",
        "        raise ValueError(\"Loading matrix column 1 (Slope) must be monotonically decreasing with maturity.\")\n",
        "\n",
        "    # 3. Curvature check (L3)\n",
        "    # L3 starts at 0 (at tau=0), increases to a peak, then decays.\n",
        "    # With lambda=0.0609, peak is around tau = 1.79 / 0.0609 ~= 29 months.\n",
        "    # So for maturities 3M..30M (peak) it increases, then decreases for 30M..360M.\n",
        "    # We check if values are generally positive (standard NS property).\n",
        "    l3 = L[:, 2]\n",
        "    if not np.all(l3 > 0):\n",
        "        raise ValueError(\"Loading matrix column 2 (Curvature) contains non-positive values.\")\n",
        "\n",
        "    # Check hump shape roughly: max should not be at the ends (3M or 30Y)\n",
        "    # Peak index\n",
        "    peak_idx = np.argmax(l3)\n",
        "    if peak_idx == 0 or peak_idx == len(l3) - 1:\n",
        "        logger.warning(f\"Curvature loading peak is at index {peak_idx} (boundary). Expected interior peak.\")\n",
        "    else:\n",
        "        logger.info(f\"Curvature loading peaks at index {peak_idx} (approx {taus[peak_idx]} months).\")\n",
        "\n",
        "    logger.info(\"Nelson-Siegel loading matrix properties verified.\")\n",
        "    return True\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def construct_dns_loading_matrix(\n",
        "    study_config: Dict[str, Any],\n",
        "    study_metadata: Dict[str, Any]\n",
        ") -> Tuple[np.ndarray, List[str]]:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 9: Construct the Nelson-Siegel loading matrix.\n",
        "\n",
        "    Args:\n",
        "        study_config: Frozen configuration.\n",
        "        study_metadata: Metadata with tau mapping.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, List[str]]: (Loading Matrix L, Ordered Maturity Labels).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 9: Constructing Nelson-Siegel Loading Matrix\")\n",
        "\n",
        "    # Extract parameters\n",
        "    canonical_maturities = study_config[\"Global_Settings\"][\"us_zero_maturities\"]\n",
        "    maturity_to_tau = study_metadata[\"maturity_to_tau_months\"]\n",
        "    lam = study_config[\"Model_Architectures\"][\"DNS\"][\"lambda_decay\"]\n",
        "\n",
        "    # Build Matrix\n",
        "    L = build_loading_matrix(canonical_maturities, maturity_to_tau, lam)\n",
        "\n",
        "    # Verify\n",
        "    taus = [maturity_to_tau[m] for m in canonical_maturities]\n",
        "    verify_loading_matrix_properties(L, taus)\n",
        "\n",
        "    logger.info(f\"Task 9 Completed Successfully. Constructed L with shape {L.shape}.\")\n",
        "    return L, canonical_maturities\n"
      ],
      "metadata": {
        "id": "Sbx__zcUKRPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Extract DNS factors $\\beta_t$ at each time $t$ by cross-sectional regression\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10: Extract DNS factors beta_t\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 1 & 2: Estimate beta_t via cross-sectional least squares\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def estimate_dns_factors_vectorized(\n",
        "    df_yields: pd.DataFrame,\n",
        "    L: np.ndarray,\n",
        "    canonical_maturities: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Estimates the Dynamic Nelson-Siegel factors (Level, Slope, Curvature) for each time t\n",
        "    using cross-sectional Ordinary Least Squares.\n",
        "\n",
        "    Equation:\n",
        "        y_t = L * beta_t + epsilon_t\n",
        "        beta_hat_t = (L.T * L)^(-1) * L.T * y_t\n",
        "\n",
        "    Implementation:\n",
        "        We use a vectorized approach: Beta_hat = Y * pinv(L.T)\n",
        "        where Y is (T x N) and L is (N x 3).\n",
        "\n",
        "    Args:\n",
        "        df_yields (pd.DataFrame): Cleansed yield panel (T x N).\n",
        "        L (np.ndarray): Loading matrix (N x 3).\n",
        "        canonical_maturities (List[str]): Ordered list of maturities corresponding to L rows.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Factor time series (T x 3) with columns ['Beta1', 'Beta2', 'Beta3'].\n",
        "    \"\"\"\n",
        "    # 1. Validate Alignment\n",
        "    if list(df_yields.columns) != canonical_maturities:\n",
        "        raise ValueError(\"Yield DataFrame columns do not match the canonical maturity order of L.\")\n",
        "\n",
        "    if L.shape[0] != len(canonical_maturities):\n",
        "        raise ValueError(f\"Loading matrix L has {L.shape[0]} rows, expected {len(canonical_maturities)}.\")\n",
        "\n",
        "    # 2. Compute Pseudoinverse of L\n",
        "    # L is fixed, so we compute (L.T L)^-1 L.T once.\n",
        "    # Shape: (3, N)\n",
        "    # We use pinv for stability, though L should be full rank.\n",
        "    L_pinv = np.linalg.pinv(L)\n",
        "\n",
        "    # 3. Vectorized Estimation\n",
        "    # Y: (T, N)\n",
        "    # Beta: (T, 3) = Y @ L_pinv.T\n",
        "    # Check dimensions: (T, N) @ (N, 3) -> (T, 3)\n",
        "    Y = df_yields.values\n",
        "    Beta = Y @ L_pinv.T\n",
        "\n",
        "    # 4. Construct DataFrame\n",
        "    df_factors = pd.DataFrame(\n",
        "        Beta,\n",
        "        index=df_yields.index,\n",
        "        columns=[\"Beta1\", \"Beta2\", \"Beta3\"]\n",
        "    )\n",
        "\n",
        "    return df_factors\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 3: Validate factor time series\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_dns_factors(df_factors: pd.DataFrame) -> bool:\n",
        "    \"\"\"\n",
        "    Validates the estimated DNS factors.\n",
        "\n",
        "    Checks:\n",
        "        - No NaNs or Infs.\n",
        "        - Shape is (T, 3).\n",
        "        - Plausibility: Beta1 (Level) should be roughly within yield ranges (0-20%).\n",
        "          Beta2 (Slope) and Beta3 (Curvature) should be finite.\n",
        "\n",
        "    Args:\n",
        "        df_factors (pd.DataFrame): Estimated factors.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if valid.\n",
        "    \"\"\"\n",
        "    # 1. Check Integrity\n",
        "    if df_factors.isna().sum().sum() > 0:\n",
        "        raise ValueError(\"Estimated DNS factors contain NaNs.\")\n",
        "\n",
        "    if not np.isfinite(df_factors.values).all():\n",
        "        raise ValueError(\"Estimated DNS factors contain infinite values.\")\n",
        "\n",
        "    # 2. Plausibility Check (Advisory)\n",
        "    # Beta1 is the long-term factor (level). It shouldn't be wildly negative or > 100%.\n",
        "    # We log warnings but don't halt, as extreme market conditions exist.\n",
        "    b1_mean = df_factors[\"Beta1\"].mean()\n",
        "    if not (-5.0 < b1_mean < 20.0): # Broad range for yields in %\n",
        "        logger.warning(f\"Beta1 (Level) mean is {b1_mean:.2f}, which is outside typical range (-5 to 20). Check units.\")\n",
        "\n",
        "    logger.info(f\"DNS Factors estimated for {len(df_factors)} periods.\")\n",
        "    return True\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def extract_dns_factors(\n",
        "    df_yields: pd.DataFrame,\n",
        "    L: np.ndarray,\n",
        "    canonical_maturities: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 10: Extract DNS factors.\n",
        "\n",
        "    Args:\n",
        "        df_yields: Cleansed yield DataFrame.\n",
        "        L: Loading matrix.\n",
        "        canonical_maturities: List of maturity labels.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Time series of Beta1, Beta2, Beta3.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 10: Extracting DNS Factors\")\n",
        "\n",
        "    # Estimate\n",
        "    df_factors = estimate_dns_factors_vectorized(df_yields, L, canonical_maturities)\n",
        "\n",
        "    # Validate\n",
        "    validate_dns_factors(df_factors)\n",
        "\n",
        "    logger.info(\"Task 10 Completed Successfully.\")\n",
        "    return df_factors\n"
      ],
      "metadata": {
        "id": "b8qvLy2jMLB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Estimate rolling VAR(1) on DNS factors with window w=60 months\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11: Estimate rolling VAR(1) on DNS factors\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 1 & 2: Estimate VAR(1) parameters rolling\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def estimate_rolling_var1(\n",
        "    df_factors: pd.DataFrame,\n",
        "    window_size: int = 60\n",
        ") -> Dict[pd.Timestamp, Dict[str, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Estimates a VAR(1) model on DNS factors using a rolling window.\n",
        "\n",
        "    Model:\n",
        "        beta_{t+1} = c + Phi * beta_t + eta_t\n",
        "\n",
        "    Estimation:\n",
        "        For each forecast origin t (where t is the last observed data point in the window),\n",
        "        we use data from t-w+1 to t.\n",
        "        Regression pairs: (beta_s, beta_{s+1}) for s in [t-w+1, t-1].\n",
        "        Sample size for regression: w-1 transitions.\n",
        "\n",
        "    Args:\n",
        "        df_factors (pd.DataFrame): Time series of factors (T x 3).\n",
        "        window_size (int): Rolling window size w (default 60).\n",
        "\n",
        "    Returns:\n",
        "        Dict[pd.Timestamp, Dict]: Mapping from forecast origin t to parameter dict\n",
        "                                  {'c': (3,), 'Phi': (3,3), 'stable': bool}.\n",
        "    \"\"\"\n",
        "    factors = df_factors.values\n",
        "    dates = df_factors.index\n",
        "    n_obs, n_vars = factors.shape\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # We need at least window_size observations to form the first window\n",
        "    # The first forecast origin is at index window_size - 1 (0-based)\n",
        "    start_idx = window_size - 1\n",
        "\n",
        "    for t_idx in range(start_idx, n_obs):\n",
        "        # Define window indices\n",
        "        # Window includes data from (t_idx - window_size + 1) to t_idx\n",
        "        # Let's say window_size=60. If t_idx=59, window is 0..59.\n",
        "\n",
        "        # Extract window data\n",
        "        window_data = factors[t_idx - window_size + 1 : t_idx + 1]\n",
        "\n",
        "        # Prepare regression matrices\n",
        "        # Y = beta_{s+1}, X = [1, beta_s]\n",
        "        # s goes from start of window to end-1\n",
        "\n",
        "        Y = window_data[1:]      # beta_{t-w+2} ... beta_t\n",
        "        X_lag = window_data[:-1] # beta_{t-w+1} ... beta_{t-1}\n",
        "\n",
        "        # Add intercept column to X\n",
        "        n_samples = X_lag.shape[0]\n",
        "        X = np.column_stack([np.ones(n_samples), X_lag])\n",
        "\n",
        "        # Estimate B = (X.T X)^-1 X.T Y\n",
        "        # B shape: (1+3, 3) -> (Intercept + Phi.T)\n",
        "        # We use lstsq for stability\n",
        "        B, residuals, rank, s = np.linalg.lstsq(X, Y, rcond=None)\n",
        "\n",
        "        # Extract parameters\n",
        "        # B[0, :] is intercept c (1 x 3) -> (3,)\n",
        "        # B[1:, :] is Phi.T (3 x 3) -> Phi is B[1:, :].T\n",
        "        c_hat = B[0, :]\n",
        "        Phi_hat = B[1:, :].T\n",
        "\n",
        "        # Check stability\n",
        "        eigenvalues = np.linalg.eigvals(Phi_hat)\n",
        "        max_eig = np.max(np.abs(eigenvalues))\n",
        "        is_stable = max_eig < 1.0\n",
        "\n",
        "        # Store results keyed by the forecast origin date (timestamp at t_idx)\n",
        "        origin_date = dates[t_idx]\n",
        "        results[origin_date] = {\n",
        "            \"c\": c_hat,\n",
        "            \"Phi\": Phi_hat,\n",
        "            \"stable\": is_stable,\n",
        "            \"max_eig\": max_eig\n",
        "        }\n",
        "\n",
        "        if not is_stable:\n",
        "            logger.debug(f\"Unstable VAR at {origin_date.date()}. Max eigenvalue: {max_eig:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 3: Verify stability (Orchestrator helper)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def log_stability_summary(results: Dict[pd.Timestamp, Dict[str, Any]]) -> None:\n",
        "    \"\"\"\n",
        "    Logs summary statistics about VAR stability.\n",
        "\n",
        "    Args:\n",
        "        results: The dictionary of estimation results.\n",
        "    \"\"\"\n",
        "    total = len(results)\n",
        "    unstable = sum(1 for res in results.values() if not res[\"stable\"])\n",
        "\n",
        "    if total > 0:\n",
        "        rate = unstable / total\n",
        "        logger.info(f\"VAR(1) Estimation Complete. Windows: {total}. Unstable: {unstable} ({rate:.1%}).\")\n",
        "    else:\n",
        "        logger.warning(\"No VAR(1) windows estimated (insufficient data?).\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def estimate_rolling_dns_var(\n",
        "    df_factors: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[pd.Timestamp, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 11: Estimate rolling VAR(1) on DNS factors.\n",
        "\n",
        "    Args:\n",
        "        df_factors: DataFrame of DNS factors.\n",
        "        study_config: Frozen configuration.\n",
        "\n",
        "    Returns:\n",
        "        Dict mapping forecast origin date to parameter dictionary.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 11: Rolling VAR(1) Estimation\")\n",
        "\n",
        "    # Extract parameters\n",
        "    window_size = study_config[\"Global_Settings\"][\"dns_fadns_window_w\"]\n",
        "\n",
        "    # Execute\n",
        "    var_results = estimate_rolling_var1(df_factors, window_size)\n",
        "\n",
        "    # Log summary\n",
        "    log_stability_summary(var_results)\n",
        "\n",
        "    logger.info(\"Task 11 Completed Successfully.\")\n",
        "    return var_results\n"
      ],
      "metadata": {
        "id": "xBY0ujjFNXOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: Compute DNS multi-horizon factor and yield forecasts for h ∈ {1,3,6,9,12}\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12: Compute DNS multi-horizon forecasts\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 1: Compute h-step-ahead factor forecasts\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_factor_forecasts(\n",
        "    current_beta: np.ndarray,\n",
        "    c: np.ndarray,\n",
        "    Phi: np.ndarray,\n",
        "    horizons: List[int]\n",
        ") -> Dict[int, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Computes recursive VAR forecasts for specified horizons.\n",
        "\n",
        "    Equation:\n",
        "        beta_{t+h|t} = (sum_{j=0}^{h-1} Phi^j * c) + Phi^h * beta_t\n",
        "\n",
        "    Args:\n",
        "        current_beta (np.ndarray): Factor vector at time t (3,).\n",
        "        c (np.ndarray): VAR intercept (3,).\n",
        "        Phi (np.ndarray): VAR coefficient matrix (3, 3).\n",
        "        horizons (List[int]): List of forecast horizons h.\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, np.ndarray]: Mapping h -> forecasted beta vector.\n",
        "    \"\"\"\n",
        "    forecasts = {}\n",
        "\n",
        "    # Precompute powers of Phi and the cumulative sum of intercepts\n",
        "    # We iterate to be efficient:\n",
        "    # Term 1 (intercept part): I_h = Phi * I_{h-1} + c, with I_0 = 0 (conceptually) -> I_1 = c\n",
        "    # Term 2 (autoregressive part): A_h = Phi * A_{h-1}, with A_0 = beta_t\n",
        "    # Initialize for h=0\n",
        "    term_intercept = np.zeros_like(c)\n",
        "    term_autoreg = current_beta.copy()\n",
        "\n",
        "    max_h = max(horizons)\n",
        "\n",
        "    for h in range(1, max_h + 1):\n",
        "        # Update terms\n",
        "        # I_h = c + Phi * I_{h-1}\n",
        "        # Note: The formula sum_{j=0}^{h-1} Phi^j c can be written recursively:\n",
        "        # S_1 = c\n",
        "        # S_2 = c + Phi*c\n",
        "        # S_h = c + Phi*S_{h-1}\n",
        "        term_intercept = c + Phi @ term_intercept\n",
        "\n",
        "        # A_h = Phi * A_{h-1}\n",
        "        term_autoreg = Phi @ term_autoreg\n",
        "\n",
        "        # Forecast = I_h + A_h\n",
        "        forecast = term_intercept + term_autoreg\n",
        "\n",
        "        if h in horizons:\n",
        "            forecasts[h] = forecast\n",
        "\n",
        "    return forecasts\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 2 & 3: Map to yields and store\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def generate_yield_forecasts_panel(\n",
        "    var_results: Dict[pd.Timestamp, Dict[str, np.ndarray]],\n",
        "    df_factors: pd.DataFrame,\n",
        "    L: np.ndarray,\n",
        "    canonical_maturities: List[str],\n",
        "    horizons: List[int]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates the full panel of yield forecasts.\n",
        "\n",
        "    Args:\n",
        "        var_results: Dictionary of VAR parameters keyed by origin date.\n",
        "        df_factors: Time series of historical factors.\n",
        "        L: Loading matrix.\n",
        "        canonical_maturities: List of maturity labels.\n",
        "        horizons: List of forecast horizons.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Long-format DataFrame with columns:\n",
        "                      ['OriginDate', 'TargetDate', 'Horizon', 'Maturity', 'Forecast']\n",
        "    \"\"\"\n",
        "    records = []\n",
        "\n",
        "    # Iterate over forecast origins where we have VAR estimates\n",
        "    sorted_origins = sorted(var_results.keys())\n",
        "\n",
        "    for origin_date in sorted_origins:\n",
        "        # Retrieve parameters\n",
        "        params = var_results[origin_date]\n",
        "        c = params[\"c\"]\n",
        "        Phi = params[\"Phi\"]\n",
        "\n",
        "        # Retrieve current factor state beta_t\n",
        "        # Ensure we have the factor at this date\n",
        "        if origin_date not in df_factors.index:\n",
        "            logger.warning(f\"Factor data missing for origin {origin_date}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        beta_t = df_factors.loc[origin_date].values\n",
        "\n",
        "        # 1. Compute Factor Forecasts\n",
        "        factor_forecasts = compute_factor_forecasts(beta_t, c, Phi, horizons)\n",
        "\n",
        "        # 2. Map to Yields and Store\n",
        "        for h, beta_pred in factor_forecasts.items():\n",
        "            # Compute yield vector: y_hat = L * beta_hat\n",
        "            # (15, 3) @ (3,) -> (15,)\n",
        "            y_pred = L @ beta_pred\n",
        "\n",
        "            # Calculate target date\n",
        "            # Use MonthEnd to ensure alignment\n",
        "            target_date = origin_date + DateOffset(months=h) + MonthEnd(0)\n",
        "\n",
        "            for i, maturity in enumerate(canonical_maturities):\n",
        "                records.append({\n",
        "                    \"OriginDate\": origin_date,\n",
        "                    \"TargetDate\": target_date,\n",
        "                    \"Horizon\": h,\n",
        "                    \"Maturity\": maturity,\n",
        "                    \"Forecast\": y_pred[i]\n",
        "                })\n",
        "\n",
        "    # Create DataFrame\n",
        "    df_forecasts = pd.DataFrame(records)\n",
        "\n",
        "    # Set types\n",
        "    df_forecasts[\"OriginDate\"] = pd.to_datetime(df_forecasts[\"OriginDate\"])\n",
        "    df_forecasts[\"TargetDate\"] = pd.to_datetime(df_forecasts[\"TargetDate\"])\n",
        "    df_forecasts[\"Horizon\"] = df_forecasts[\"Horizon\"].astype(int)\n",
        "\n",
        "    return df_forecasts\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def generate_dns_forecasts(\n",
        "    var_results: Dict[pd.Timestamp, Dict[str, np.ndarray]],\n",
        "    df_factors: pd.DataFrame,\n",
        "    L: np.ndarray,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 12: Generate DNS forecasts.\n",
        "\n",
        "    Args:\n",
        "        var_results: VAR estimation results.\n",
        "        df_factors: Historical factors.\n",
        "        L: Loading matrix.\n",
        "        study_config: Frozen configuration.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Forecast panel.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 12: Generating DNS Forecasts\")\n",
        "\n",
        "    # Extract parameters\n",
        "    horizons = study_config[\"Global_Settings\"][\"forecast_horizons\"]\n",
        "    canonical_maturities = study_config[\"Global_Settings\"][\"us_zero_maturities\"]\n",
        "\n",
        "    # Execute\n",
        "    df_forecasts = generate_yield_forecasts_panel(\n",
        "        var_results,\n",
        "        df_factors,\n",
        "        L,\n",
        "        canonical_maturities,\n",
        "        horizons\n",
        "    )\n",
        "\n",
        "    logger.info(f\"Task 12 Completed Successfully. Generated {len(df_forecasts)} forecast records.\")\n",
        "    return df_forecasts\n"
      ],
      "metadata": {
        "id": "fmoHuU1qO0gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Compute DNS forecast errors and RMSFE table (bps)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Compute DNS forecast errors and RMSFE table\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 1: Compute forecast errors\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_forecast_errors(\n",
        "    df_forecasts: pd.DataFrame,\n",
        "    df_actuals: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes forecast errors by aligning forecasts with realized values.\n",
        "\n",
        "    Equation:\n",
        "        e_{t+h} = y_{t+h} - y_hat_{t+h|t}\n",
        "\n",
        "    Args:\n",
        "        df_forecasts (pd.DataFrame): Long-format forecasts with 'TargetDate', 'Maturity', 'Forecast'.\n",
        "        df_actuals (pd.DataFrame): Realized yields (index=Date, columns=Maturities).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with 'Error' column added, rows with missing actuals dropped.\n",
        "    \"\"\"\n",
        "    # Prepare actuals for merge: melt to long format\n",
        "    # Index is Date, columns are maturities\n",
        "    df_actuals_long = df_actuals.reset_index().melt(\n",
        "        id_vars=[df_actuals.index.name or \"index\"],\n",
        "        var_name=\"Maturity\",\n",
        "        value_name=\"Actual\"\n",
        "    )\n",
        "    # Rename index col to TargetDate for merge\n",
        "    df_actuals_long = df_actuals_long.rename(columns={df_actuals_long.columns[0]: \"TargetDate\"})\n",
        "\n",
        "    # Ensure types match\n",
        "    df_actuals_long[\"TargetDate\"] = pd.to_datetime(df_actuals_long[\"TargetDate\"])\n",
        "\n",
        "    # Merge\n",
        "    # We use inner join: we only evaluate where we have both a forecast and a realization\n",
        "    df_merged = pd.merge(\n",
        "        df_forecasts,\n",
        "        df_actuals_long,\n",
        "        on=[\"TargetDate\", \"Maturity\"],\n",
        "        how=\"inner\"\n",
        "    )\n",
        "\n",
        "    # Compute Error\n",
        "    df_merged[\"Error\"] = df_merged[\"Actual\"] - df_merged[\"Forecast\"]\n",
        "\n",
        "    # Log drop count\n",
        "    total_forecasts = len(df_forecasts)\n",
        "    matched_forecasts = len(df_merged)\n",
        "    if matched_forecasts < total_forecasts:\n",
        "        logger.info(f\"Dropped {total_forecasts - matched_forecasts} forecasts due to missing realizations (end of sample or gaps).\")\n",
        "\n",
        "    return df_merged\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 2 & 3: Compute RMSFE and Format\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def calculate_rmsfe_table(\n",
        "    df_errors: pd.DataFrame,\n",
        "    multiplier: float = 100.0\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculates RMSFE in basis points and formats as a table.\n",
        "\n",
        "    Equation:\n",
        "        RMSFE = sqrt(mean(Error^2)) * multiplier\n",
        "\n",
        "    Args:\n",
        "        df_errors (pd.DataFrame): DataFrame containing 'Error', 'Horizon', 'Maturity'.\n",
        "        multiplier (float): Unit conversion multiplier (default 100 for % -> bps).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Pivot table (Rows=Maturity, Cols=Horizon) of RMSFE in bps.\n",
        "    \"\"\"\n",
        "    # Group by Horizon and Maturity\n",
        "    grouped = df_errors.groupby([\"Horizon\", \"Maturity\"])[\"Error\"]\n",
        "\n",
        "    # Compute MSE then Sqrt\n",
        "    mse = grouped.apply(lambda x: np.mean(x**2))\n",
        "    rmsfe = np.sqrt(mse) * multiplier\n",
        "\n",
        "    # Log sample sizes for audit\n",
        "    counts = grouped.count()\n",
        "    min_count = counts.min()\n",
        "    logger.info(f\"RMSFE computed. Min sample size per cell: {min_count}.\")\n",
        "\n",
        "    # Pivot to table\n",
        "    # Reset index to make columns accessible\n",
        "    rmsfe_df = rmsfe.reset_index()\n",
        "\n",
        "    # Pivot: Index=Maturity, Columns=Horizon, Values=Error\n",
        "    rmsfe_table = rmsfe_df.pivot(index=\"Maturity\", columns=\"Horizon\", values=\"Error\")\n",
        "\n",
        "    # Reorder rows (Maturities) and columns (Horizons)\n",
        "    # We rely on the caller to enforce canonical order if needed,\n",
        "    # but we can sort columns (Horizons are ints) easily.\n",
        "    rmsfe_table = rmsfe_table.sort_index(axis=1)\n",
        "\n",
        "    # For rows, we might want canonical order.\n",
        "    # We'll leave it lexicographical or data-driven here,\n",
        "    # but the orchestrator can reindex.\n",
        "\n",
        "    return rmsfe_table\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_dns_rmsfe(\n",
        "    df_forecasts: pd.DataFrame,\n",
        "    df_yields: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 13: Compute DNS RMSFE table.\n",
        "\n",
        "    Args:\n",
        "        df_forecasts: Forecast DataFrame.\n",
        "        df_yields: Realized yields.\n",
        "        study_config: Frozen configuration.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: RMSFE table in bps.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 13: Computing DNS RMSFE\")\n",
        "\n",
        "    # Extract parameters\n",
        "    multiplier = study_config[\"Global_Settings\"][\"pct_points_to_bps_multiplier\"]\n",
        "    canonical_maturities = study_config[\"Global_Settings\"][\"us_zero_maturities\"]\n",
        "\n",
        "    # 1. Compute Errors\n",
        "    df_errors = compute_forecast_errors(df_forecasts, df_yields)\n",
        "\n",
        "    # 2. Calculate RMSFE Table\n",
        "    rmsfe_table = calculate_rmsfe_table(df_errors, multiplier)\n",
        "\n",
        "    # 3. Enforce Canonical Row Order\n",
        "    # Reindex to ensure rows match the standard order (3M, 6M, ...)\n",
        "    # Filter to only those present in the table (in case some dropped entirely)\n",
        "    available_mats = [m for m in canonical_maturities if m in rmsfe_table.index]\n",
        "    rmsfe_table = rmsfe_table.reindex(available_mats)\n",
        "\n",
        "    logger.info(\"Task 13 Completed Successfully.\")\n",
        "    return rmsfe_table\n"
      ],
      "metadata": {
        "id": "QDp27qs0QfS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14: Preprocess macroeconomic predictors for FADNS: enforce publication lag and define information set\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 14: Preprocess macroeconomic predictors for FADNS\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 1: Define information set bounds\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def get_macro_window_bounds(\n",
        "    origin_date: pd.Timestamp,\n",
        "    window_size: int,\n",
        "    publication_lag: int = 1\n",
        ") -> Tuple[pd.Timestamp, pd.Timestamp]:\n",
        "    \"\"\"\n",
        "    Calculates the start and end dates for the macro information set at a given forecast origin.\n",
        "\n",
        "    Equation:\n",
        "        I_t = {Z_{t-lag}, ..., Z_{t-lag-w+1}}\n",
        "        End date = t - lag (months)\n",
        "        Start date = t - lag - w + 1 (months)\n",
        "\n",
        "    Args:\n",
        "        origin_date (pd.Timestamp): The forecast origin date t.\n",
        "        window_size (int): Rolling window size w (e.g., 60).\n",
        "        publication_lag (int): Lag in months (default 1).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.Timestamp, pd.Timestamp]: (start_date, end_date) of the macro block.\n",
        "    \"\"\"\n",
        "    # Calculate end date: t - 1 month\n",
        "    # We use DateOffset(months=lag) and normalize to MonthEnd\n",
        "    window_end = origin_date - DateOffset(months=publication_lag) + MonthEnd(0)\n",
        "\n",
        "    # Calculate start date: end - (w - 1) months\n",
        "    # Note: If w=60, we need 60 observations ending at window_end.\n",
        "    # So we go back 59 months from window_end.\n",
        "    window_start = window_end - DateOffset(months=window_size - 1) + MonthEnd(0)\n",
        "\n",
        "    return window_start, window_end\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 2 & 3: Extract blocks and validate look-ahead\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def extract_rolling_macro_blocks(\n",
        "    df_macro: pd.DataFrame,\n",
        "    window_size: int,\n",
        "    publication_lag: int,\n",
        "    missing_policy: str\n",
        ") -> Dict[pd.Timestamp, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Extracts lagged macro blocks for all feasible forecast origins.\n",
        "\n",
        "    Args:\n",
        "        df_macro (pd.DataFrame): Cleansed macro DataFrame.\n",
        "        window_size (int): Window size w.\n",
        "        publication_lag (int): Lag in months.\n",
        "        missing_policy (str): Policy for handling missing data in window ('drop_window' or 'impute').\n",
        "\n",
        "    Returns:\n",
        "        Dict[pd.Timestamp, pd.DataFrame]: Mapping from origin date t to macro block DataFrame.\n",
        "    \"\"\"\n",
        "    blocks = {}\n",
        "\n",
        "    # Determine feasible origins\n",
        "    # We need at least window_size + lag history\n",
        "    # Earliest possible end_date is df_macro.index[window_size - 1]\n",
        "    # Earliest possible origin is end_date + lag\n",
        "\n",
        "    if len(df_macro) < window_size:\n",
        "        logger.warning(\"Macro history shorter than window size. No blocks extracted.\")\n",
        "        return blocks\n",
        "\n",
        "    # Iterate through potential origins\n",
        "    # We can iterate through the macro index, treating each date as a potential \"end of window\"\n",
        "    # Then map forward to the origin date.\n",
        "\n",
        "    # Let i be the index of the window end (t-1)\n",
        "    # i must be >= window_size - 1\n",
        "\n",
        "    for i in range(window_size - 1, len(df_macro)):\n",
        "        window_end_date = df_macro.index[i]\n",
        "\n",
        "        # The forecast origin t is window_end + lag\n",
        "        origin_date = window_end_date + DateOffset(months=publication_lag) + MonthEnd(0)\n",
        "\n",
        "        # Calculate bounds explicitly to be safe\n",
        "        start_date, end_date = get_macro_window_bounds(origin_date, window_size, publication_lag)\n",
        "\n",
        "        # Validate alignment (sanity check)\n",
        "        if end_date != window_end_date:\n",
        "            # This might happen if origin_date calculation shifted due to EOM logic differently\n",
        "            # But with MonthEnd(0) it should be consistent.\n",
        "            pass\n",
        "\n",
        "        # Extract block\n",
        "        # Use loc for date-based slicing (inclusive)\n",
        "        block = df_macro.loc[start_date:end_date].copy()\n",
        "\n",
        "        # Validate shape\n",
        "        if len(block) != window_size:\n",
        "            # This can happen if there are gaps in the index\n",
        "            # Log and skip\n",
        "            # logger.debug(f\"Block for origin {origin_date} has {len(block)} rows, expected {window_size}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Validate Look-Ahead Bias\n",
        "        if block.index.max() >= origin_date:\n",
        "            raise ValueError(f\"Look-ahead bias detected for origin {origin_date}. Max block date: {block.index.max()}\")\n",
        "\n",
        "        # Handle Missingness\n",
        "        if block.isna().sum().sum() > 0:\n",
        "            if missing_policy == \"drop_window\":\n",
        "                # logger.debug(f\"Dropping origin {origin_date} due to NaNs in macro block.\")\n",
        "                continue\n",
        "            elif missing_policy == \"impute_linear\": # Or other impute policies\n",
        "                # Apply imputation within the window\n",
        "                block = block.interpolate(method='time', limit_direction='both')\n",
        "                if block.isna().sum().sum() > 0:\n",
        "                    # Still NaNs? Drop.\n",
        "                    continue\n",
        "            else:\n",
        "                # Default to drop if unknown\n",
        "                continue\n",
        "\n",
        "        blocks[origin_date] = block\n",
        "\n",
        "    return blocks\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def preprocess_fadns_macro(\n",
        "    df_macro: pd.DataFrame,\n",
        "    study_config: Dict[str, Any],\n",
        "    study_metadata: Dict[str, Any]\n",
        ") -> Dict[pd.Timestamp, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 14: Preprocess macro predictors for FADNS.\n",
        "\n",
        "    Args:\n",
        "        df_macro: Cleansed US macro DataFrame.\n",
        "        study_config: Frozen configuration.\n",
        "        study_metadata: Metadata.\n",
        "\n",
        "    Returns:\n",
        "        Dict mapping origin date to macro block.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 14: Preprocessing FADNS Macro Predictors\")\n",
        "\n",
        "    # Extract parameters\n",
        "    window_size = study_config[\"Model_Architectures\"][\"FADNS\"][\"rolling_window_w\"]\n",
        "    lag = study_config[\"Model_Architectures\"][\"FADNS\"][\"macro_publication_lag_months\"]\n",
        "    policy = study_metadata[\"missing_data_policy\"][\"macro\"]\n",
        "\n",
        "    # Execute\n",
        "    blocks = extract_rolling_macro_blocks(df_macro, window_size, lag, policy)\n",
        "\n",
        "    logger.info(f\"Task 14 Completed Successfully. Extracted {len(blocks)} rolling macro blocks.\")\n",
        "    return blocks\n"
      ],
      "metadata": {
        "id": "s7jdDbcNR_fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15: Apply rolling ADF unit-root filtering within each FADNS window\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15: Apply rolling ADF unit-root filtering\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 1 & 2: Apply ADF test and determine transformations\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def determine_stationarity_transformations(\n",
        "    df_block: pd.DataFrame,\n",
        "    p_value_threshold: float = 0.10,\n",
        "    regression: str = \"c\",\n",
        "    max_lag: int = 12,\n",
        "    autolag: str = \"AIC\"\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Applies the Augmented Dickey-Fuller test to each column in the rolling window.\n",
        "    Returns a list of column names that require differencing (Non-Stationary).\n",
        "\n",
        "    Null Hypothesis (H0): Unit Root exists (Non-Stationary).\n",
        "    Decision:\n",
        "        - If p-value >= threshold: Fail to reject H0 -> Assume Non-Stationary -> Flag for Differencing.\n",
        "        - If p-value < threshold: Reject H0 -> Assume Stationary -> Keep Levels.\n",
        "\n",
        "    Args:\n",
        "        df_block (pd.DataFrame): Macro data block for a specific window (w x p).\n",
        "        p_value_threshold (float): Significance level (default 0.10).\n",
        "        regression (str): Regression type for ADF ('c', 'ct', etc.).\n",
        "        max_lag (int): Max lag for ADF.\n",
        "        autolag (str): Method to select lag length.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: List of column names to be differenced.\n",
        "    \"\"\"\n",
        "    cols_to_difference = []\n",
        "\n",
        "    for col in df_block.columns:\n",
        "        series = df_block[col].dropna()\n",
        "\n",
        "        # Handle constant series (zero variance)\n",
        "        if series.nunique() <= 1:\n",
        "            # A constant series is technically stationary (variance=0, mean=const).\n",
        "            # However, adfuller might throw an error or return weird stats.\n",
        "            # We treat it as stationary (no differencing needed).\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Run ADF\n",
        "            # adfuller returns: (adf_stat, pvalue, usedlag, nobs, crit_values, icbest)\n",
        "            result = adfuller(\n",
        "                series,\n",
        "                maxlag=max_lag,\n",
        "                regression=regression,\n",
        "                autolag=autolag\n",
        "            )\n",
        "            p_value = result[1]\n",
        "\n",
        "            # Check H0\n",
        "            if p_value >= p_value_threshold:\n",
        "                # Fail to reject H0 -> Non-Stationary -> Difference\n",
        "                cols_to_difference.append(col)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Fallback for numerical errors (e.g., SVD convergence in ADF)\n",
        "            # Log warning and default to differencing (conservative for financial data)\n",
        "            # logger.warning(f\"ADF test failed for {col}: {str(e)}. Defaulting to differencing.\")\n",
        "            cols_to_difference.append(col)\n",
        "\n",
        "    return cols_to_difference\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 3: Apply differencing and align window\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def apply_transformations_and_align(\n",
        "    df_block: pd.DataFrame,\n",
        "    cols_to_difference: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies first differencing to flagged columns and aligns the window.\n",
        "\n",
        "    Alignment Rule:\n",
        "        - Differencing introduces a NaN at the first position.\n",
        "        - To maintain a rectangular matrix for PCA, we drop the first row\n",
        "          for ALL columns (even those not differenced).\n",
        "        - Resulting window length is w' = w - 1.\n",
        "\n",
        "    Args:\n",
        "        df_block (pd.DataFrame): Original macro block (w x p).\n",
        "        cols_to_difference (List[str]): Columns to difference.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Transformed and aligned block (w-1 x p).\n",
        "    \"\"\"\n",
        "    df_transformed = df_block.copy()\n",
        "\n",
        "    # Apply differencing\n",
        "    if cols_to_difference:\n",
        "        df_transformed[cols_to_difference] = df_transformed[cols_to_difference].diff()\n",
        "\n",
        "    # Drop the first row to align\n",
        "    # This removes the NaN created by diff() and aligns stationary vars to the same time range\n",
        "    df_aligned = df_transformed.iloc[1:].copy()\n",
        "\n",
        "    return df_aligned\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def apply_rolling_adf_filtering(\n",
        "    macro_blocks: Dict[pd.Timestamp, pd.DataFrame],\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[pd.Timestamp, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 15: Rolling ADF filtering.\n",
        "\n",
        "    Args:\n",
        "        macro_blocks: Dictionary of macro blocks from Task 14.\n",
        "        study_config: Frozen configuration.\n",
        "\n",
        "    Returns:\n",
        "        Dict[pd.Timestamp, pd.DataFrame]: Processed macro blocks (w-1 x p).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 15: Rolling ADF Unit-Root Filtering\")\n",
        "\n",
        "    # Extract parameters\n",
        "    params = study_config[\"Preprocessing_Params\"][\"Stationarity_ADF\"]\n",
        "    p_thresh = params[\"p_value_threshold\"]\n",
        "    regression = params[\"regression\"]\n",
        "    max_lag = params[\"max_lag\"]\n",
        "    autolag = params[\"autolag\"]\n",
        "\n",
        "    processed_blocks = {}\n",
        "\n",
        "    # Iterate over sorted origins for deterministic log order\n",
        "    sorted_origins = sorted(macro_blocks.keys())\n",
        "\n",
        "    for origin in sorted_origins:\n",
        "        block = macro_blocks[origin]\n",
        "\n",
        "        # 1. Determine Transformations\n",
        "        cols_diff = determine_stationarity_transformations(\n",
        "            block, p_thresh, regression, max_lag, autolag\n",
        "        )\n",
        "\n",
        "        # 2. Apply and Align\n",
        "        block_clean = apply_transformations_and_align(block, cols_diff)\n",
        "\n",
        "        processed_blocks[origin] = block_clean\n",
        "\n",
        "        # Optional: Log stats occasionally\n",
        "        # if len(cols_diff) > 0:\n",
        "        #     logger.debug(f\"Origin {origin.date()}: Differenced {len(cols_diff)} variables.\")\n",
        "\n",
        "    logger.info(f\"Task 15 Completed Successfully. Processed {len(processed_blocks)} blocks.\")\n",
        "    return processed_blocks\n"
      ],
      "metadata": {
        "id": "dUCk6xoKTe1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16: Standardize transformed macro variables within each rolling window (z-score)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 16: Standardize transformed macro variables within each rolling window\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 1 & 2: Compute stats and standardize\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def zscore_standardize_block(df_block: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Standardizes a macro data block to zero mean and unit variance (Z-score).\n",
        "\n",
        "    Equation:\n",
        "        Z_tilde = (Z - mean) / std\n",
        "        where std is sample standard deviation (ddof=1).\n",
        "\n",
        "    Handling Constants:\n",
        "        If a column is constant (std=0), the standardized values are set to 0.\n",
        "\n",
        "    Args:\n",
        "        df_block (pd.DataFrame): The macro data block (w' x p).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Standardized block.\n",
        "    \"\"\"\n",
        "    # Compute mean and sample std (ddof=1 is default in pandas)\n",
        "    means = df_block.mean()\n",
        "    stds = df_block.std()\n",
        "\n",
        "    # Handle constant columns (std=0)\n",
        "    # Replace 0 with 1 to avoid DivisionByZero, then fill result with 0\n",
        "    # We identify them first\n",
        "    constant_cols = stds[stds == 0].index\n",
        "\n",
        "    # Safe divisor\n",
        "    stds_safe = stds.replace(0, 1.0)\n",
        "\n",
        "    # Standardize\n",
        "    df_standardized = (df_block - means) / stds_safe\n",
        "\n",
        "    # Fix constant columns (0 / 1 -> 0, which is correct for z-score of constant)\n",
        "    # But if the constant was 5, (5-5)/1 = 0. Correct.\n",
        "    # Just ensure no NaNs crept in.\n",
        "    if len(constant_cols) > 0:\n",
        "        # Explicitly set to 0.0 to be safe against floating point noise\n",
        "        df_standardized[constant_cols] = 0.0\n",
        "        # logger.debug(f\"Block has {len(constant_cols)} constant columns. Set to 0.\")\n",
        "\n",
        "    return df_standardized\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 3: Assemble blocks (Orchestrator helper)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_standardization(df_std: pd.DataFrame) -> bool:\n",
        "    \"\"\"\n",
        "    Validates that the block is correctly standardized.\n",
        "\n",
        "    Checks:\n",
        "        - Means are approx 0.\n",
        "        - Stds are approx 1 (or 0 for constant cols).\n",
        "        - No NaNs.\n",
        "    \"\"\"\n",
        "    if df_std.isna().sum().sum() > 0:\n",
        "        return False\n",
        "\n",
        "    # Check means (tolerance 1e-10)\n",
        "    means = df_std.mean()\n",
        "    if not np.allclose(means, 0, atol=1e-7):\n",
        "        # This might fail if data is huge, but for w=60 it should be precise\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def standardize_rolling_macro_blocks(\n",
        "    processed_blocks: Dict[pd.Timestamp, pd.DataFrame],\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[pd.Timestamp, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 16: Rolling Z-score standardization.\n",
        "\n",
        "    Args:\n",
        "        processed_blocks: Dictionary of differenced/aligned macro blocks.\n",
        "        study_config: Frozen configuration.\n",
        "\n",
        "    Returns:\n",
        "        Dict[pd.Timestamp, pd.DataFrame]: Standardized macro blocks.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 16: Rolling Z-Score Standardization\")\n",
        "\n",
        "    # Extract parameters (if any specific config needed, e.g. ddof)\n",
        "    # Manuscript implies standard sample std.\n",
        "    standardized_blocks = {}\n",
        "\n",
        "    sorted_origins = sorted(processed_blocks.keys())\n",
        "\n",
        "    for origin in sorted_origins:\n",
        "        block = processed_blocks[origin]\n",
        "\n",
        "        # Standardize\n",
        "        block_std = zscore_standardize_block(block)\n",
        "\n",
        "        # Validate\n",
        "        if not validate_standardization(block_std):\n",
        "            logger.warning(f\"Standardization validation failed for origin {origin}. Check data integrity.\")\n",
        "            # We proceed, but log warning. Failure usually means NaNs.\n",
        "\n",
        "        standardized_blocks[origin] = block_std\n",
        "\n",
        "    logger.info(f\"Task 16 Completed Successfully. Standardized {len(standardized_blocks)} blocks.\")\n",
        "    return standardized_blocks\n"
      ],
      "metadata": {
        "id": "AJdv8gJ2VOiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17: Conduct rolling PCA with eigenvector sign alignment and construct macro factors $F_t^{(k)}$\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: Rolling PCA with Sign Alignment\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Step 1: Compute Covariance and Eigendecomposition\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_eigenpairs(df_std_block: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Computes eigenvalues and eigenvectors of the sample covariance matrix.\n",
        "\n",
        "    Args:\n",
        "        df_std_block (pd.DataFrame): Standardized macro block (w' x p).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, np.ndarray]:\n",
        "            - eigenvalues (p,) sorted descending\n",
        "            - eigenvectors (p, p) sorted columns corresponding to eigenvalues\n",
        "    \"\"\"\n",
        "    # Compute sample covariance (unbiased, ddof=1 implicit in cov definition if we used pandas cov,\n",
        "    # but here we do matrix mult manually to be explicit about the formula)\n",
        "\n",
        "    # Formula: Sigma = (Z.T @ Z) / (n - 1)\n",
        "    Z = df_std_block.values\n",
        "    n = Z.shape[0]\n",
        "    Sigma = (Z.T @ Z) / (n - 1)\n",
        "\n",
        "    # Eigendecomposition (symmetric)\n",
        "    # eigh returns eigenvalues in ascending order\n",
        "    evals, evecs = np.linalg.eigh(Sigma)\n",
        "\n",
        "    # Sort descending\n",
        "    idx = np.argsort(evals)[::-1]\n",
        "    evals = evals[idx]\n",
        "    evecs = evecs[:, idx]\n",
        "\n",
        "    return evals, evecs\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Step 2: Sign Alignment\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def align_eigenvectors(\n",
        "    curr_evecs: np.ndarray,\n",
        "    prev_evecs: Optional[np.ndarray]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Aligns eigenvector signs to be consistent with the previous window.\n",
        "\n",
        "    Rule: If dot(v_curr, v_prev) < 0, flip v_curr.\n",
        "\n",
        "    Args:\n",
        "        curr_evecs (np.ndarray): Current eigenvectors (p, p).\n",
        "        prev_evecs (Optional[np.ndarray]): Previous aligned eigenvectors.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Aligned current eigenvectors.\n",
        "    \"\"\"\n",
        "    if prev_evecs is None:\n",
        "        return curr_evecs.copy()\n",
        "\n",
        "    aligned_evecs = curr_evecs.copy()\n",
        "    n_components = curr_evecs.shape[1]\n",
        "\n",
        "    # Iterate columns (components)\n",
        "    for j in range(n_components):\n",
        "        v_curr = curr_evecs[:, j]\n",
        "        v_prev = prev_evecs[:, j]\n",
        "\n",
        "        dot_prod = np.dot(v_curr, v_prev)\n",
        "\n",
        "        if dot_prod < 0:\n",
        "            aligned_evecs[:, j] = -v_curr\n",
        "\n",
        "    return aligned_evecs\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Step 3: Construct Factors\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_pca_factors(\n",
        "    df_std_block: pd.DataFrame,\n",
        "    aligned_evecs: np.ndarray,\n",
        "    k_max: int = 10\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the first k principal components using the last observation in the block.\n",
        "\n",
        "    PC_t = V.T * Z_{t-1}\n",
        "\n",
        "    Args:\n",
        "        df_std_block (pd.DataFrame): Standardized block ending at t-1.\n",
        "        aligned_evecs (np.ndarray): Aligned eigenvectors (p, p).\n",
        "        k_max (int): Number of factors to extract.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Vector of k factors (k_max,).\n",
        "    \"\"\"\n",
        "    # Extract Z_{t-1} (last row)\n",
        "    z_last = df_std_block.iloc[-1].values\n",
        "\n",
        "    # Project\n",
        "    # Factors = Z_{t-1} @ V\n",
        "    # Shape: (p,) @ (p, p) -> (p,)\n",
        "    # We only need first k columns of V\n",
        "    V_k = aligned_evecs[:, :k_max]\n",
        "    factors = z_last @ V_k\n",
        "\n",
        "    return factors\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def construct_rolling_pca_factors(\n",
        "    standardized_blocks: Dict[pd.Timestamp, pd.DataFrame],\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 17: Rolling PCA factor extraction.\n",
        "\n",
        "    Args:\n",
        "        standardized_blocks: Dictionary of standardized macro blocks.\n",
        "        study_config: Frozen configuration.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame of factors indexed by origin date t, columns PC1..PC10.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 17: Rolling PCA Factor Extraction\")\n",
        "\n",
        "    k_max = max(study_config[\"Model_Architectures\"][\"FADNS\"][\"pca_k_grid\"])\n",
        "\n",
        "    sorted_origins = sorted(standardized_blocks.keys())\n",
        "\n",
        "    factor_records = []\n",
        "    prev_evecs = None\n",
        "    prev_origin = None\n",
        "\n",
        "    for origin in sorted_origins:\n",
        "        block = standardized_blocks[origin]\n",
        "\n",
        "        # 1. Eigendecomposition\n",
        "        evals, evecs = compute_eigenpairs(block)\n",
        "\n",
        "        # 2. Alignment\n",
        "        # Check continuity: is this origin the immediate successor?\n",
        "        # We assume monthly frequency. If gap > 32 days, reset alignment.\n",
        "        is_continuous = False\n",
        "        if prev_origin is not None:\n",
        "            gap = (origin - prev_origin).days\n",
        "            if 20 <= gap <= 32: # Allow standard month length\n",
        "                is_continuous = True\n",
        "\n",
        "        if not is_continuous:\n",
        "            prev_evecs = None # Reset\n",
        "\n",
        "        aligned_evecs = align_eigenvectors(evecs, prev_evecs)\n",
        "\n",
        "        # 3. Factor Construction\n",
        "        factors = compute_pca_factors(block, aligned_evecs, k_max)\n",
        "\n",
        "        # Store\n",
        "        record = {\"Date\": origin}\n",
        "        for i in range(k_max):\n",
        "            record[f\"PC{i+1}\"] = factors[i]\n",
        "        factor_records.append(record)\n",
        "\n",
        "        # Update state\n",
        "        prev_evecs = aligned_evecs\n",
        "        prev_origin = origin\n",
        "\n",
        "    # Create DataFrame\n",
        "    df_factors = pd.DataFrame(factor_records)\n",
        "    if not df_factors.empty:\n",
        "        df_factors = df_factors.set_index(\"Date\")\n",
        "\n",
        "    logger.info(f\"Task 17 Completed Successfully. Extracted factors for {len(df_factors)} periods.\")\n",
        "    return df_factors\n"
      ],
      "metadata": {
        "id": "oDpK-x7SeIne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18: Construct the FADNS augmented state and estimate rolling VAR(1) for each k\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 18: FADNS Estimation and Forecasting\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Step 1: Form Augmented State\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def form_augmented_states(\n",
        "    df_beta: pd.DataFrame,\n",
        "    df_macro_factors: pd.DataFrame,\n",
        "    k_grid: List[int]\n",
        ") -> Dict[int, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Constructs the augmented state vector X_t = [beta_t, F_t] for each k.\n",
        "\n",
        "    Args:\n",
        "        df_beta (pd.DataFrame): DNS factors (T x 3).\n",
        "        df_macro_factors (pd.DataFrame): Macro factors (T x 10).\n",
        "        k_grid (List[int]): List of k values (e.g., 1..10).\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, pd.DataFrame]: Mapping k -> Augmented State DataFrame (T_common x (3+k)).\n",
        "    \"\"\"\n",
        "    augmented_states = {}\n",
        "\n",
        "    # Align indices\n",
        "    common_index = df_beta.index.intersection(df_macro_factors.index)\n",
        "\n",
        "    if len(common_index) == 0:\n",
        "        raise ValueError(\"No overlapping dates between DNS factors and Macro factors.\")\n",
        "\n",
        "    beta_aligned = df_beta.loc[common_index]\n",
        "    macro_aligned = df_macro_factors.loc[common_index]\n",
        "\n",
        "    for k in k_grid:\n",
        "        # Select first k macro factors\n",
        "        # Macro cols are PC1, PC2, ...\n",
        "        cols_k = [f\"PC{i+1}\" for i in range(k)]\n",
        "\n",
        "        # Concatenate\n",
        "        # Result columns: Beta1, Beta2, Beta3, PC1, ..., PCk\n",
        "        X_k = pd.concat([beta_aligned, macro_aligned[cols_k]], axis=1)\n",
        "\n",
        "        augmented_states[k] = X_k\n",
        "\n",
        "    return augmented_states\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Step 2 & 3: Estimate VAR and Forecast\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def estimate_and_forecast_fadns(\n",
        "    augmented_states: Dict[int, pd.DataFrame],\n",
        "    window_size: int,\n",
        "    horizons: List[int]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Estimates rolling VAR(1) for each k and generates forecasts.\n",
        "\n",
        "    Args:\n",
        "        augmented_states (Dict): Mapping k -> DataFrame.\n",
        "        window_size (int): Rolling window size w.\n",
        "        horizons (List[int]): Forecast horizons.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Long-format forecasts with columns:\n",
        "                      ['OriginDate', 'Horizon', 'k', 'Beta1', 'Beta2', 'Beta3']\n",
        "    \"\"\"\n",
        "    forecast_records = []\n",
        "\n",
        "    # Iterate over k\n",
        "    for k, df_state in augmented_states.items():\n",
        "        data = df_state.values\n",
        "        dates = df_state.index\n",
        "        n_obs, n_vars = data.shape # n_vars = 3 + k\n",
        "\n",
        "        # Start index for rolling window\n",
        "        start_idx = window_size - 1\n",
        "\n",
        "        for t_idx in range(start_idx, n_obs):\n",
        "            # 1. Extract Window\n",
        "            # Window: t-w+1 to t\n",
        "            window_data = data[t_idx - window_size + 1 : t_idx + 1]\n",
        "\n",
        "            # 2. Estimate VAR(1)\n",
        "            # Y = X_{s+1}, Z = [1, X_s]\n",
        "            Y = window_data[1:]\n",
        "            X_lag = window_data[:-1]\n",
        "            n_samples = X_lag.shape[0]\n",
        "\n",
        "            # Add intercept\n",
        "            Z = np.column_stack([np.ones(n_samples), X_lag])\n",
        "\n",
        "            # OLS: B = (Z.T Z)^-1 Z.T Y\n",
        "            # B shape: (1 + n_vars, n_vars)\n",
        "            # c = B[0], Phi.T = B[1:]\n",
        "            try:\n",
        "                B, _, _, _ = np.linalg.lstsq(Z, Y, rcond=None)\n",
        "            except np.linalg.LinAlgError:\n",
        "                # Skip if singular (rare with w=60, k<=10)\n",
        "                continue\n",
        "\n",
        "            c_hat = B[0, :]\n",
        "            Phi_hat = B[1:, :].T\n",
        "\n",
        "            # 3. Forecast\n",
        "            # Current state X_t is the last row of window_data\n",
        "            current_state = window_data[-1]\n",
        "            origin_date = dates[t_idx]\n",
        "\n",
        "            # Iterative forecast\n",
        "            term_intercept = np.zeros_like(c_hat)\n",
        "            term_autoreg = current_state.copy()\n",
        "\n",
        "            max_h = max(horizons)\n",
        "\n",
        "            for h in range(1, max_h + 1):\n",
        "                # Update terms\n",
        "                term_intercept = c_hat + Phi_hat @ term_intercept\n",
        "                term_autoreg = Phi_hat @ term_autoreg\n",
        "\n",
        "                pred_state = term_intercept + term_autoreg\n",
        "\n",
        "                if h in horizons:\n",
        "                    # Extract Beta (first 3 components)\n",
        "                    beta_pred = pred_state[:3]\n",
        "\n",
        "                    forecast_records.append({\n",
        "                        \"OriginDate\": origin_date,\n",
        "                        \"Horizon\": h,\n",
        "                        \"k\": k,\n",
        "                        \"Beta1\": beta_pred[0],\n",
        "                        \"Beta2\": beta_pred[1],\n",
        "                        \"Beta3\": beta_pred[2]\n",
        "                    })\n",
        "\n",
        "    # Create DataFrame\n",
        "    df_forecasts = pd.DataFrame(forecast_records)\n",
        "    return df_forecasts\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_fadns_estimation_and_forecast(\n",
        "    df_beta: pd.DataFrame,\n",
        "    df_macro_factors: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 18: FADNS estimation and forecasting.\n",
        "\n",
        "    Args:\n",
        "        df_beta: DNS factors.\n",
        "        df_macro_factors: Macro PCA factors.\n",
        "        study_config: Frozen configuration.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Forecasts of Beta factors for all k, h, t.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 18: FADNS Estimation and Forecasting\")\n",
        "\n",
        "    # Extract parameters\n",
        "    k_grid = study_config[\"Model_Architectures\"][\"FADNS\"][\"pca_k_grid\"]\n",
        "    window_size = study_config[\"Model_Architectures\"][\"FADNS\"][\"rolling_window_w\"]\n",
        "    horizons = study_config[\"Global_Settings\"][\"forecast_horizons\"]\n",
        "\n",
        "    # 1. Form States\n",
        "    augmented_states = form_augmented_states(df_beta, df_macro_factors, k_grid)\n",
        "\n",
        "    # 2. Estimate and Forecast\n",
        "    df_fadns_beta_forecasts = estimate_and_forecast_fadns(augmented_states, window_size, horizons)\n",
        "\n",
        "    logger.info(f\"Task 18 Completed Successfully. Generated {len(df_fadns_beta_forecasts)} forecast records.\")\n",
        "    return df_fadns_beta_forecasts\n"
      ],
      "metadata": {
        "id": "8CotLGE8f5RU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19: Compute FADNS yield forecasts and RMSFE tables by $k$, maturity, and horizon\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 19: Compute FADNS yield forecasts and RMSFE tables\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 1: Map FADNS factor forecasts to yield forecasts\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def map_fadns_factors_to_yields(\n",
        "    df_beta_forecasts: pd.DataFrame,\n",
        "    L: np.ndarray,\n",
        "    canonical_maturities: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Maps FADNS beta forecasts to yield forecasts using the Nelson-Siegel loading matrix.\n",
        "\n",
        "    Equation:\n",
        "        y_hat = L * beta_hat\n",
        "\n",
        "    Args:\n",
        "        df_beta_forecasts (pd.DataFrame): Long format ['OriginDate', 'Horizon', 'k', 'Beta1', 'Beta2', 'Beta3'].\n",
        "        L (np.ndarray): Loading matrix (N x 3).\n",
        "        canonical_maturities (List[str]): Ordered maturity labels.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Long format yield forecasts ['OriginDate', 'TargetDate', 'Horizon', 'k', 'Maturity', 'Forecast'].\n",
        "    \"\"\"\n",
        "    # 1. Prepare Beta Matrix\n",
        "    # We need a matrix of shape (M_samples, 3)\n",
        "    # We preserve the index information to map back\n",
        "    # Set index to identify rows\n",
        "    df_indexed = df_beta_forecasts.set_index(['OriginDate', 'Horizon', 'k'])\n",
        "\n",
        "    # Extract Beta columns\n",
        "    beta_matrix = df_indexed[['Beta1', 'Beta2', 'Beta3']].values\n",
        "\n",
        "    # 2. Compute Yields\n",
        "    # Y = Beta @ L.T\n",
        "    # Shape: (M_samples, 3) @ (3, N) -> (M_samples, N)\n",
        "    yield_matrix = beta_matrix @ L.T\n",
        "\n",
        "    # 3. Reconstruct DataFrame\n",
        "    # Create a DataFrame with the computed yields, indexed by the metadata\n",
        "    df_yields_wide = pd.DataFrame(\n",
        "        yield_matrix,\n",
        "        index=df_indexed.index,\n",
        "        columns=canonical_maturities\n",
        "    )\n",
        "\n",
        "    # 4. Melt to Long Format\n",
        "    df_yields_long = df_yields_wide.reset_index().melt(\n",
        "        id_vars=['OriginDate', 'Horizon', 'k'],\n",
        "        var_name='Maturity',\n",
        "        value_name='Forecast'\n",
        "    )\n",
        "\n",
        "    # 5. Compute TargetDate\n",
        "    # TargetDate = OriginDate + Horizon (months)\n",
        "    # We do this vectorized if possible, or apply\n",
        "    # Since Horizon is an integer month offset, we can use a helper or apply\n",
        "    # Optimization: Group by Horizon to apply offset once per group\n",
        "\n",
        "    # Initialize TargetDate column\n",
        "    df_yields_long['TargetDate'] = pd.NaT\n",
        "\n",
        "    for h in df_yields_long['Horizon'].unique():\n",
        "        mask = df_yields_long['Horizon'] == h\n",
        "        # Apply offset\n",
        "        # Note: We assume OriginDate is MonthEnd. Adding months keeps it MonthEnd usually,\n",
        "        # but + MonthEnd(0) ensures it.\n",
        "        origins = df_yields_long.loc[mask, 'OriginDate']\n",
        "        targets = origins + DateOffset(months=h) + MonthEnd(0)\n",
        "        df_yields_long.loc[mask, 'TargetDate'] = targets\n",
        "\n",
        "    return df_yields_long\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 2: Compute forecast errors and RMSFE\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_fadns_errors_and_rmsfe(\n",
        "    df_forecasts: pd.DataFrame,\n",
        "    df_actuals: pd.DataFrame,\n",
        "    multiplier: float = 100.0\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes forecast errors and RMSFE for FADNS models.\n",
        "\n",
        "    Args:\n",
        "        df_forecasts (pd.DataFrame): Long format yield forecasts.\n",
        "        df_actuals (pd.DataFrame): Realized yields (Date x Maturity).\n",
        "        multiplier (float): Unit conversion (default 100 for bps).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: RMSFE table indexed by ['Horizon', 'Maturity', 'k'], value 'RMSFE'.\n",
        "    \"\"\"\n",
        "    # Prepare actuals (Long format)\n",
        "    df_actuals_long = df_actuals.reset_index().melt(\n",
        "        id_vars=[df_actuals.index.name or \"index\"],\n",
        "        var_name=\"Maturity\",\n",
        "        value_name=\"Actual\"\n",
        "    )\n",
        "    df_actuals_long = df_actuals_long.rename(columns={df_actuals_long.columns[0]: \"TargetDate\"})\n",
        "    df_actuals_long[\"TargetDate\"] = pd.to_datetime(df_actuals_long[\"TargetDate\"])\n",
        "\n",
        "    # Merge\n",
        "    df_merged = pd.merge(\n",
        "        df_forecasts,\n",
        "        df_actuals_long,\n",
        "        on=[\"TargetDate\", \"Maturity\"],\n",
        "        how=\"inner\"\n",
        "    )\n",
        "\n",
        "    # Error\n",
        "    df_merged[\"Error\"] = df_merged[\"Actual\"] - df_merged[\"Forecast\"]\n",
        "\n",
        "    # RMSFE Groupby\n",
        "    # Group by k, Horizon, Maturity\n",
        "    grouped = df_merged.groupby([\"k\", \"Horizon\", \"Maturity\"])[\"Error\"]\n",
        "\n",
        "    mse = grouped.apply(lambda x: np.mean(x**2))\n",
        "    rmsfe = np.sqrt(mse) * multiplier\n",
        "\n",
        "    return rmsfe.reset_index(name=\"RMSFE\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 3: Produce RMSFE tables per horizon\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def format_fadns_rmsfe_tables(\n",
        "    df_rmsfe: pd.DataFrame,\n",
        "    canonical_maturities: List[str]\n",
        ") -> Dict[int, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Formats RMSFE results into tables per horizon.\n",
        "\n",
        "    Args:\n",
        "        df_rmsfe (pd.DataFrame): Long format RMSFE results.\n",
        "        canonical_maturities (List[str]): Ordered maturity labels.\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, pd.DataFrame]: Mapping Horizon -> DataFrame (Rows=Maturity, Cols=k).\n",
        "    \"\"\"\n",
        "    tables = {}\n",
        "    horizons = sorted(df_rmsfe[\"Horizon\"].unique())\n",
        "\n",
        "    for h in horizons:\n",
        "        # Filter\n",
        "        subset = df_rmsfe[df_rmsfe[\"Horizon\"] == h]\n",
        "\n",
        "        # Pivot: Index=Maturity, Columns=k, Values=RMSFE\n",
        "        pivot = subset.pivot(index=\"Maturity\", columns=\"k\", values=\"RMSFE\")\n",
        "\n",
        "        # Reorder rows\n",
        "        available_mats = [m for m in canonical_maturities if m in pivot.index]\n",
        "        pivot = pivot.reindex(available_mats)\n",
        "\n",
        "        # Rename columns to PCA(k)\n",
        "        pivot.columns = [f\"PCA({k})\" for k in pivot.columns]\n",
        "\n",
        "        tables[h] = pivot\n",
        "\n",
        "    return tables\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_fadns_rmsfe(\n",
        "    df_beta_forecasts: pd.DataFrame,\n",
        "    df_yields: pd.DataFrame,\n",
        "    L: np.ndarray,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Tuple[Dict[int, pd.DataFrame], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 19: Compute FADNS RMSFE.\n",
        "\n",
        "    Args:\n",
        "        df_beta_forecasts: FADNS beta forecasts.\n",
        "        df_yields: Realized yields.\n",
        "        L: Loading matrix.\n",
        "        study_config: Frozen configuration.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict, pd.DataFrame]: (RMSFE Tables per Horizon, Raw RMSFE Long DataFrame).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 19: Computing FADNS RMSFE\")\n",
        "\n",
        "    canonical_maturities = study_config[\"Global_Settings\"][\"us_zero_maturities\"]\n",
        "    multiplier = study_config[\"Global_Settings\"][\"pct_points_to_bps_multiplier\"]\n",
        "\n",
        "    # 1. Map to Yields\n",
        "    df_yield_forecasts = map_fadns_factors_to_yields(df_beta_forecasts, L, canonical_maturities)\n",
        "\n",
        "    # 2. Compute RMSFE\n",
        "    df_rmsfe_long = compute_fadns_errors_and_rmsfe(df_yield_forecasts, df_yields, multiplier)\n",
        "\n",
        "    # 3. Format Tables\n",
        "    rmsfe_tables = format_fadns_rmsfe_tables(df_rmsfe_long, canonical_maturities)\n",
        "\n",
        "    logger.info(f\"Task 19 Completed Successfully. Generated tables for {len(rmsfe_tables)} horizons.\")\n",
        "    return rmsfe_tables, df_rmsfe_long\n"
      ],
      "metadata": {
        "id": "VVpUQ9SKhn1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 20: Select the best PCA dimension k* per maturity and horizon, and produce the best-k table\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 20: Select best PCA dimension k*\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Step 1 & 2: Identify k* and construct selection table\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def identify_best_k_and_rmsfe(\n",
        "    rmsfe_tables: Dict[int, pd.DataFrame]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Identifies the optimal PCA dimension k for each maturity and horizon.\n",
        "\n",
        "    Selection Rule:\n",
        "        k* = argmin_k RMSFE(k)\n",
        "        Tie-breaking: Smallest k (via idxmin on sorted columns).\n",
        "\n",
        "    Args:\n",
        "        rmsfe_tables (Dict[int, pd.DataFrame]): Mapping Horizon -> DataFrame(Maturity x k).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "            - best_k_table: DataFrame with entries \"PCA(k)\".\n",
        "            - best_rmsfe_table: DataFrame with min RMSFE values.\n",
        "    \"\"\"\n",
        "    best_k_data = {}\n",
        "    best_rmsfe_data = {}\n",
        "\n",
        "    horizons = sorted(rmsfe_tables.keys())\n",
        "\n",
        "    for h in horizons:\n",
        "        df = rmsfe_tables[h]\n",
        "\n",
        "        # Ensure columns are sorted by k for deterministic tie-breaking\n",
        "        # Columns are \"PCA(1)\", \"PCA(2)\", ...\n",
        "        # We sort by the integer value\n",
        "        sorted_cols = sorted(df.columns, key=lambda x: int(x.replace(\"PCA(\", \"\").replace(\")\", \"\")))\n",
        "        df = df[sorted_cols]\n",
        "\n",
        "        # Find best k (column name)\n",
        "        best_k_col = df.idxmin(axis=1)\n",
        "\n",
        "        # Find min RMSFE\n",
        "        min_rmsfe = df.min(axis=1)\n",
        "\n",
        "        # Store\n",
        "        best_k_data[h] = best_k_col\n",
        "        best_rmsfe_data[h] = min_rmsfe\n",
        "\n",
        "    # Construct DataFrames\n",
        "    # Index will be Maturity (from the series)\n",
        "    df_best_k = pd.DataFrame(best_k_data)\n",
        "    df_best_rmsfe = pd.DataFrame(best_rmsfe_data)\n",
        "\n",
        "    # Rename columns to H1, H3, ...\n",
        "    df_best_k.columns = [f\"H{h}\" for h in df_best_k.columns]\n",
        "    df_best_rmsfe.columns = [f\"H{h}\" for h in df_best_rmsfe.columns]\n",
        "\n",
        "    return df_best_k, df_best_rmsfe\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def select_best_fadns_k(\n",
        "    rmsfe_tables: Dict[int, pd.DataFrame],\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 20: Select best FADNS k.\n",
        "\n",
        "    Args:\n",
        "        rmsfe_tables: Dictionary of RMSFE tables per horizon.\n",
        "        study_config: Frozen configuration.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]: (Best k Table, Best RMSFE Table).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 20: Selecting Best FADNS k\")\n",
        "\n",
        "    # Execute selection\n",
        "    df_best_k, df_best_rmsfe = identify_best_k_and_rmsfe(rmsfe_tables)\n",
        "\n",
        "    # Enforce canonical row order\n",
        "    canonical_maturities = study_config[\"Global_Settings\"][\"us_zero_maturities\"]\n",
        "    available_mats = [m for m in canonical_maturities if m in df_best_k.index]\n",
        "\n",
        "    df_best_k = df_best_k.reindex(available_mats)\n",
        "    df_best_rmsfe = df_best_rmsfe.reindex(available_mats)\n",
        "\n",
        "    logger.info(\"Task 20 Completed Successfully.\")\n",
        "    return df_best_k, df_best_rmsfe\n"
      ],
      "metadata": {
        "id": "qBy5vHa2jKgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 21: Construct RF feature vectors with asymmetric lag structure for U.S. zero-coupon yields\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 21: Construct RF feature vectors\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 1: Define lag index sets\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def get_lag_definitions(study_config: Dict[str, Any]) -> Tuple[List[int], List[int]]:\n",
        "    \"\"\"\n",
        "    Retrieves the lag definitions for macro and yield variables.\n",
        "\n",
        "    Args:\n",
        "        study_config: Frozen configuration.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[int], List[int]]: (macro_lags, yield_lags).\n",
        "    \"\"\"\n",
        "    params = study_config[\"Preprocessing_Params\"][\"Lag_Structure\"]\n",
        "    macro_lags = params[\"macro_lags\"]\n",
        "    yield_lags = params[\"yield_lags\"]\n",
        "    return macro_lags, yield_lags\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 2: Construct feature matrices\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def construct_lagged_features(\n",
        "    df_data: pd.DataFrame,\n",
        "    lags: List[int],\n",
        "    feature_type: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs a DataFrame of lagged features.\n",
        "\n",
        "    Args:\n",
        "        df_data (pd.DataFrame): Input time series (T x N).\n",
        "        lags (List[int]): List of lags to apply.\n",
        "        feature_type (str): Prefix for column names ('Macro' or 'Yield').\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with lagged columns. Index is same as df_data (rows with NaNs will exist).\n",
        "    \"\"\"\n",
        "    lagged_dfs = []\n",
        "\n",
        "    # Iterate lags\n",
        "    for lag in lags:\n",
        "        # Shift data\n",
        "        # shift(1) moves data from t to t+1. So at t+1 we see data from t.\n",
        "        # We want at time t to see data from t-lag.\n",
        "        # So we shift by +lag.\n",
        "        df_shifted = df_data.shift(lag)\n",
        "\n",
        "        # Rename columns\n",
        "        df_shifted.columns = [f\"{feature_type}_{col}_L{lag}\" for col in df_data.columns]\n",
        "\n",
        "        lagged_dfs.append(df_shifted)\n",
        "\n",
        "    # Concatenate\n",
        "    df_features = pd.concat(lagged_dfs, axis=1)\n",
        "\n",
        "    return df_features\n",
        "\n",
        "def construct_rf_feature_matrices(\n",
        "    df_macro: pd.DataFrame,\n",
        "    df_yields: pd.DataFrame,\n",
        "    macro_lags: List[int],\n",
        "    yield_lags: List[int]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Constructs the full feature sets for Random Forest.\n",
        "\n",
        "    Args:\n",
        "        df_macro: Macro data.\n",
        "        df_yields: Yield data.\n",
        "        macro_lags: Lags for macro.\n",
        "        yield_lags: Lags for yields.\n",
        "\n",
        "    Returns:\n",
        "        Tuple:\n",
        "            - df_macro_features: DataFrame of macro features (shared across maturities).\n",
        "            - dict_yield_features: Dictionary mapping maturity -> DataFrame of yield features.\n",
        "    \"\"\"\n",
        "    # 1. Macro Features (Shared)\n",
        "    # Flatten macro panel: 111 vars * 60 lags\n",
        "    df_macro_features = construct_lagged_features(df_macro, macro_lags, \"Macro\")\n",
        "\n",
        "    # 2. Yield Features (Per Maturity)\n",
        "    # Each maturity's RF uses its own lags\n",
        "    dict_yield_features = {}\n",
        "\n",
        "    for maturity in df_yields.columns:\n",
        "        # Extract single series as DataFrame\n",
        "        df_single = df_yields[[maturity]]\n",
        "        df_feat = construct_lagged_features(df_single, yield_lags, \"Yield\")\n",
        "        dict_yield_features[maturity] = df_feat\n",
        "\n",
        "    return df_macro_features, dict_yield_features\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 3: Define rolling training sample helper\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def get_rf_training_data(\n",
        "    origin_date: pd.Timestamp,\n",
        "    horizon: int,\n",
        "    maturity: str,\n",
        "    df_macro_features: pd.DataFrame,\n",
        "    dict_yield_features: Dict[str, pd.DataFrame],\n",
        "    df_yields: pd.DataFrame,\n",
        "    window_size: int = 60\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Retrieves the training X and y for a specific RF model (origin, horizon, maturity).\n",
        "\n",
        "    Training Window:\n",
        "        s = t - W + 1 ... t\n",
        "        X_s = [Macro_Features_s, Yield_Features_s]\n",
        "        y_s = Yield_{s+h}\n",
        "\n",
        "    Args:\n",
        "        origin_date: Forecast origin t.\n",
        "        horizon: Forecast horizon h.\n",
        "        maturity: Target maturity.\n",
        "        df_macro_features: Precomputed macro features.\n",
        "        dict_yield_features: Precomputed yield features.\n",
        "        df_yields: Realized yields (targets).\n",
        "        window_size: Training window size W.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, np.ndarray]: (X_train, y_train).\n",
        "    \"\"\"\n",
        "    # 1. Identify Training Indices (s)\n",
        "    # We need W observations ending at origin_date\n",
        "    # Get integer location of origin_date in the index\n",
        "    # We assume indices are aligned and sorted\n",
        "    # Use the macro features index as the reference\n",
        "    if origin_date not in df_macro_features.index:\n",
        "        raise ValueError(f\"Origin date {origin_date} not found in feature index.\")\n",
        "\n",
        "    loc_idx = df_macro_features.index.get_loc(origin_date)\n",
        "\n",
        "    # Start index\n",
        "    start_idx = loc_idx - window_size + 1\n",
        "\n",
        "    if start_idx < 0:\n",
        "        raise ValueError(f\"Insufficient history for origin {origin_date}. Need {window_size} rows.\")\n",
        "\n",
        "    # Extract X components\n",
        "    # Slice is inclusive of start, exclusive of end in iloc usually, but we want up to loc_idx inclusive\n",
        "    # So iloc[start : loc+1]\n",
        "    X_macro = df_macro_features.iloc[start_idx : loc_idx + 1]\n",
        "    X_yield = dict_yield_features[maturity].iloc[start_idx : loc_idx + 1]\n",
        "\n",
        "    # Concatenate X\n",
        "    # Ensure alignment\n",
        "    if not X_macro.index.equals(X_yield.index):\n",
        "         raise ValueError(\"Index mismatch between macro and yield features.\")\n",
        "\n",
        "    X_train_df = pd.concat([X_macro, X_yield], axis=1)\n",
        "\n",
        "    # 2. Identify Targets (y)\n",
        "    # y_s = Yield_{s+h}\n",
        "    # We need to shift the yield series back by h to align with s\n",
        "    # Or lookup by date: target_date = s_date + h months\n",
        "    # Vectorized lookup:\n",
        "    # Get dates s\n",
        "    dates_s = X_train_df.index\n",
        "\n",
        "    # Calculate target dates\n",
        "    # Note: This assumes MonthEnd alignment\n",
        "    dates_target = dates_s + DateOffset(months=horizon) + MonthEnd(0)\n",
        "\n",
        "    # Lookup yields\n",
        "    # We use reindex to handle potential missing future dates (though training should be in-sample)\n",
        "    y_train_series = df_yields[maturity].reindex(dates_target)\n",
        "\n",
        "    # Check for NaNs in X or y\n",
        "    # If NaNs exist (e.g. missing target), we must drop those rows\n",
        "    # This handles the \"available data\" constraint\n",
        "    # Combine to drop\n",
        "    # We create a temp frame\n",
        "    temp_df = X_train_df.copy()\n",
        "    temp_df[\"Target\"] = y_train_series.values\n",
        "\n",
        "    # Drop NaNs\n",
        "    temp_df_clean = temp_df.dropna()\n",
        "\n",
        "    if len(temp_df_clean) < window_size:\n",
        "        # Log if we lost data\n",
        "        # logger.debug(f\"Training data for {origin_date} reduced from {window_size} to {len(temp_df_clean)} due to NaNs.\")\n",
        "        pass\n",
        "\n",
        "    # Split back\n",
        "    X_train = temp_df_clean.drop(columns=[\"Target\"]).values\n",
        "    y_train = temp_df_clean[\"Target\"].values\n",
        "\n",
        "    return X_train, y_train\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def construct_rf_features(\n",
        "    df_macro: pd.DataFrame,\n",
        "    df_yields: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 21: Construct RF feature vectors.\n",
        "\n",
        "    Args:\n",
        "        df_macro: Cleansed macro data.\n",
        "        df_yields: Cleansed yield data.\n",
        "        study_config: Frozen configuration.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (Macro Features DataFrame, Dictionary of Yield Features DataFrames).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 21: Constructing RF Features\")\n",
        "\n",
        "    macro_lags, yield_lags = get_lag_definitions(study_config)\n",
        "\n",
        "    df_macro_feat, dict_yield_feat = construct_rf_feature_matrices(\n",
        "        df_macro, df_yields, macro_lags, yield_lags\n",
        "    )\n",
        "\n",
        "    # Validate dimensions\n",
        "    n_macro = len(df_macro.columns)\n",
        "    n_yield = 1 # per maturity\n",
        "    expected_dim = len(macro_lags) * n_macro + len(yield_lags) * n_yield\n",
        "\n",
        "    # Check one maturity\n",
        "    sample_mat = list(dict_yield_feat.keys())[0]\n",
        "    actual_dim = df_macro_feat.shape[1] + dict_yield_feat[sample_mat].shape[1]\n",
        "\n",
        "    if actual_dim != expected_dim:\n",
        "        logger.warning(f\"Feature dimension {actual_dim} does not match expected {expected_dim}.\")\n",
        "    else:\n",
        "        logger.info(f\"Feature construction complete. Dimension: {actual_dim}.\")\n",
        "\n",
        "    return df_macro_feat, dict_yield_feat\n"
      ],
      "metadata": {
        "id": "3peqThBbk-iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 22: Apply min–max normalization to RF features and response within each rolling window\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 22: Apply min-max normalization\n",
        "# ==============================================================================\n",
        "\n",
        "class ScalerParams(NamedTuple):\n",
        "    \"\"\"\n",
        "    A NamedTuple container for storing the min-max normalization parameters derived from a training window.\n",
        "\n",
        "    This structure encapsulates the statistics required to normalize new data (test sets)\n",
        "    or inverse-transform predictions back to the original scale, ensuring no data leakage\n",
        "    from future observations.\n",
        "\n",
        "    Attributes:\n",
        "        x_min (np.ndarray): The minimum values for each feature column in the training set.\n",
        "                            Shape: (n_features,).\n",
        "        x_max (np.ndarray): The maximum values for each feature column in the training set.\n",
        "                            Shape: (n_features,).\n",
        "        y_min (float): The minimum value of the target variable in the training set.\n",
        "        y_max (float): The maximum value of the target variable in the training set.\n",
        "    \"\"\"\n",
        "    x_min: np.ndarray\n",
        "    x_max: np.ndarray\n",
        "    y_min: float\n",
        "    y_max: float\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 1 & 2: Compute stats and normalize\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_min_max_and_normalize(\n",
        "    data: np.ndarray\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Computes min/max and normalizes data to [0, 1].\n",
        "    Handles constant columns by setting them to 0.\n",
        "\n",
        "    Args:\n",
        "        data (np.ndarray): Input data (n_samples, n_features).\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (normalized_data, min_vals, max_vals).\n",
        "    \"\"\"\n",
        "    # Compute stats\n",
        "    min_vals = np.min(data, axis=0)\n",
        "    max_vals = np.max(data, axis=0)\n",
        "\n",
        "    # Compute range\n",
        "    ranges = max_vals - min_vals\n",
        "\n",
        "    # Handle constant columns (range = 0)\n",
        "    # Avoid division by zero\n",
        "    # If range is 0, we set divisor to 1 (result will be 0/1 = 0)\n",
        "    # But we must ensure numerator is 0. Numerator is x - min. If x is constant, x=min, so x-min=0.\n",
        "    # So safe division works.\n",
        "    ranges_safe = np.where(ranges == 0, 1.0, ranges)\n",
        "\n",
        "    # Normalize\n",
        "    # Shape broadcasting: (N, D) - (D,) / (D,)\n",
        "    normalized_data = (data - min_vals) / ranges_safe\n",
        "\n",
        "    return normalized_data, min_vals, max_vals\n",
        "\n",
        "def normalize_rf_training_data(\n",
        "    X_train: np.ndarray,\n",
        "    y_train: np.ndarray\n",
        ") -> Tuple[np.ndarray, np.ndarray, ScalerParams]:\n",
        "    \"\"\"\n",
        "    Normalizes predictors and response for RF training.\n",
        "\n",
        "    Args:\n",
        "        X_train (np.ndarray): Predictors.\n",
        "        y_train (np.ndarray): Response.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (X_norm, y_norm, ScalerParams).\n",
        "    \"\"\"\n",
        "    # Normalize X\n",
        "    X_norm, x_min, x_max = compute_min_max_and_normalize(X_train)\n",
        "\n",
        "    # Normalize y (reshape to 2D for consistent function usage, then flatten)\n",
        "    y_2d = y_train.reshape(-1, 1)\n",
        "    y_norm_2d, y_min_arr, y_max_arr = compute_min_max_and_normalize(y_2d)\n",
        "\n",
        "    y_norm = y_norm_2d.flatten()\n",
        "    y_min = float(y_min_arr[0])\n",
        "    y_max = float(y_max_arr[0])\n",
        "\n",
        "    params = ScalerParams(x_min, x_max, y_min, y_max)\n",
        "\n",
        "    return X_norm, y_norm, params\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 3: Apply normalization to test data (Helper)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def apply_normalization(\n",
        "    X_test: np.ndarray,\n",
        "    params: ScalerParams\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Applies stored normalization parameters to test data.\n",
        "    Note: Values can exceed [0, 1] if test data is outside training range.\n",
        "\n",
        "    Args:\n",
        "        X_test (np.ndarray): Test predictors.\n",
        "        params (ScalerParams): Stored parameters.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Normalized test data.\n",
        "    \"\"\"\n",
        "    ranges = params.x_max - params.x_min\n",
        "    ranges_safe = np.where(ranges == 0, 1.0, ranges)\n",
        "\n",
        "    X_norm = (X_test - params.x_min) / ranges_safe\n",
        "\n",
        "    return X_norm\n",
        "\n",
        "def inverse_transform_response(\n",
        "    y_pred_norm: np.ndarray,\n",
        "    params: ScalerParams\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Inverse transforms normalized predictions to original scale.\n",
        "\n",
        "    Args:\n",
        "        y_pred_norm (np.ndarray): Normalized predictions.\n",
        "        params (ScalerParams): Stored parameters.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Predictions in original scale.\n",
        "    \"\"\"\n",
        "    y_range = params.y_max - params.y_min\n",
        "    y_pred = y_pred_norm * y_range + params.y_min\n",
        "    return y_pred\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def normalize_rf_data(\n",
        "    X_train: np.ndarray,\n",
        "    y_train: np.ndarray\n",
        ") -> Tuple[np.ndarray, np.ndarray, ScalerParams]:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 22: Normalize RF data.\n",
        "\n",
        "    Args:\n",
        "        X_train: Training features.\n",
        "        y_train: Training targets.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (X_norm, y_norm, ScalerParams).\n",
        "    \"\"\"\n",
        "    # logger.debug(\"Starting Task 22: Normalizing RF Data\") # Verbose for inner loop\n",
        "\n",
        "    X_norm, y_norm, params = normalize_rf_training_data(X_train, y_train)\n",
        "\n",
        "    # Validate bounds (sanity check)\n",
        "    if np.max(X_norm) > 1.0 + 1e-7 or np.min(X_norm) < 0.0 - 1e-7:\n",
        "        logger.warning(\"Normalization failed to bound X within [0, 1].\")\n",
        "\n",
        "    # logger.debug(\"Task 22 Completed Successfully.\")\n",
        "    return X_norm, y_norm, params\n"
      ],
      "metadata": {
        "id": "4CHtQ89Dm8kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 23: Train Random Forest models with randomized CV hyperparameter tuning\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 23: Train Random Forest models\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Step 1, 2 & 3: Define, Tune, and Fit RF\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def train_rf_with_randomized_cv(\n",
        "    X_train: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    study_config: Dict[str, Any],\n",
        "    seed: int\n",
        ") -> RandomForestRegressor:\n",
        "    \"\"\"\n",
        "    Trains a Random Forest model with Randomized Cross-Validation using TimeSeriesSplit.\n",
        "\n",
        "    Algorithm:\n",
        "        1. Define hyperparameter space Theta.\n",
        "        2. Split training data into time-series folds.\n",
        "        3. Sample n_iter configurations.\n",
        "        4. Evaluate MSE on validation folds.\n",
        "        5. Select theta* minimizing MSE.\n",
        "        6. Refit on full training set.\n",
        "\n",
        "    Args:\n",
        "        X_train (np.ndarray): Normalized predictors.\n",
        "        y_train (np.ndarray): Normalized response.\n",
        "        study_config (Dict): Frozen configuration containing 'cv' settings.\n",
        "        seed (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        RandomForestRegressor: The fitted best model.\n",
        "    \"\"\"\n",
        "    # Extract config\n",
        "    rf_config = study_config[\"Model_Architectures\"][\"Random_Forest\"]\n",
        "    cv_config = rf_config[\"cv\"]\n",
        "\n",
        "    param_dist = cv_config[\"hyperparam_space\"]\n",
        "    n_iter = cv_config[\"n_iter\"]\n",
        "    n_folds = cv_config[\"n_folds\"]\n",
        "\n",
        "    # 1. Define Base Model\n",
        "    # Criterion is MSE (squared_error in newer sklearn)\n",
        "    rf = RandomForestRegressor(\n",
        "        criterion=\"squared_error\",\n",
        "        bootstrap=True,\n",
        "        random_state=seed\n",
        "    )\n",
        "\n",
        "    # 2. Define CV Splitter\n",
        "    # TimeSeriesSplit ensures no look-ahead bias in validation\n",
        "    tscv = TimeSeriesSplit(n_splits=n_folds)\n",
        "\n",
        "    # 3. Randomized Search\n",
        "    # n_jobs=-1 uses all cores for the search\n",
        "    search = RandomizedSearchCV(\n",
        "        estimator=rf,\n",
        "        param_distributions=param_dist,\n",
        "        n_iter=n_iter,\n",
        "        cv=tscv,\n",
        "        scoring=\"neg_mean_squared_error\", # Maximizes negative MSE -> Minimizes MSE\n",
        "        random_state=seed,\n",
        "        n_jobs=-1,\n",
        "        refit=True # Step 6: Refit on full data\n",
        "    )\n",
        "\n",
        "    # 4. Fit\n",
        "    # This executes the search and the final refit\n",
        "    search.fit(X_train, y_train)\n",
        "\n",
        "    # Log best params (optional, verbose)\n",
        "    # logger.debug(f\"Best RF Params (Seed {seed}): {search.best_params_}\")\n",
        "\n",
        "    return search.best_estimator_\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def train_rf_model(\n",
        "    X_train: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    study_config: Dict[str, Any],\n",
        "    seed: int\n",
        ") -> RandomForestRegressor:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 23: Train RF model.\n",
        "\n",
        "    Args:\n",
        "        X_train: Training features.\n",
        "        y_train: Training targets.\n",
        "        study_config: Frozen configuration.\n",
        "        seed: Random seed.\n",
        "\n",
        "    Returns:\n",
        "        RandomForestRegressor: Fitted model.\n",
        "    \"\"\"\n",
        "    # logger.debug(f\"Starting Task 23: Training RF (Seed {seed})\")\n",
        "\n",
        "    model = train_rf_with_randomized_cv(X_train, y_train, study_config, seed)\n",
        "\n",
        "    # logger.debug(\"Task 23 Completed Successfully.\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "28mZBsCYpy7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 24: Generate RF direct forecasts and inverse-transform to original scale\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 24: Generate RF direct forecasts\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Step 1 & 2: Predict and Inverse Transform (Single Step)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def predict_single_rf_step(\n",
        "    model: Any, # sklearn estimator\n",
        "    X_test_raw: np.ndarray,\n",
        "    scaler_params: Any # ScalerParams named tuple\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Normalizes test input, predicts, and inverse transforms the response.\n",
        "\n",
        "    Args:\n",
        "        model: Fitted Random Forest model.\n",
        "        X_test_raw: Raw test feature vector (1D or 2D).\n",
        "        scaler_params: Normalization parameters from training.\n",
        "\n",
        "    Returns:\n",
        "        float: Forecast in original scale.\n",
        "    \"\"\"\n",
        "    # 1. Normalize X\n",
        "    # Ensure 2D shape (1, n_features)\n",
        "    if X_test_raw.ndim == 1:\n",
        "        X_test_raw = X_test_raw.reshape(1, -1)\n",
        "\n",
        "    # Apply normalization using stored params\n",
        "    # Formula: (X - min) / (max - min)\n",
        "    # Handle constant columns (range=0) by setting divisor to 1 (numerator is 0)\n",
        "    ranges = scaler_params.x_max - scaler_params.x_min\n",
        "    ranges_safe = np.where(ranges == 0, 1.0, ranges)\n",
        "\n",
        "    X_test_norm = (X_test_raw - scaler_params.x_min) / ranges_safe\n",
        "\n",
        "    # 2. Predict\n",
        "    y_pred_norm = model.predict(X_test_norm)\n",
        "\n",
        "    # 3. Inverse Transform y\n",
        "    y_range = scaler_params.y_max - scaler_params.y_min\n",
        "    y_pred = y_pred_norm * y_range + scaler_params.y_min\n",
        "\n",
        "    return float(y_pred[0])\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Step 3: Rolling Forecast Loop\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def execute_rolling_rf_forecasts(\n",
        "    df_macro_features: pd.DataFrame,\n",
        "    dict_yield_features: Dict[str, pd.DataFrame],\n",
        "    df_yields: pd.DataFrame,\n",
        "    study_config: Dict[str, Any],\n",
        "    seeds: List[int]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Executes the full rolling forecast procedure for Random Forests.\n",
        "\n",
        "    Loops over:\n",
        "        - Seeds\n",
        "        - Maturities\n",
        "        - Horizons\n",
        "        - Forecast Origins\n",
        "\n",
        "    Args:\n",
        "        df_macro_features: Precomputed macro features.\n",
        "        dict_yield_features: Precomputed yield features per maturity.\n",
        "        df_yields: Realized yields.\n",
        "        study_config: Frozen configuration.\n",
        "        seeds: List of random seeds.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Long-format forecasts.\n",
        "    \"\"\"\n",
        "    # Extract config\n",
        "    window_size = study_config[\"Model_Architectures\"][\"Random_Forest\"][\"rolling_window_W\"]\n",
        "    horizons = study_config[\"Global_Settings\"][\"forecast_horizons\"]\n",
        "    maturities = study_config[\"Global_Settings\"][\"us_zero_maturities\"]\n",
        "\n",
        "    forecast_records = []\n",
        "\n",
        "    # Determine feasible origins\n",
        "    # We need window_size history.\n",
        "    # The features index is the reference.\n",
        "    valid_dates = df_macro_features.index\n",
        "    if len(valid_dates) < window_size:\n",
        "        logger.error(\"Insufficient history for RF training.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Start origin: index window_size - 1\n",
        "    # End origin: last date\n",
        "    start_idx = window_size - 1\n",
        "\n",
        "    # Optimization: Pre-calculate indices to avoid repeated lookups\n",
        "    # We iterate origins, then inner loops\n",
        "    total_origins = len(valid_dates) - start_idx\n",
        "    logger.info(f\"Starting RF Rolling Forecasts. Origins: {total_origins}, Seeds: {len(seeds)}, Mats: {len(maturities)}, Horizons: {len(horizons)}\")\n",
        "\n",
        "    for i in range(start_idx, len(valid_dates)):\n",
        "        origin_date = valid_dates[i]\n",
        "\n",
        "        # Define Training Window Indices (inclusive)\n",
        "        # Window: [i - window_size + 1, i]\n",
        "        train_start_idx = i - window_size + 1\n",
        "        train_end_idx = i\n",
        "\n",
        "        # Test Vector Index: i (current origin)\n",
        "        for maturity in maturities:\n",
        "            # Construct X (Macro + Yield)\n",
        "            # We slice using integer positions for speed\n",
        "            # Macro\n",
        "            X_macro_train = df_macro_features.iloc[train_start_idx : train_end_idx + 1].values\n",
        "            X_macro_test = df_macro_features.iloc[i].values\n",
        "\n",
        "            # Yield\n",
        "            df_yf = dict_yield_features[maturity]\n",
        "            X_yield_train = df_yf.iloc[train_start_idx : train_end_idx + 1].values\n",
        "            X_yield_test = df_yf.iloc[i].values\n",
        "\n",
        "            # Combine\n",
        "            X_train_raw = np.hstack([X_macro_train, X_yield_train])\n",
        "            X_test_raw = np.hstack([X_macro_test, X_yield_test])\n",
        "\n",
        "            for h in horizons:\n",
        "                # Construct y (Target)\n",
        "                # Target for training row s is Yield_{s+h}\n",
        "                # We need to shift the yield series\n",
        "                # Get target dates for the training window\n",
        "                train_dates = df_macro_features.index[train_start_idx : train_end_idx + 1]\n",
        "                target_dates = train_dates + DateOffset(months=h) + MonthEnd(0)\n",
        "\n",
        "                # Lookup yields\n",
        "                y_train_raw = df_yields[maturity].reindex(target_dates).values\n",
        "\n",
        "                # Handle Missing Targets (if h pushes into future or gaps)\n",
        "                # Drop rows with NaN targets\n",
        "                valid_mask = np.isfinite(y_train_raw)\n",
        "\n",
        "                if np.sum(valid_mask) < window_size:\n",
        "                    # If we lose too much data, maybe skip or warn\n",
        "                    # For strictness, we use what's available if it meets a minimum\n",
        "                    if np.sum(valid_mask) < window_size * 0.9: # Arbitrary threshold for stability\n",
        "                        continue\n",
        "\n",
        "                X_train_clean = X_train_raw[valid_mask]\n",
        "                y_train_clean = y_train_raw[valid_mask]\n",
        "\n",
        "                # Normalize\n",
        "                X_train_norm, y_train_norm, scaler_params = normalize_rf_data(X_train_clean, y_train_clean)\n",
        "\n",
        "                # Train and Predict per Seed\n",
        "                for seed in seeds:\n",
        "                    # Train\n",
        "                    model = train_rf_model(X_train_norm, y_train_norm, study_config, seed)\n",
        "\n",
        "                    # Predict\n",
        "                    forecast_val = predict_single_rf_step(model, X_test_raw, scaler_params)\n",
        "\n",
        "                    # Store\n",
        "                    target_date_test = origin_date + DateOffset(months=h) + MonthEnd(0)\n",
        "\n",
        "                    forecast_records.append({\n",
        "                        \"OriginDate\": origin_date,\n",
        "                        \"TargetDate\": target_date_test,\n",
        "                        \"Horizon\": h,\n",
        "                        \"Maturity\": maturity,\n",
        "                        \"Seed\": seed,\n",
        "                        \"Forecast\": forecast_val\n",
        "                    })\n",
        "\n",
        "    return pd.DataFrame(forecast_records)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def generate_rf_forecasts(\n",
        "    df_macro_features: pd.DataFrame,\n",
        "    dict_yield_features: Dict[str, pd.DataFrame],\n",
        "    df_yields: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 24: Generate RF forecasts.\n",
        "\n",
        "    Args:\n",
        "        df_macro_features: Macro features.\n",
        "        dict_yield_features: Yield features.\n",
        "        df_yields: Realized yields.\n",
        "        study_config: Frozen configuration.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Forecasts.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 24: Generating RF Forecasts\")\n",
        "\n",
        "    # Extract seeds\n",
        "    seeds = study_config[\"Model_Architectures\"][\"Random_Forest\"][\"rf_seeds_main_required_for_exact_replication\"]\n",
        "\n",
        "    # Make forecasts\n",
        "    df_forecasts = execute_rolling_rf_forecasts(\n",
        "        df_macro_features,\n",
        "        dict_yield_features,\n",
        "        df_yields,\n",
        "        study_config,\n",
        "        seeds\n",
        "    )\n",
        "\n",
        "    logger.info(f\"Task 24 Completed Successfully. Generated {len(df_forecasts)} forecast records.\")\n",
        "    return df_forecasts\n"
      ],
      "metadata": {
        "id": "rEH6WcfryQu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 25: Compute RF forecast errors and RMSFE summary table with [min,max] across seeds\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 25: Compute RF RMSFE summary\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 25, Step 1: Compute RMSFE per seed\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_rf_rmsfe_per_seed(\n",
        "    df_forecasts: pd.DataFrame,\n",
        "    df_actuals: pd.DataFrame,\n",
        "    multiplier: float = 100.0\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes RMSFE for each seed, horizon, and maturity.\n",
        "\n",
        "    Args:\n",
        "        df_forecasts (pd.DataFrame): RF forecasts with 'Seed' column.\n",
        "        df_actuals (pd.DataFrame): Realized yields.\n",
        "        multiplier (float): Unit conversion (default 100 for bps).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: RMSFE indexed by ['Seed', 'Horizon', 'Maturity'].\n",
        "    \"\"\"\n",
        "    # Prepare actuals\n",
        "    df_actuals_long = df_actuals.reset_index().melt(\n",
        "        id_vars=[df_actuals.index.name or \"index\"],\n",
        "        var_name=\"Maturity\",\n",
        "        value_name=\"Actual\"\n",
        "    )\n",
        "    df_actuals_long = df_actuals_long.rename(columns={df_actuals_long.columns[0]: \"TargetDate\"})\n",
        "    df_actuals_long[\"TargetDate\"] = pd.to_datetime(df_actuals_long[\"TargetDate\"])\n",
        "\n",
        "    # Merge\n",
        "    df_merged = pd.merge(\n",
        "        df_forecasts,\n",
        "        df_actuals_long,\n",
        "        on=[\"TargetDate\", \"Maturity\"],\n",
        "        how=\"inner\"\n",
        "    )\n",
        "\n",
        "    # Error\n",
        "    df_merged[\"Error\"] = df_merged[\"Actual\"] - df_merged[\"Forecast\"]\n",
        "\n",
        "    # RMSFE Groupby\n",
        "    grouped = df_merged.groupby([\"Seed\", \"Horizon\", \"Maturity\"])[\"Error\"]\n",
        "\n",
        "    mse = grouped.apply(lambda x: np.mean(x**2))\n",
        "    rmsfe = np.sqrt(mse) * multiplier\n",
        "\n",
        "    return rmsfe.reset_index(name=\"RMSFE\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 25, Step 2 & 3: Aggregate and Format\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def aggregate_and_format_rf_rmsfe(\n",
        "    df_rmsfe_seed: pd.DataFrame,\n",
        "    canonical_maturities: List[str]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Aggregates RMSFE across seeds and formats the summary table.\n",
        "\n",
        "    Args:\n",
        "        df_rmsfe_seed (pd.DataFrame): RMSFE per seed.\n",
        "        canonical_maturities (List[str]): Ordered maturity labels.\n",
        "\n",
        "    Returns:\n",
        "        Tuple:\n",
        "            - summary_table: DataFrame with string formatted \"Mean [Min, Max]\".\n",
        "            - numeric_stats: Dictionary with 'mean', 'min', 'max' DataFrames.\n",
        "    \"\"\"\n",
        "    # Group by Horizon, Maturity (aggregating over Seed)\n",
        "    grouped = df_rmsfe_seed.groupby([\"Horizon\", \"Maturity\"])[\"RMSFE\"]\n",
        "\n",
        "    stats = grouped.agg([\"mean\", \"min\", \"max\"])\n",
        "\n",
        "    # Create formatted string\n",
        "    # Format: \"Mean [Min, Max]\"\n",
        "    # We use .apply with axis=1\n",
        "    stats[\"formatted\"] = stats.apply(\n",
        "        lambda row: f\"{row['mean']:.2f} [{row['min']:.2f}, {row['max']:.2f}]\",\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Pivot tables\n",
        "    # We want Rows=Maturity, Cols=Horizon\n",
        "    def pivot_stat(col_name):\n",
        "        df = stats.reset_index().pivot(index=\"Maturity\", columns=\"Horizon\", values=col_name)\n",
        "        # Reorder rows\n",
        "        available = [m for m in canonical_maturities if m in df.index]\n",
        "        df = df.reindex(available)\n",
        "\n",
        "        # Rename cols\n",
        "        df.columns = [f\"H{h}\" for h in df.columns]\n",
        "        return df\n",
        "\n",
        "    summary_table = pivot_stat(\"formatted\")\n",
        "    mean_table = pivot_stat(\"mean\")\n",
        "    min_table = pivot_stat(\"min\")\n",
        "    max_table = pivot_stat(\"max\")\n",
        "\n",
        "    numeric_stats = {\n",
        "        \"mean\": mean_table,\n",
        "        \"min\": min_table,\n",
        "        \"max\": max_table\n",
        "    }\n",
        "\n",
        "    return summary_table, numeric_stats\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 25, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_rf_rmsfe_summary(\n",
        "    df_forecasts: pd.DataFrame,\n",
        "    df_yields: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 25: Compute RF RMSFE summary.\n",
        "\n",
        "    Args:\n",
        "        df_forecasts: RF forecasts.\n",
        "        df_yields: Realized yields.\n",
        "        study_config: Frozen configuration.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (Summary String Table, Numeric Stats Dict).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 25: Computing RF RMSFE Summary\")\n",
        "\n",
        "    multiplier = study_config[\"Global_Settings\"][\"pct_points_to_bps_multiplier\"]\n",
        "    canonical_maturities = study_config[\"Global_Settings\"][\"us_zero_maturities\"]\n",
        "\n",
        "    # 1. Compute per seed\n",
        "    df_rmsfe_seed = compute_rf_rmsfe_per_seed(df_forecasts, df_yields, multiplier)\n",
        "\n",
        "    # 2. Aggregate\n",
        "    summary_table, numeric_stats = aggregate_and_format_rf_rmsfe(df_rmsfe_seed, canonical_maturities)\n",
        "\n",
        "    logger.info(\"Task 25 Completed Successfully.\")\n",
        "    return summary_table, numeric_stats\n"
      ],
      "metadata": {
        "id": "CJ5SopnQB4K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 26: Build forecast combination candidate pools (hybrid and RF-only)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 26: Build forecast combination pools\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Step 1 & 2: Define pools and align\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def construct_model_pools(\n",
        "    df_fadns_yields: pd.DataFrame,\n",
        "    df_rf_forecasts: pd.DataFrame,\n",
        "    canonical_maturities: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs the hybrid pool of 20 models (10 FADNS + 10 RF).\n",
        "\n",
        "    Structure:\n",
        "        Index: (OriginDate, TargetDate, Horizon, Maturity)\n",
        "        Columns: Model_1 ... Model_20\n",
        "\n",
        "    Mapping:\n",
        "        Model_1..10: FADNS PCA(1)..PCA(10)\n",
        "        Model_11..20: RF Seed_1..Seed_10 (sorted seeds)\n",
        "\n",
        "    Args:\n",
        "        df_fadns_yields: Long format FADNS yields.\n",
        "        df_rf_forecasts: Long format RF forecasts.\n",
        "        canonical_maturities: List of maturities.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Wide format forecasts.\n",
        "    \"\"\"\n",
        "    # 1. Process FADNS\n",
        "    # Pivot k to columns\n",
        "    # Index keys: OriginDate, TargetDate, Horizon, Maturity\n",
        "    fadns_wide = df_fadns_yields.pivot(\n",
        "        index=[\"OriginDate\", \"TargetDate\", \"Horizon\", \"Maturity\"],\n",
        "        columns=\"k\",\n",
        "        values=\"Forecast\"\n",
        "    )\n",
        "    # Rename columns: 1..10 -> FADNS_1..FADNS_10\n",
        "    fadns_wide.columns = [f\"FADNS_{k}\" for k in fadns_wide.columns]\n",
        "\n",
        "    # 2. Process RF\n",
        "    # Pivot Seed to columns\n",
        "    rf_wide = df_rf_forecasts.pivot(\n",
        "        index=[\"OriginDate\", \"TargetDate\", \"Horizon\", \"Maturity\"],\n",
        "        columns=\"Seed\",\n",
        "        values=\"Forecast\"\n",
        "    )\n",
        "    # Rename columns: Seed_X -> RF_1..RF_10\n",
        "    # Sort seeds to ensure deterministic mapping\n",
        "    sorted_seeds = sorted(rf_wide.columns)\n",
        "    rf_wide = rf_wide[sorted_seeds]\n",
        "    rf_wide.columns = [f\"RF_{i+1}\" for i in range(len(sorted_seeds))]\n",
        "\n",
        "    # 3. Join\n",
        "    # Inner join to ensure we only have rows where both exist\n",
        "    df_pool = pd.concat([fadns_wide, rf_wide], axis=1, join=\"inner\")\n",
        "\n",
        "    # Sort index\n",
        "    df_pool = df_pool.sort_index()\n",
        "\n",
        "    return df_pool\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Step 3: Construct Error Matrices\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_pool_errors(\n",
        "    df_pool: pd.DataFrame,\n",
        "    df_actuals: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes forecast errors for the entire pool.\n",
        "\n",
        "    Args:\n",
        "        df_pool: Wide format forecasts (Models as columns).\n",
        "        df_actuals: Realized yields.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Wide format errors (same shape as df_pool).\n",
        "    \"\"\"\n",
        "    # Prepare actuals\n",
        "    # Melt to match the MultiIndex of df_pool\n",
        "    # We need to map TargetDate + Maturity -> Actual\n",
        "\n",
        "    # Reset index of pool to access columns\n",
        "    pool_reset = df_pool.reset_index()\n",
        "\n",
        "    # Lookup actuals\n",
        "    # We can use a merge\n",
        "    # Actuals: Index=Date, Cols=Maturities\n",
        "    actuals_long = df_actuals.reset_index().melt(\n",
        "        id_vars=[df_actuals.index.name or \"index\"],\n",
        "        var_name=\"Maturity\",\n",
        "        value_name=\"Actual\"\n",
        "    )\n",
        "    actuals_long = actuals_long.rename(columns={actuals_long.columns[0]: \"TargetDate\"})\n",
        "    actuals_long[\"TargetDate\"] = pd.to_datetime(actuals_long[\"TargetDate\"])\n",
        "\n",
        "    # Merge\n",
        "    # Left join on pool to keep structure\n",
        "    merged = pd.merge(\n",
        "        pool_reset,\n",
        "        actuals_long,\n",
        "        on=[\"TargetDate\", \"Maturity\"],\n",
        "        how=\"left\"\n",
        "    )\n",
        "\n",
        "    # Compute Errors\n",
        "    # Columns starting with FADNS or RF\n",
        "    model_cols = [c for c in df_pool.columns if c.startswith(\"FADNS\") or c.startswith(\"RF\")]\n",
        "\n",
        "    error_data = {}\n",
        "    for col in model_cols:\n",
        "        error_data[col] = merged[\"Actual\"] - merged[col]\n",
        "\n",
        "    # Reconstruct DataFrame with MultiIndex\n",
        "    df_errors = pd.DataFrame(error_data)\n",
        "    df_errors.index = df_pool.index\n",
        "\n",
        "    # Drop rows where Actual was missing (NaN errors)\n",
        "    # We need complete error vectors for combination\n",
        "    df_errors = df_errors.dropna()\n",
        "\n",
        "    return df_errors\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_forecast_combination_data(\n",
        "    df_fadns_yields: pd.DataFrame,\n",
        "    df_rf_forecasts: pd.DataFrame,\n",
        "    df_yields: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 26: Build pools and error matrices.\n",
        "\n",
        "    Args:\n",
        "        df_fadns_yields: FADNS forecasts.\n",
        "        df_rf_forecasts: RF forecasts.\n",
        "        df_yields: Realized yields.\n",
        "        study_config: Frozen configuration.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (df_pool_forecasts, df_pool_errors).\n",
        "        Both are wide DataFrames indexed by (OriginDate, TargetDate, Horizon, Maturity).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 26: Building Forecast Combination Pools\")\n",
        "\n",
        "    canonical_maturities = study_config[\"Global_Settings\"][\"us_zero_maturities\"]\n",
        "\n",
        "    # 1. Build Pool\n",
        "    df_pool = construct_model_pools(df_fadns_yields, df_rf_forecasts, canonical_maturities)\n",
        "\n",
        "    # 2. Compute Errors\n",
        "    df_errors = compute_pool_errors(df_pool, df_yields)\n",
        "\n",
        "    logger.info(f\"Task 26 Completed. Pool size: {df_pool.shape[1]} models. Error samples: {len(df_errors)}.\")\n",
        "    return df_pool, df_errors\n"
      ],
      "metadata": {
        "id": "VwLfz4_0D8-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 27: Implement the 14 forecast combination weight rules\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 27: Forecast Combination Weight Rules\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Helper: Simplex Projection\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def project_simplex(v: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Projects an arbitrary vector v onto the probability simplex.\n",
        "\n",
        "    The projection solves the optimization problem:\n",
        "        min_w ||w - v||^2\n",
        "        s.t. w >= 0, sum(w) = 1\n",
        "\n",
        "    Algorithm:\n",
        "        Sort-based Euclidean projection as described in Duchi et al. (2008).\n",
        "\n",
        "    Args:\n",
        "        v (np.ndarray): Input vector of shape (M,).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Projected vector w of shape (M,) satisfying simplex constraints.\n",
        "    \"\"\"\n",
        "    n = len(v)\n",
        "    # Sort v in descending order\n",
        "    u = np.sort(v)[::-1]\n",
        "\n",
        "    # Compute cumulative sum of sorted vector\n",
        "    cssv = np.cumsum(u)\n",
        "\n",
        "    # Find the index rho\n",
        "    # rho is the largest index j such that u_j + (1 - cssv_j) / j > 0\n",
        "    # Note: indices are 0-based in code, 1-based in formula\n",
        "    rho = np.nonzero(u * np.arange(1, n + 1) > (cssv - 1))[0][-1]\n",
        "\n",
        "    # Compute theta\n",
        "    theta = (cssv[rho] - 1) / (rho + 1)\n",
        "\n",
        "    # Compute w\n",
        "    w = np.maximum(v - theta, 0)\n",
        "\n",
        "    return w\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 27, Step 1: Classic Schemes\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def weights_ew(E: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes Equal Weights (FC-EW).\n",
        "\n",
        "    Equation:\n",
        "        w_{m,t} = 1 / M\n",
        "\n",
        "    Args:\n",
        "        E (np.ndarray): Error matrix of shape (W, M).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Weight vector of shape (M,).\n",
        "    \"\"\"\n",
        "    M = E.shape[1]\n",
        "    return np.ones(M) / M\n",
        "\n",
        "def weights_rank(E: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes Rank-Based Weights (FC-RANK).\n",
        "\n",
        "    Equation:\n",
        "        w_{m,t} = r_{m,t}^{-1} / sum(r_{k,t}^{-1})\n",
        "        where r_{m,t} is the rank of model m based on RMSFE (1 = best).\n",
        "\n",
        "    Args:\n",
        "        E (np.ndarray): Error matrix of shape (W, M).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Weight vector of shape (M,).\n",
        "    \"\"\"\n",
        "    # Compute RMSFE for each model\n",
        "    rmsfe = np.sqrt(np.mean(E**2, axis=0))\n",
        "\n",
        "    # Compute ranks (1 = lowest RMSFE)\n",
        "    ranks = rankdata(rmsfe, method='ordinal')\n",
        "\n",
        "    # Compute inverse ranks\n",
        "    inv_ranks = 1.0 / ranks\n",
        "\n",
        "    # Normalize to sum to 1\n",
        "    return inv_ranks / np.sum(inv_ranks)\n",
        "\n",
        "def weights_rmse(E: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes Inverse-RMSE Weights (FC-RMSE).\n",
        "\n",
        "    Equation:\n",
        "        w_{m,t} = RMSFE_{m,t}^{-1} / sum(RMSFE_{k,t}^{-1})\n",
        "\n",
        "    Args:\n",
        "        E (np.ndarray): Error matrix of shape (W, M).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Weight vector of shape (M,).\n",
        "    \"\"\"\n",
        "    # Compute RMSFE\n",
        "    rmsfe = np.sqrt(np.mean(E**2, axis=0))\n",
        "\n",
        "    # Avoid division by zero by enforcing a small floor\n",
        "    rmsfe = np.maximum(rmsfe, 1e-8)\n",
        "\n",
        "    # Compute inverse RMSFE\n",
        "    inv_rmsfe = 1.0 / rmsfe\n",
        "\n",
        "    # Normalize\n",
        "    return inv_rmsfe / np.sum(inv_rmsfe)\n",
        "\n",
        "def weights_mse(E: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes Winner-Take-All Weights (FC-MSE).\n",
        "\n",
        "    Equation:\n",
        "        w_{m,t} = 1 if m = argmin(MSE), 0 otherwise.\n",
        "\n",
        "    Args:\n",
        "        E (np.ndarray): Error matrix of shape (W, M).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Weight vector of shape (M,).\n",
        "    \"\"\"\n",
        "    # Compute MSE\n",
        "    mse = np.mean(E**2, axis=0)\n",
        "\n",
        "    # Identify best model\n",
        "    best_idx = np.argmin(mse)\n",
        "\n",
        "    # Assign weights\n",
        "    w = np.zeros_like(mse)\n",
        "    w[best_idx] = 1.0\n",
        "\n",
        "    return w\n",
        "\n",
        "def weights_ols(E: np.ndarray, screening_fraction: float = 0.3) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes OLS-Screened Weights (FC-OLS).\n",
        "\n",
        "    Algorithm:\n",
        "        1. Select top q models based on RMSFE (q = floor(screening_fraction * M)).\n",
        "        2. Solve OLS: min || bar_e - E_q * b ||^2\n",
        "           where bar_e is the average error of all models.\n",
        "        3. Set weights proportional to absolute coefficients: w = |b| / sum(|b|).\n",
        "\n",
        "    Args:\n",
        "        E (np.ndarray): Error matrix of shape (W, M).\n",
        "        screening_fraction (float): Fraction of models to retain (default 0.3).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Weight vector of shape (M,).\n",
        "    \"\"\"\n",
        "    M = E.shape[1]\n",
        "    q = max(1, int(np.floor(screening_fraction * M)))\n",
        "\n",
        "    # 1. Screen models\n",
        "    rmsfe = np.sqrt(np.mean(E**2, axis=0))\n",
        "    top_indices = np.argsort(rmsfe)[:q]\n",
        "\n",
        "    E_q = E[:, top_indices] # Shape (W, q)\n",
        "    bar_e = np.mean(E, axis=1) # Shape (W,)\n",
        "\n",
        "    # 2. Solve OLS\n",
        "    # b = (E_q.T E_q)^-1 E_q.T bar_e\n",
        "    try:\n",
        "        b, _, _, _ = np.linalg.lstsq(E_q, bar_e, rcond=None)\n",
        "    except np.linalg.LinAlgError:\n",
        "        # Fallback to equal weights on selected subset if singular\n",
        "        b = np.ones(q) / q\n",
        "\n",
        "    # 3. Construct weights\n",
        "    w_full = np.zeros(M)\n",
        "\n",
        "    # Normalize using absolute values\n",
        "    sum_abs = np.sum(np.abs(b))\n",
        "    if sum_abs > 0:\n",
        "        w_sub = np.abs(b) / sum_abs\n",
        "    else:\n",
        "        w_sub = np.ones(q) / q\n",
        "\n",
        "    w_full[top_indices] = w_sub\n",
        "\n",
        "    return w_full\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 27, Step 2: Variance/Risk Minimizing Schemes\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def weights_mv(E: np.ndarray, ridge: float = 1e-6) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes Minimum-Variance Weights (FC-MV) with Ridge Regularization.\n",
        "\n",
        "    Equation:\n",
        "        w = (Sigma + lambda*I)^-1 * 1 / (1^T * (Sigma + lambda*I)^-1 * 1)\n",
        "        where Sigma is the sample covariance of errors.\n",
        "\n",
        "    Args:\n",
        "        E (np.ndarray): Error matrix of shape (W, M).\n",
        "        ridge (float): Ridge regularization parameter lambda (default 1e-6).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Weight vector of shape (M,).\n",
        "    \"\"\"\n",
        "    # Compute sample covariance (demeaned)\n",
        "    Sigma = np.cov(E, rowvar=False)\n",
        "    M = E.shape[1]\n",
        "\n",
        "    # Apply ridge regularization\n",
        "    Sigma_reg = Sigma + ridge * np.eye(M)\n",
        "\n",
        "    # Compute inverse\n",
        "    try:\n",
        "        inv_Sigma = np.linalg.pinv(Sigma_reg)\n",
        "    except np.linalg.LinAlgError:\n",
        "        # Fallback to equal weights if singular despite ridge\n",
        "        return weights_ew(E)\n",
        "\n",
        "    # Compute unconstrained weights\n",
        "    ones = np.ones(M)\n",
        "    w_unc = inv_Sigma @ ones / (ones.T @ inv_Sigma @ ones)\n",
        "\n",
        "    # Project onto simplex (w >= 0, sum(w) = 1)\n",
        "    return project_simplex(w_unc)\n",
        "\n",
        "def weights_stack(E: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes Stacking Regression Weights (FC-STACK).\n",
        "\n",
        "    Optimization Problem:\n",
        "        min_w  w^T * (E^T E / W) * w\n",
        "        s.t.   sum(w) = 1, w >= 0\n",
        "\n",
        "    Args:\n",
        "        E (np.ndarray): Error matrix of shape (W, M).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Weight vector of shape (M,).\n",
        "    \"\"\"\n",
        "    W_obs, M = E.shape\n",
        "    # Compute second moment matrix (not centered covariance)\n",
        "    S = (E.T @ E) / W_obs\n",
        "\n",
        "    # Objective function: 0.5 * w^T S w\n",
        "    def objective(w):\n",
        "        return 0.5 * w.T @ S @ w\n",
        "\n",
        "    # Jacobian: S w\n",
        "    def jac(w):\n",
        "        return S @ w\n",
        "\n",
        "    # Constraints: sum(w) = 1\n",
        "    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
        "    # Bounds: 0 <= w <= 1\n",
        "    bounds = [(0, 1) for _ in range(M)]\n",
        "    # Initial guess: Equal weights\n",
        "    x0 = np.ones(M) / M\n",
        "\n",
        "    try:\n",
        "        res = minimize(objective, x0, jac=jac, bounds=bounds, constraints=constraints, method='SLSQP', tol=1e-6)\n",
        "        if res.success:\n",
        "            return res.x\n",
        "    except Exception:\n",
        "        pass\n",
        "    # Fallback to Inverse-RMSE if optimization fails\n",
        "    return weights_rmse(E)\n",
        "\n",
        "def weights_lad(E: np.ndarray, phi: float = 0.02) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes Penalized Least Absolute Deviation Weights (FC-LAD).\n",
        "\n",
        "    Optimization Problem (LP Form):\n",
        "        min_{w,u} (1/W) sum(u) + (phi/W) sum(w)\n",
        "        s.t.      -E w <= u\n",
        "                   E w <= u\n",
        "                   sum(w) = 1\n",
        "                   w >= 0, u >= 0\n",
        "\n",
        "    Args:\n",
        "        E (np.ndarray): Error matrix of shape (W, M).\n",
        "        phi (float): Penalty parameter (default 0.02).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Weight vector of shape (M,).\n",
        "    \"\"\"\n",
        "    W_obs, M = E.shape\n",
        "\n",
        "    # Objective vector c: [phi/W ... phi/W (M times), 1/W ... 1/W (W_obs times)]\n",
        "    # Variables x = [w_1...w_M, u_1...u_W]\n",
        "    c = np.concatenate([np.full(M, phi/W_obs), np.full(W_obs, 1.0/W_obs)])\n",
        "\n",
        "    # Inequality constraints A_ub @ x <= b_ub\n",
        "    # -E w - u <= 0  => [-E, -I] @ x <= 0\n",
        "    #  E w - u <= 0  => [ E, -I] @ x <= 0\n",
        "\n",
        "    I_W = np.eye(W_obs)\n",
        "    A_ub = np.vstack([\n",
        "        np.hstack([-E, -I_W]),\n",
        "        np.hstack([ E, -I_W])\n",
        "    ])\n",
        "    b_ub = np.zeros(2 * W_obs)\n",
        "\n",
        "    # Equality constraint: sum(w) = 1\n",
        "    # [1...1, 0...0] @ x = 1\n",
        "    A_eq = np.hstack([np.ones(M), np.zeros(W_obs)]).reshape(1, -1)\n",
        "    b_eq = np.array([1.0])\n",
        "\n",
        "    # Bounds: w >= 0, u >= 0\n",
        "    bounds = [(0, None) for _ in range(M + W_obs)]\n",
        "\n",
        "    try:\n",
        "        res = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n",
        "        if res.success:\n",
        "            w = res.x[:M]\n",
        "            # Normalize to ensure exact sum to 1 despite numerical noise\n",
        "            return w / np.sum(w)\n",
        "    except Exception:\n",
        "        pass\n",
        "    # Fallback to Equal Weights\n",
        "    return weights_ew(E)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 27, Step 3: DRO and AFTER Schemes\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_es(errors: np.ndarray, alpha: float = 0.10) -> float:\n",
        "    \"\"\"\n",
        "    Computes the Empirical Expected Shortfall (ES) of squared errors.\n",
        "\n",
        "    Definition:\n",
        "        ES_alpha = Mean of the worst (1-alpha)% losses.\n",
        "        Loss is defined as squared error.\n",
        "\n",
        "    Args:\n",
        "        errors (np.ndarray): Vector of forecast errors.\n",
        "        alpha (float): Confidence level (default 0.10).\n",
        "\n",
        "    Returns:\n",
        "        float: Expected Shortfall value.\n",
        "    \"\"\"\n",
        "    losses = errors**2\n",
        "    # Quantile for the worst losses (top 1-alpha percent)\n",
        "    q = np.quantile(losses, 1 - alpha)\n",
        "    # Tail losses\n",
        "    tail = losses[losses >= q]\n",
        "    if len(tail) == 0: return 0.0\n",
        "    return np.mean(tail)\n",
        "\n",
        "def weights_dro_es(E: np.ndarray, alpha: float = 0.10, eta: float = 5.0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes Distributionally Robust ES Weights (FC-DRO-ES).\n",
        "\n",
        "    Equation:\n",
        "        w_k ~ exp(-eta * (L_k - min(L)))\n",
        "        where L_k = ES_alpha(errors_k).\n",
        "\n",
        "    Args:\n",
        "        E (np.ndarray): Error matrix of shape (W, M).\n",
        "        alpha (float): ES confidence level.\n",
        "        eta (float): Robustness parameter (learning rate).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Weight vector of shape (M,).\n",
        "    \"\"\"\n",
        "    M = E.shape[1]\n",
        "    # Compute ES loss for each model\n",
        "    losses = np.array([compute_es(E[:, m], alpha) for m in range(M)])\n",
        "\n",
        "    # Stabilize exponent\n",
        "    L_tilde = losses - np.min(losses)\n",
        "\n",
        "    # Exponential weighting (lower loss -> higher weight)\n",
        "    numer = np.exp(-eta * L_tilde)\n",
        "    return numer / np.sum(numer)\n",
        "\n",
        "def weights_dro_mix(E: np.ndarray, alpha: float = 0.10, lam: float = 0.5, eta: float = 5.0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes Hybrid DRO Weights (FC-DRO-MIX).\n",
        "\n",
        "    Loss Function:\n",
        "        L_k = (1 - lambda) * MSE_k + lambda * ES_k\n",
        "\n",
        "    Equation:\n",
        "        w_k ~ exp(-eta * (L_k - min(L)))\n",
        "\n",
        "    Args:\n",
        "        E (np.ndarray): Error matrix of shape (W, M).\n",
        "        alpha (float): ES confidence level.\n",
        "        lam (float): Mixing parameter lambda (0 <= lam <= 1).\n",
        "        eta (float): Robustness parameter.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Weight vector of shape (M,).\n",
        "    \"\"\"\n",
        "    M = E.shape[1]\n",
        "    # Compute MSE\n",
        "    mse = np.mean(E**2, axis=0)\n",
        "    # Compute ES\n",
        "    es = np.array([compute_es(E[:, m], alpha) for m in range(M)])\n",
        "\n",
        "    # Combined Loss\n",
        "    loss = (1 - lam) * mse + lam * es\n",
        "\n",
        "    # Stabilize exponent\n",
        "    L_tilde = loss - np.min(loss)\n",
        "\n",
        "    # Exponential weighting\n",
        "    numer = np.exp(-eta * L_tilde)\n",
        "    return numer / np.sum(numer)\n",
        "\n",
        "def weights_drmv(E: np.ndarray, tau: float = 0.05) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes Distributionally Robust Mean-Variance Weights (FC-DRMV).\n",
        "\n",
        "    Equation:\n",
        "        w = (Sigma + tau*I)^-1 * 1 / (1^T * (Sigma + tau*I)^-1 * 1)\n",
        "        Followed by clipping negative weights and renormalization.\n",
        "\n",
        "    Args:\n",
        "        E (np.ndarray): Error matrix of shape (W, M).\n",
        "        tau (float): Regularization parameter (radius of ambiguity set).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Weight vector of shape (M,).\n",
        "    \"\"\"\n",
        "    # Compute sample covariance (demeaned)\n",
        "    Sigma = np.cov(E, rowvar=False)\n",
        "    M = E.shape[1]\n",
        "\n",
        "    # Regularize covariance\n",
        "    Sigma_reg = Sigma + tau * np.eye(M)\n",
        "\n",
        "    try:\n",
        "        inv = np.linalg.pinv(Sigma_reg)\n",
        "        ones = np.ones(M)\n",
        "        w = inv @ ones\n",
        "        w = w / (ones.T @ w)\n",
        "    except np.linalg.LinAlgError:\n",
        "        return weights_ew(E)\n",
        "\n",
        "    # Clip negative weights (heuristic projection) and normalize\n",
        "    w = np.maximum(w, 0)\n",
        "    return w / np.sum(w)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 27, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_forecast_weights(\n",
        "    df_errors: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrator to compute forecast combination weights for all methods over time.\n",
        "\n",
        "    This function iterates through the time series of forecast errors, applying each of the 14\n",
        "    combination schemes (Classic, Variance/Risk, DRO, AFTER) using a rolling window of historical errors.\n",
        "    It maintains recursive state for adaptive methods (AFTER) and handles the minimum observation period.\n",
        "\n",
        "    Args:\n",
        "        df_errors (pd.DataFrame): Wide format DataFrame of forecast errors.\n",
        "                                  Index: OriginDate (DatetimeIndex).\n",
        "                                  Columns: Model names (e.g., 'FADNS_1', ..., 'RF_10').\n",
        "        study_config (Dict[str, Any]): Frozen configuration dictionary containing parameters\n",
        "                                       for window sizes, decay rates, and method specifications.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame of computed weights.\n",
        "                      Index: OriginDate (same as input).\n",
        "                      Columns: MultiIndex (Method, Model), where Method is the combination scheme\n",
        "                               and Model corresponds to the input columns.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 27: Computing Forecast Combination Weights\")\n",
        "\n",
        "    # Extract configuration parameters\n",
        "    W = study_config[\"Forecast_Combination\"][\"General\"][\"rolling_window_W\"]\n",
        "    min_obs = study_config[\"Forecast_Combination\"][\"General\"][\"min_observations\"]\n",
        "    methods = study_config[\"Forecast_Combination\"][\"Methods\"]\n",
        "\n",
        "    # Initialize storage for weights\n",
        "    # Structure: Dict[Method Name, List of weight vectors]\n",
        "    weight_history = {m: [] for m in methods}\n",
        "\n",
        "    # Extract data structures for iteration\n",
        "    E_mat = df_errors.values\n",
        "    model_names = df_errors.columns\n",
        "    time_index = df_errors.index\n",
        "    T = len(time_index)\n",
        "    M = len(model_names)\n",
        "\n",
        "    # Initialize state for recursive AFTER methods\n",
        "    # Initial weights are uniform (1/M)\n",
        "    after_weights = {\n",
        "        \"AFTER-Rolling\": np.ones(M) / M,\n",
        "        \"AFTER-EWMA\": np.ones(M) / M,\n",
        "        \"AFTER-Simplified\": np.ones(M) / M\n",
        "    }\n",
        "    # Initialize variance state for AFTER-EWMA\n",
        "    after_var = np.zeros(M)\n",
        "\n",
        "    # Iterate through time steps\n",
        "    # Weights at time t are computed using errors available up to t-1 (indices 0 to t-1)\n",
        "    for t in range(T):\n",
        "        # Define the rolling window indices\n",
        "        # Window includes errors from [t-W, t-1]\n",
        "        start_idx = max(0, t - W)\n",
        "        end_idx = t # Exclusive upper bound for slicing\n",
        "\n",
        "        # Check if minimum observation requirement is met\n",
        "        if (end_idx - start_idx) < min_obs:\n",
        "            # During warm-up, assign Equal Weights to all methods\n",
        "            w_default = np.ones(M) / M\n",
        "            for method in methods:\n",
        "                weight_history[method].append(w_default)\n",
        "\n",
        "            # Initialize AFTER variance state once enough data is available\n",
        "            if t == min_obs - 1:\n",
        "                 # Use variance of the initial window for initialization\n",
        "                 E_init = E_mat[0:min_obs]\n",
        "                 after_var = np.var(E_init, axis=0)\n",
        "            continue\n",
        "\n",
        "        # Extract the window of historical errors\n",
        "        E_window = E_mat[start_idx:end_idx]\n",
        "\n",
        "        # Dictionary to hold weights for the current time step t\n",
        "        w_dict = {}\n",
        "\n",
        "        # --- 1. Classic Schemes ---\n",
        "        w_dict[\"FC-EW\"] = weights_ew(E_window)\n",
        "        w_dict[\"FC-RANK\"] = weights_rank(E_window)\n",
        "        w_dict[\"FC-RMSE\"] = weights_rmse(E_window)\n",
        "        w_dict[\"FC-MSE\"] = weights_mse(E_window)\n",
        "        w_dict[\"FC-OLS\"] = weights_ols(E_window)\n",
        "\n",
        "        # --- 2. Variance/Risk Minimizing Schemes ---\n",
        "        w_dict[\"FC-MV\"] = weights_mv(E_window)\n",
        "        w_dict[\"FC-STACK\"] = weights_stack(E_window)\n",
        "        w_dict[\"FC-LAD\"] = weights_lad(E_window)\n",
        "\n",
        "        # --- 3. Distributionally Robust Optimization (DRO) Schemes ---\n",
        "        w_dict[\"FC-DRO-ES\"] = weights_dro_es(E_window)\n",
        "        w_dict[\"FC-DRO-MIX\"] = weights_dro_mix(E_window)\n",
        "        w_dict[\"FC-DRMV\"] = weights_drmv(E_window)\n",
        "\n",
        "        # --- 4. Adaptive (AFTER) Schemes ---\n",
        "        # Retrieve the single most recent error vector e_{t-1} for recursive update\n",
        "        e_prev = E_mat[t-1]\n",
        "\n",
        "        # AFTER-Simplified (Homoskedastic)\n",
        "        w_prev = after_weights[\"AFTER-Simplified\"]\n",
        "\n",
        "        # Update rule: w_new ~ w_prev * exp(-0.5 * e^2)\n",
        "        w_new = w_prev * np.exp(-0.5 * e_prev**2)\n",
        "        w_new /= np.sum(w_new) # Normalize\n",
        "        after_weights[\"AFTER-Simplified\"] = w_new\n",
        "        w_dict[\"AFTER-Simplified\"] = w_new\n",
        "\n",
        "        # AFTER-Rolling (Rolling Variance)\n",
        "        # Variance is computed over the rolling window\n",
        "        v_roll = np.var(E_window, axis=0)\n",
        "        v_roll = np.maximum(v_roll, 1e-6) # Enforce floor\n",
        "        w_prev = after_weights[\"AFTER-Rolling\"]\n",
        "\n",
        "        # Update rule: w_new ~ w_prev * (v^-0.5) * exp(-0.5 * e^2 / v)\n",
        "        w_new = w_prev * (v_roll**-0.5) * np.exp(-0.5 * e_prev**2 / v_roll)\n",
        "        w_new /= np.sum(w_new)\n",
        "        after_weights[\"AFTER-Rolling\"] = w_new\n",
        "        w_dict[\"AFTER-Rolling\"] = w_new\n",
        "\n",
        "        # AFTER-EWMA (Exponentially Weighted Variance)\n",
        "        lam = 0.97 # Decay factor (frozen)\n",
        "\n",
        "        # Update variance state recursively\n",
        "        after_var = (1 - lam) * e_prev**2 + lam * after_var\n",
        "        v_ewma = np.maximum(after_var, 1e-6)\n",
        "\n",
        "        w_prev = after_weights[\"AFTER-EWMA\"]\n",
        "        w_new = w_prev * (v_ewma**-0.5) * np.exp(-0.5 * e_prev**2 / v_ewma)\n",
        "        w_new /= np.sum(w_new)\n",
        "        after_weights[\"AFTER-EWMA\"] = w_new\n",
        "        w_dict[\"AFTER-EWMA\"] = w_new\n",
        "\n",
        "        # Store computed weights for all methods\n",
        "        for method in methods:\n",
        "            weight_history[method].append(w_dict[method])\n",
        "\n",
        "    # Construct the final DataFrame with MultiIndex columns\n",
        "    frames = []\n",
        "    for method in methods:\n",
        "        # Create DataFrame for one method\n",
        "        df = pd.DataFrame(weight_history[method], index=time_index, columns=model_names)\n",
        "\n",
        "        # Add 'Method' level to columns\n",
        "        df.columns = pd.MultiIndex.from_product([[method], df.columns])\n",
        "        frames.append(df)\n",
        "\n",
        "    # Concatenate all method frames horizontally\n",
        "    df_all_weights = pd.concat(frames, axis=1)\n",
        "\n",
        "    return df_all_weights\n",
        "\n"
      ],
      "metadata": {
        "id": "Jle6FuFGGCrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 28: Compute combined forecasts, errors, and combination RMSFE tables\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 28: Compute combined forecasts, errors, and combination RMSFE tables\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 28, Step 1: Compute combined forecasts for each method\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def generate_combined_forecasts(\n",
        "    df_pool: pd.DataFrame,\n",
        "    df_weights: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the combined forecast for each combination method by taking the\n",
        "    dot product of the model forecasts and the method-specific weights.\n",
        "\n",
        "    Equation:\n",
        "        y_hat_c(t, h, tau, method) = sum_{m=1}^M w_{m,t,method} * y_hat_m(t, h, tau)\n",
        "\n",
        "    Args:\n",
        "        df_pool (pd.DataFrame): Wide-format DataFrame of candidate model forecasts.\n",
        "                                Index: MultiIndex (OriginDate, TargetDate, Horizon, Maturity).\n",
        "                                Columns: Model names (e.g., 'FADNS_1', ..., 'RF_10').\n",
        "        df_weights (pd.DataFrame): Wide-format DataFrame of combination weights.\n",
        "                                   Index: OriginDate.\n",
        "                                   Columns: MultiIndex (Method, Model).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Long-format DataFrame of combined forecasts.\n",
        "                      Columns: [OriginDate, TargetDate, Horizon, Maturity, Method, Forecast].\n",
        "    \"\"\"\n",
        "    # 1. Prepare Forecast Pool\n",
        "    # Reset index to make join keys available as columns\n",
        "    pool_reset = df_pool.reset_index()\n",
        "\n",
        "    # Identify the list of candidate models from the pool columns\n",
        "    # We assume all columns in df_pool are model forecasts\n",
        "    model_cols = df_pool.columns.tolist()\n",
        "\n",
        "    # 2. Prepare Weights\n",
        "    # df_weights has columns MultiIndex(Method, Model).\n",
        "    # We extract the list of methods.\n",
        "    methods = df_weights.columns.get_level_values(0).unique().tolist()\n",
        "\n",
        "    combined_forecasts_list = []\n",
        "\n",
        "    # 3. Iterate through each method to compute combined forecasts\n",
        "    for method in methods:\n",
        "        # Extract weights for this specific method\n",
        "        # Result is DataFrame with Index=OriginDate, Columns=Models\n",
        "        w_method = df_weights[method]\n",
        "\n",
        "        # Ensure weight columns match pool model columns\n",
        "        # We align weights to the pool's model columns.\n",
        "        # Any model in pool not in weights gets NaN (should not happen if consistent).\n",
        "        # Any model in weights not in pool is ignored.\n",
        "        common_models = [m for m in model_cols if m in w_method.columns]\n",
        "\n",
        "        if len(common_models) != len(model_cols):\n",
        "            logger.warning(f\"Method {method}: Mismatch in models. Pool has {len(model_cols)}, Weights have {len(w_method.columns)}. Using intersection of {len(common_models)}.\")\n",
        "\n",
        "        # 4. Merge Weights with Forecasts\n",
        "        # We join on 'OriginDate'.\n",
        "        # pool_reset has many rows per OriginDate (different horizons/maturities).\n",
        "        # w_method has 1 row per OriginDate.\n",
        "        # Left join ensures we keep all forecast targets.\n",
        "\n",
        "        # We only need the weights corresponding to the common models\n",
        "        w_subset = w_method[common_models]\n",
        "\n",
        "        # Perform the merge\n",
        "        # We suffix weight columns with '_w' to distinguish from forecast columns\n",
        "        merged = pd.merge(\n",
        "            pool_reset,\n",
        "            w_subset,\n",
        "            left_on=\"OriginDate\",\n",
        "            right_index=True,\n",
        "            how=\"left\",\n",
        "            suffixes=(\"\", \"_w\")\n",
        "        )\n",
        "\n",
        "        # 5. Compute Dot Product\n",
        "        # Forecasts: columns `common_models`\n",
        "        # Weights: columns `m + \"_w\"` for m in `common_models` (automatically handled by merge if names collide? No, merge on index with different column names keeps names.\n",
        "        # Wait, w_subset has columns \"FADNS_1\", etc. pool_reset has \"FADNS_1\".\n",
        "        # Merge will produce \"FADNS_1_x\" and \"FADNS_1_y\" if suffixes provided.\n",
        "\n",
        "        # Re-merge with explicit suffixes\n",
        "        merged = pd.merge(\n",
        "            pool_reset,\n",
        "            w_subset.add_suffix(\"_w\"),\n",
        "            left_on=\"OriginDate\",\n",
        "            right_index=True,\n",
        "            how=\"left\"\n",
        "        )\n",
        "\n",
        "        # Extract arrays\n",
        "        F = merged[common_models].values  # Forecasts (N_rows x M_models)\n",
        "        W = merged[[m + \"_w\" for m in common_models]].values # Weights (N_rows x M_models)\n",
        "\n",
        "        # Compute row-wise dot product: sum(F * W, axis=1)\n",
        "        # Handle NaNs: if weights are NaN (e.g. insufficient history), result is NaN.\n",
        "        combined_values = np.sum(F * W, axis=1)\n",
        "\n",
        "        # 6. Construct Result DataFrame for this method\n",
        "        method_df = pool_reset[[\"OriginDate\", \"TargetDate\", \"Horizon\", \"Maturity\"]].copy()\n",
        "        method_df[\"Method\"] = method\n",
        "        method_df[\"Forecast\"] = combined_values\n",
        "\n",
        "        combined_forecasts_list.append(method_df)\n",
        "\n",
        "    # 7. Concatenate all methods\n",
        "    df_combined = pd.concat(combined_forecasts_list, ignore_index=True)\n",
        "\n",
        "    return df_combined\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 28, Step 2: Compute combined forecast errors and RMSFE\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_combined_errors_and_rmsfe(\n",
        "    df_combined: pd.DataFrame,\n",
        "    df_actuals: pd.DataFrame,\n",
        "    multiplier: float = 100.0\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Computes forecast errors and RMSFE for the combined forecasts.\n",
        "\n",
        "    Args:\n",
        "        df_combined (pd.DataFrame): Long-format combined forecasts.\n",
        "        df_actuals (pd.DataFrame): Realized yields (Index: Date, Cols: Maturities).\n",
        "        multiplier (float): Unit conversion multiplier (e.g., 100 for bps).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "            - df_errors: Long-format DataFrame of errors.\n",
        "            - df_rmsfe: DataFrame of RMSFE values indexed by (Method, Horizon, Maturity).\n",
        "    \"\"\"\n",
        "    # 1. Prepare Actuals\n",
        "    # Melt to long format: TargetDate, Maturity, Actual\n",
        "    df_actuals_long = df_actuals.reset_index().melt(\n",
        "        id_vars=[df_actuals.index.name or \"index\"],\n",
        "        var_name=\"Maturity\",\n",
        "        value_name=\"Actual\"\n",
        "    )\n",
        "    # Rename index column to TargetDate\n",
        "    df_actuals_long = df_actuals_long.rename(columns={df_actuals_long.columns[0]: \"TargetDate\"})\n",
        "    df_actuals_long[\"TargetDate\"] = pd.to_datetime(df_actuals_long[\"TargetDate\"])\n",
        "\n",
        "    # 2. Merge Forecasts with Actuals\n",
        "    # Inner join: we only evaluate where we have realizations\n",
        "    df_merged = pd.merge(\n",
        "        df_combined,\n",
        "        df_actuals_long,\n",
        "        on=[\"TargetDate\", \"Maturity\"],\n",
        "        how=\"inner\"\n",
        "    )\n",
        "\n",
        "    # 3. Compute Errors\n",
        "    # Error = Actual - Forecast\n",
        "    df_merged[\"Error\"] = df_merged[\"Actual\"] - df_merged[\"Forecast\"]\n",
        "\n",
        "    # 4. Compute RMSFE\n",
        "    # Group by Method, Horizon, Maturity\n",
        "    # RMSFE = sqrt(mean(Error^2)) * multiplier\n",
        "    def calc_rmsfe(x):\n",
        "        return np.sqrt(np.mean(x**2)) * multiplier\n",
        "\n",
        "    df_rmsfe = df_merged.groupby([\"Method\", \"Horizon\", \"Maturity\"])[\"Error\"].apply(calc_rmsfe).reset_index()\n",
        "    df_rmsfe.rename(columns={\"Error\": \"RMSFE\"}, inplace=True)\n",
        "\n",
        "    # Return errors and RMSFE summary\n",
        "    return df_merged, df_rmsfe\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 28, Step 3: Produce combination RMSFE tables\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def format_rmsfe_table_h1(\n",
        "    df_rmsfe: pd.DataFrame,\n",
        "    canonical_maturities: List[str],\n",
        "    method_order: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Formats the RMSFE results into a table for Horizon h=1, matching the appendix format.\n",
        "\n",
        "    Args:\n",
        "        df_rmsfe (pd.DataFrame): RMSFE summary data.\n",
        "        canonical_maturities (List[str]): Ordered list of maturities for rows.\n",
        "        method_order (List[str]): Ordered list of methods for columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Pivot table (Rows: Maturity, Cols: Method).\n",
        "    \"\"\"\n",
        "    # Filter for Horizon = 1\n",
        "    df_h1 = df_rmsfe[df_rmsfe[\"Horizon\"] == 1]\n",
        "\n",
        "    if df_h1.empty:\n",
        "        logger.warning(\"No RMSFE data found for Horizon=1.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Pivot\n",
        "    pivot_table = df_h1.pivot(index=\"Maturity\", columns=\"Method\", values=\"RMSFE\")\n",
        "\n",
        "    # Reorder Rows (Maturities)\n",
        "    # Filter canonical list to those present in data\n",
        "    present_mats = [m for m in canonical_maturities if m in pivot_table.index]\n",
        "    pivot_table = pivot_table.reindex(present_mats)\n",
        "\n",
        "    # Reorder Columns (Methods)\n",
        "    present_methods = [m for m in method_order if m in pivot_table.columns]\n",
        "    pivot_table = pivot_table[present_methods]\n",
        "\n",
        "    return pivot_table\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 28, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_combination_results(\n",
        "    df_pool: pd.DataFrame,\n",
        "    df_weights: pd.DataFrame,\n",
        "    df_yields: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute Task 28: Compute combined forecasts and RMSFE tables.\n",
        "\n",
        "    Args:\n",
        "        df_pool: Wide-format forecast pool.\n",
        "        df_weights: Wide-format weights.\n",
        "        df_yields: Realized yields.\n",
        "        study_config: Frozen configuration.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "            - rmsfe_table_h1: Formatted RMSFE table for h=1.\n",
        "            - df_combined_forecasts: Long-format DataFrame of all combined forecasts.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 28: Computing Combined Forecasts and RMSFE\")\n",
        "\n",
        "    # Extract config\n",
        "    multiplier = study_config[\"Global_Settings\"][\"pct_points_to_bps_multiplier\"]\n",
        "    canonical_maturities = study_config[\"Global_Settings\"][\"us_zero_maturities\"]\n",
        "    method_order = study_config[\"Forecast_Combination\"][\"Methods\"]\n",
        "\n",
        "    # 1. Generate Combined Forecasts\n",
        "    df_combined_forecasts = generate_combined_forecasts(df_pool, df_weights)\n",
        "\n",
        "    # 2. Compute Errors and RMSFE\n",
        "    df_errors, df_rmsfe_summary = compute_combined_errors_and_rmsfe(\n",
        "        df_combined_forecasts,\n",
        "        df_yields,\n",
        "        multiplier\n",
        "    )\n",
        "\n",
        "    # 3. Format Table for h=1\n",
        "    rmsfe_table_h1 = format_rmsfe_table_h1(\n",
        "        df_rmsfe_summary,\n",
        "        canonical_maturities,\n",
        "        method_order\n",
        "    )\n",
        "\n",
        "    logger.info(\"Task 28 Completed Successfully.\")\n",
        "    return rmsfe_table_h1, df_combined_forecasts\n"
      ],
      "metadata": {
        "id": "s9tv8at7IoY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 29: Create the main-study orchestrator function specification\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 29: Main Study Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def run_main_study_pipeline(\n",
        "    df_us_yields_raw: pd.DataFrame,\n",
        "    df_us_macro_raw: pd.DataFrame,\n",
        "    df_us_benchmark_yields_raw: pd.DataFrame,\n",
        "    df_us_tic_raw: pd.DataFrame,\n",
        "    df_global_yields_raw: pd.DataFrame,\n",
        "    global_macro_panels_raw: Dict[str, pd.DataFrame],\n",
        "    raw_study_config: Dict[str, Any],\n",
        "    study_metadata: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete end-to-end pipeline for the main U.S. Treasury yield forecasting study.\n",
        "\n",
        "    This function executes Tasks 1 through 28 sequentially, ensuring all data validation,\n",
        "    cleansing, modeling, and combination steps are performed with strict fidelity to the\n",
        "    manuscript and the replication protocol.\n",
        "\n",
        "    Sequence:\n",
        "        1.  Validation (Schema, Metadata, Config Resolution)\n",
        "        2.  Data Cleansing & Alignment (Indices, Missingness, Interpolation)\n",
        "        3.  Structural Break Detection (CUSUM, PELT)\n",
        "        4.  DNS Parametric Modeling (Factors, VAR, Forecasts, RMSFE)\n",
        "        5.  FADNS Factor-Augmented Modeling (Macro Prep, PCA, VAR, Forecasts, RMSFE, Best-k)\n",
        "        6.  Random Forest Nonparametric Modeling (Features, Rolling Forecasts, RMSFE)\n",
        "        7.  Forecast Combination (Pool Construction, Weight Estimation, Combination RMSFE)\n",
        "\n",
        "    Args:\n",
        "        df_us_yields_raw: Raw U.S. zero-coupon yields DataFrame.\n",
        "        df_us_macro_raw: Raw U.S. macro predictors DataFrame.\n",
        "        df_us_benchmark_yields_raw: Raw U.S. benchmark yields DataFrame.\n",
        "        df_us_tic_raw: Raw U.S. TIC data DataFrame.\n",
        "        df_global_yields_raw: Raw Global 10Y yields DataFrame.\n",
        "        global_macro_panels_raw: Dictionary of raw Global macro panels.\n",
        "        raw_study_config: The initial configuration dictionary (containing placeholders).\n",
        "        study_metadata: The metadata dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing all intermediate and final artifacts.\n",
        "                        Keys include 'frozen_config', 'cleansed_data', 'breakpoints',\n",
        "                        'dns_results', 'fadns_results', 'rf_results', 'combination_results'.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Main Study Pipeline Execution\")\n",
        "\n",
        "    artifacts = {}\n",
        "\n",
        "    try:\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 1: Validation and Configuration (Tasks 1-3)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 1: Validation and Configuration ---\")\n",
        "\n",
        "        # Task 1: Schema Validation\n",
        "        validate_all_input_schemas(\n",
        "            df_us_yields_raw, df_us_macro_raw, df_us_benchmark_yields_raw,\n",
        "            df_us_tic_raw, df_global_yields_raw, global_macro_panels_raw,\n",
        "            raw_study_config, study_metadata\n",
        "        )\n",
        "\n",
        "        # Task 2: Metadata Validation\n",
        "        # We need the macro columns list for validation\n",
        "        us_macro_cols = df_us_macro_raw.columns.tolist()\n",
        "        validate_metadata_bundle(study_metadata, raw_study_config, us_macro_cols)\n",
        "\n",
        "        # Task 3: Config Resolution\n",
        "        frozen_config, config_hash = resolve_and_freeze_config(raw_study_config)\n",
        "        artifacts['frozen_config'] = frozen_config\n",
        "        artifacts['config_hash'] = config_hash\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 2: Data Cleansing and Alignment (Tasks 4-6)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 2: Data Cleansing and Alignment ---\")\n",
        "\n",
        "        # Task 4: Index Alignment\n",
        "        (df_us_yields_aligned, df_us_macro_aligned, df_benchmark_aligned,\n",
        "         df_tic_aligned, df_global_yields_aligned, global_macro_panels_aligned) = \\\n",
        "            cleanse_and_align_indices(\n",
        "                df_us_yields_raw, df_us_macro_raw, df_us_benchmark_yields_raw,\n",
        "                df_us_tic_raw, df_global_yields_raw, global_macro_panels_raw,\n",
        "                frozen_config\n",
        "            )\n",
        "\n",
        "        # Task 5: Yield Cleansing\n",
        "        df_us_yields_clean = cleanse_yield_panel(\n",
        "            df_us_yields_aligned, frozen_config, study_metadata\n",
        "        )\n",
        "\n",
        "        # Task 6: Macro Interpolation\n",
        "        df_us_macro_clean, global_macro_panels_clean = interpolate_macro_panels(\n",
        "            df_us_macro_aligned, global_macro_panels_aligned, study_metadata, frozen_config\n",
        "        )\n",
        "\n",
        "        # Store cleansed data\n",
        "        artifacts['cleansed_data'] = {\n",
        "            'us_yields': df_us_yields_clean,\n",
        "            'us_macro': df_us_macro_clean,\n",
        "            'benchmark_yields': df_benchmark_aligned, # No specific cleansing task defined beyond alignment\n",
        "            'tic': df_tic_aligned,\n",
        "            'global_yields': df_global_yields_aligned,\n",
        "            'global_macro': global_macro_panels_clean\n",
        "        }\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 3: Structural Break Detection (Tasks 7-8)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 3: Structural Break Detection ---\")\n",
        "\n",
        "        # Task 7: CUSUM\n",
        "        cusum_results = run_cusum_tests(df_us_yields_clean, frozen_config)\n",
        "\n",
        "        # Task 8: PELT\n",
        "        pelt_table, pelt_raw = run_pelt_detection(df_us_yields_clean, frozen_config)\n",
        "\n",
        "        artifacts['breakpoints'] = {\n",
        "            'cusum_table': cusum_results,\n",
        "            'pelt_table': pelt_table,\n",
        "            'pelt_raw_dates': pelt_raw\n",
        "        }\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 4: DNS Parametric Modeling (Tasks 9-13)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 4: DNS Modeling ---\")\n",
        "\n",
        "        # Task 9: Loading Matrix\n",
        "        L, maturities = construct_dns_loading_matrix(frozen_config, study_metadata)\n",
        "\n",
        "        # Task 10: Factor Extraction\n",
        "        df_dns_factors = extract_dns_factors(df_us_yields_clean, L, maturities)\n",
        "\n",
        "        # Task 11: Rolling VAR\n",
        "        dns_var_params = estimate_rolling_dns_var(df_dns_factors, frozen_config)\n",
        "\n",
        "        # Task 12: Forecasting\n",
        "        df_dns_forecasts = generate_dns_forecasts(dns_var_params, df_dns_factors, L, frozen_config)\n",
        "\n",
        "        # Task 13: RMSFE\n",
        "        dns_rmsfe_table = compute_dns_rmsfe(df_dns_forecasts, df_us_yields_clean, frozen_config)\n",
        "\n",
        "        artifacts['dns_results'] = {\n",
        "            'loading_matrix': L,\n",
        "            'factors': df_dns_factors,\n",
        "            'var_params': dns_var_params,\n",
        "            'forecasts': df_dns_forecasts,\n",
        "            'rmsfe_table': dns_rmsfe_table\n",
        "        }\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 5: FADNS Factor-Augmented Modeling (Tasks 14-20)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 5: FADNS Modeling ---\")\n",
        "\n",
        "        # Task 14: Macro Preprocessing (Lagging)\n",
        "        macro_blocks = preprocess_fadns_macro(df_us_macro_clean, frozen_config, study_metadata)\n",
        "\n",
        "        # Task 15: Stationarity (ADF)\n",
        "        macro_blocks_diff = apply_rolling_adf_filtering(macro_blocks, frozen_config)\n",
        "\n",
        "        # Task 16: Standardization\n",
        "        macro_blocks_std = standardize_rolling_macro_blocks(macro_blocks_diff, frozen_config)\n",
        "\n",
        "        # Task 17: PCA Factors\n",
        "        df_macro_factors = construct_rolling_pca_factors(macro_blocks_std, frozen_config)\n",
        "\n",
        "        # Task 18: Estimation & Forecasting\n",
        "        # Note: Uses DNS factors from Phase 4\n",
        "        df_fadns_beta_forecasts = run_fadns_estimation_and_forecast(\n",
        "            df_dns_factors, df_macro_factors, frozen_config\n",
        "        )\n",
        "\n",
        "        # Task 19: Yield Mapping & RMSFE\n",
        "        fadns_rmsfe_tables, df_fadns_yield_forecasts = compute_fadns_rmsfe(\n",
        "            df_fadns_beta_forecasts, df_us_yields_clean, L, frozen_config\n",
        "        )\n",
        "\n",
        "        # Task 20: Best-k Selection\n",
        "        df_best_k, df_best_rmsfe = select_best_fadns_k(fadns_rmsfe_tables, frozen_config)\n",
        "\n",
        "        artifacts['fadns_results'] = {\n",
        "            'macro_factors': df_macro_factors,\n",
        "            'beta_forecasts': df_fadns_beta_forecasts,\n",
        "            'yield_forecasts': df_fadns_yield_forecasts,\n",
        "            'rmsfe_tables': fadns_rmsfe_tables,\n",
        "            'best_k_table': df_best_k,\n",
        "            'best_rmsfe_table': df_best_rmsfe\n",
        "        }\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 6: Random Forest Modeling (Tasks 21-25)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 6: Random Forest Modeling ---\")\n",
        "\n",
        "        # Task 21: Feature Construction\n",
        "        df_rf_macro_feat, dict_rf_yield_feat = construct_rf_features(\n",
        "            df_us_macro_clean, df_us_yields_clean, frozen_config\n",
        "        )\n",
        "\n",
        "        # Task 24: Forecasting (includes Normalization Task 22 & Training Task 23)\n",
        "        df_rf_forecasts = generate_rf_forecasts(\n",
        "            df_rf_macro_feat, dict_rf_yield_feat, df_us_yields_clean, frozen_config\n",
        "        )\n",
        "\n",
        "        # Task 25: RMSFE Summary\n",
        "        rf_summary_table, rf_numeric_stats = compute_rf_rmsfe_summary(\n",
        "            df_rf_forecasts, df_us_yields_clean, frozen_config\n",
        "        )\n",
        "\n",
        "        artifacts['rf_results'] = {\n",
        "            'forecasts': df_rf_forecasts,\n",
        "            'rmsfe_summary': rf_summary_table,\n",
        "            'rmsfe_stats': rf_numeric_stats\n",
        "        }\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 7: Forecast Combination (Tasks 26-28)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 7: Forecast Combination ---\")\n",
        "\n",
        "        # Task 26: Pool Construction\n",
        "        df_pool, df_pool_errors = prepare_forecast_combination_data(\n",
        "            df_fadns_yield_forecasts, df_rf_forecasts, df_us_yields_clean, frozen_config\n",
        "        )\n",
        "\n",
        "        # Task 27: Weight Estimation\n",
        "        df_weights = compute_forecast_weights(df_pool_errors, frozen_config)\n",
        "\n",
        "        # Task 28: Combination Results\n",
        "        combo_rmsfe_table, df_combined_forecasts = compute_combination_results(\n",
        "            df_pool, df_weights, df_us_yields_clean, frozen_config\n",
        "        )\n",
        "\n",
        "        artifacts['combination_results'] = {\n",
        "            'pool_forecasts': df_pool,\n",
        "            'pool_errors': df_pool_errors,\n",
        "            'weights': df_weights,\n",
        "            'combined_forecasts': df_combined_forecasts,\n",
        "            'rmsfe_table_h1': combo_rmsfe_table\n",
        "        }\n",
        "\n",
        "        logger.info(\"Main Study Pipeline Completed Successfully.\")\n",
        "        return artifacts\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Pipeline Failed: {str(e)}\", exc_info=True)\n",
        "        raise\n"
      ],
      "metadata": {
        "id": "s-i2FZOTRAwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 30: Create the benchmark-RF orchestrator function specification (for robustness analysis)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 30: Benchmark RF Orchestrator (Multi vs Single)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 30, Step 1: Feature Construction Helpers\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def construct_benchmark_features(\n",
        "    df_macro: pd.DataFrame,\n",
        "    df_yields: pd.DataFrame,\n",
        "    macro_lags: List[int],\n",
        "    yield_lags: List[int],\n",
        "    mode: str = \"single\",\n",
        "    target_maturity: Optional[str] = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs features for benchmark analysis.\n",
        "\n",
        "    Args:\n",
        "        df_macro: Macro data.\n",
        "        df_yields: Benchmark yields (9 maturities).\n",
        "        macro_lags: List of macro lags.\n",
        "        yield_lags: List of yield lags.\n",
        "        mode: 'single' or 'multi'.\n",
        "        target_maturity: Required if mode='single'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Feature matrix.\n",
        "    \"\"\"\n",
        "    # 1. Macro Features (Always included)\n",
        "    macro_feat_list = []\n",
        "    for lag in macro_lags:\n",
        "        shifted = df_macro.shift(lag)\n",
        "        shifted.columns = [f\"Macro_{c}_L{lag}\" for c in df_macro.columns]\n",
        "        macro_feat_list.append(shifted)\n",
        "    df_macro_feat = pd.concat(macro_feat_list, axis=1)\n",
        "\n",
        "    # 2. Yield Features\n",
        "    yield_feat_list = []\n",
        "\n",
        "    if mode == \"single\":\n",
        "        if target_maturity is None:\n",
        "            raise ValueError(\"target_maturity required for single mode\")\n",
        "        # Lags of specific maturity\n",
        "        series = df_yields[[target_maturity]]\n",
        "        for lag in yield_lags:\n",
        "            shifted = series.shift(lag)\n",
        "            shifted.columns = [f\"Yield_{target_maturity}_L{lag}\"]\n",
        "            yield_feat_list.append(shifted)\n",
        "\n",
        "    elif mode == \"multi\":\n",
        "        # Lags of ALL maturities\n",
        "        for col in df_yields.columns:\n",
        "            series = df_yields[[col]]\n",
        "            for lag in yield_lags:\n",
        "                shifted = series.shift(lag)\n",
        "                shifted.columns = [f\"Yield_{col}_L{lag}\"]\n",
        "                yield_feat_list.append(shifted)\n",
        "\n",
        "    df_yield_feat = pd.concat(yield_feat_list, axis=1)\n",
        "\n",
        "    # Combine\n",
        "    df_features = pd.concat([df_macro_feat, df_yield_feat], axis=1)\n",
        "    return df_features\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 30, Step 2: Training and Forecasting Logic\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def execute_benchmark_rf(\n",
        "    df_features: pd.DataFrame,\n",
        "    df_targets: pd.DataFrame, # Single column or Multi columns\n",
        "    study_config: Dict[str, Any],\n",
        "    seeds: List[int],\n",
        "    horizon: int,\n",
        "    eval_start_date: str\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Executes RF training and forecasting for a specific feature/target set.\n",
        "\n",
        "    Args:\n",
        "        df_features: Feature DataFrame.\n",
        "        df_targets: Target DataFrame (shifted to align with features? No, we handle alignment here).\n",
        "        study_config: Config.\n",
        "        seeds: Seeds.\n",
        "        horizon: Forecast horizon.\n",
        "        eval_start_date: Start of evaluation period.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Forecast records.\n",
        "    \"\"\"\n",
        "    window_size = study_config[\"Model_Architectures\"][\"Random_Forest\"][\"rolling_window_W\"]\n",
        "\n",
        "    # Align Features and Targets\n",
        "    # Target at time t is y_{t+h}\n",
        "    # We align by shifting target back by h, or using date lookup\n",
        "\n",
        "    # Let's use date lookup for robustness\n",
        "    # Valid origins: dates in df_features where we have history AND target exists\n",
        "\n",
        "    # Filter for evaluation period\n",
        "    eval_start_ts = pd.Timestamp(eval_start_date)\n",
        "\n",
        "    # Identify feasible origins\n",
        "    # Must have window_size history in features\n",
        "    # Must be >= eval_start_ts\n",
        "    valid_origins = []\n",
        "    feature_dates = df_features.index\n",
        "\n",
        "    # Optimization: Pre-calculate valid indices\n",
        "    # Start index in features: window_size - 1\n",
        "    start_idx = window_size - 1\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Iterate origins\n",
        "    for i in range(start_idx, len(feature_dates)):\n",
        "        origin_date = feature_dates[i]\n",
        "\n",
        "        if origin_date < eval_start_ts:\n",
        "            continue\n",
        "\n",
        "        # Define Training Window\n",
        "        train_start_idx = i - window_size + 1\n",
        "        train_end_idx = i\n",
        "\n",
        "        # Get Training Data\n",
        "        X_train_raw = df_features.iloc[train_start_idx : train_end_idx + 1].values\n",
        "        X_test_raw = df_features.iloc[i].values.reshape(1, -1)\n",
        "\n",
        "        # Get Training Targets\n",
        "        # Target for X_s is Y_{s+h}\n",
        "        train_dates = df_features.index[train_start_idx : train_end_idx + 1]\n",
        "        target_dates = train_dates + DateOffset(months=horizon) + MonthEnd(0)\n",
        "\n",
        "        # Lookup targets\n",
        "        # Reindex handles missing future dates by putting NaN\n",
        "        Y_train_raw = df_targets.reindex(target_dates).values\n",
        "\n",
        "        # Drop rows with NaN targets\n",
        "        # For multi-output, if ANY target is NaN, drop row? Or impute?\n",
        "        # Standard: drop row if any target missing\n",
        "        if Y_train_raw.ndim == 1:\n",
        "            valid_mask = np.isfinite(Y_train_raw)\n",
        "        else:\n",
        "            valid_mask = np.isfinite(Y_train_raw).all(axis=1)\n",
        "\n",
        "        if np.sum(valid_mask) < window_size * 0.9:\n",
        "            continue\n",
        "\n",
        "        X_train = X_train_raw[valid_mask]\n",
        "        Y_train = Y_train_raw[valid_mask]\n",
        "\n",
        "        # Normalize\n",
        "        # X\n",
        "        x_min = X_train.min(axis=0)\n",
        "        x_max = X_train.max(axis=0)\n",
        "        x_range = x_max - x_min\n",
        "        x_range[x_range == 0] = 1.0\n",
        "        X_train_norm = (X_train - x_min) / x_range\n",
        "        X_test_norm = (X_test_raw - x_min) / x_range\n",
        "\n",
        "        # Y\n",
        "        y_min = Y_train.min(axis=0)\n",
        "        y_max = Y_train.max(axis=0)\n",
        "        y_range = y_max - y_min\n",
        "        y_range[y_range == 0] = 1.0\n",
        "        Y_train_norm = (Y_train - y_min) / y_range\n",
        "\n",
        "        # Train/Predict per Seed\n",
        "        for seed in seeds:\n",
        "            # Define CV\n",
        "            rf = RandomForestRegressor(random_state=seed)\n",
        "\n",
        "            # Minimal grid for speed in this example, but should match main study\n",
        "            param_dist = study_config[\"Model_Architectures\"][\"Random_Forest\"][\"cv\"][\"hyperparam_space\"]\n",
        "\n",
        "            tscv = TimeSeriesSplit(n_splits=3) # Reduced splits for benchmark speed? Or match main? Match main.\n",
        "\n",
        "            search = RandomizedSearchCV(\n",
        "                rf, param_dist, n_iter=10, # Reduced iter for benchmark? Or match? Match main config.\n",
        "                cv=tscv, scoring=\"neg_mean_squared_error\", random_state=seed, n_jobs=-1\n",
        "            )\n",
        "            search.fit(X_train_norm, Y_train_norm)\n",
        "            model = search.best_estimator_\n",
        "\n",
        "            # Predict\n",
        "            y_pred_norm = model.predict(X_test_norm)\n",
        "\n",
        "            # Inverse Transform\n",
        "            y_pred = y_pred_norm * y_range + y_min\n",
        "\n",
        "            # Store\n",
        "            # If multi-output, y_pred is (1, 9). If single, (1,).\n",
        "            # We need to map back to maturity names\n",
        "            target_cols = df_targets.columns\n",
        "            if len(target_cols) == 1:\n",
        "                # Single\n",
        "                val = float(y_pred[0])\n",
        "                results.append({\n",
        "                    \"OriginDate\": origin_date,\n",
        "                    \"Horizon\": horizon,\n",
        "                    \"Seed\": seed,\n",
        "                    \"Maturity\": target_cols[0],\n",
        "                    \"Forecast\": val\n",
        "                })\n",
        "            else:\n",
        "                # Multi\n",
        "                vals = y_pred.flatten()\n",
        "                for m_idx, mat in enumerate(target_cols):\n",
        "                    results.append({\n",
        "                        \"OriginDate\": origin_date,\n",
        "                        \"Horizon\": horizon,\n",
        "                        \"Seed\": seed,\n",
        "                        \"Maturity\": mat,\n",
        "                        \"Forecast\": vals[m_idx]\n",
        "                    })\n",
        "\n",
        "    return results\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 30, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_benchmark_rf_analysis(\n",
        "    df_benchmark_yields: pd.DataFrame,\n",
        "    df_macro: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrator for Benchmark RF Analysis (Multi vs Single).\n",
        "\n",
        "    Args:\n",
        "        df_benchmark_yields: Benchmark yields (9 cols).\n",
        "        df_macro: Macro data.\n",
        "        study_config: Config.\n",
        "\n",
        "    Returns:\n",
        "        Dict containing 'multi_results', 'single_results', 'comparison_table'.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Benchmark RF Analysis\")\n",
        "\n",
        "    seeds = [8270, 1860] # Fixed per manuscript\n",
        "    horizons = study_config[\"Global_Settings\"][\"forecast_horizons\"]\n",
        "    eval_start = study_config[\"Raw_Data_Schemas\"][\"US_Benchmark_Yields\"][\"evaluation_start_date\"]\n",
        "\n",
        "    macro_lags = study_config[\"Preprocessing_Params\"][\"Lag_Structure\"][\"macro_lags\"]\n",
        "    yield_lags = study_config[\"Preprocessing_Params\"][\"Lag_Structure\"][\"yield_lags\"]\n",
        "\n",
        "    all_multi_forecasts = []\n",
        "    all_single_forecasts = []\n",
        "\n",
        "    # 1. Multi-Output RF\n",
        "    logger.info(\"Running Multi-Output RF...\")\n",
        "    # Construct features (Multi mode)\n",
        "    df_feat_multi = construct_benchmark_features(\n",
        "        df_macro, df_benchmark_yields, macro_lags, yield_lags, mode=\"multi\"\n",
        "    )\n",
        "\n",
        "    for h in horizons:\n",
        "        res = execute_benchmark_rf(\n",
        "            df_feat_multi, df_benchmark_yields, study_config, seeds, h, eval_start\n",
        "        )\n",
        "        all_multi_forecasts.extend(res)\n",
        "\n",
        "    # 2. Single-Maturity RF\n",
        "    logger.info(\"Running Single-Maturity RF...\")\n",
        "    for maturity in df_benchmark_yields.columns:\n",
        "        # Construct features (Single mode)\n",
        "        df_feat_single = construct_benchmark_features(\n",
        "            df_macro, df_benchmark_yields, macro_lags, yield_lags, mode=\"single\", target_maturity=maturity\n",
        "        )\n",
        "\n",
        "        for h in horizons:\n",
        "            # Target is single column\n",
        "            df_target_single = df_benchmark_yields[[maturity]]\n",
        "            res = execute_benchmark_rf(\n",
        "                df_feat_single, df_target_single, study_config, seeds, h, eval_start\n",
        "            )\n",
        "            all_single_forecasts.extend(res)\n",
        "\n",
        "    # 3. Compute RMSFE and Compare\n",
        "    # Convert to DataFrame\n",
        "    df_multi = pd.DataFrame(all_multi_forecasts)\n",
        "    df_single = pd.DataFrame(all_single_forecasts)\n",
        "\n",
        "    # Helper to calc RMSFE\n",
        "    def calc_rmsfe(df_forecasts, label):\n",
        "        # Merge with actuals\n",
        "        # Actuals need to be melted\n",
        "        actuals_long = df_benchmark_yields.reset_index().melt(\n",
        "            id_vars=[df_benchmark_yields.index.name or \"index\"], var_name=\"Maturity\", value_name=\"Actual\"\n",
        "        )\n",
        "        actuals_long = actuals_long.rename(columns={actuals_long.columns[0]: \"TargetDate\"})\n",
        "\n",
        "        # Add TargetDate to forecasts\n",
        "        # Re-calculating TargetDate here for safety\n",
        "        df_forecasts[\"TargetDate\"] = df_forecasts.apply(\n",
        "            lambda row: row[\"OriginDate\"] + DateOffset(months=row[\"Horizon\"]) + MonthEnd(0), axis=1\n",
        "        )\n",
        "\n",
        "        merged = pd.merge(df_forecasts, actuals_long, on=[\"TargetDate\", \"Maturity\"], how=\"inner\")\n",
        "        merged[\"Error\"] = merged[\"Actual\"] - merged[\"Forecast\"]\n",
        "\n",
        "        # Group by Seed, Horizon, Maturity\n",
        "        rmsfe = merged.groupby([\"Seed\", \"Horizon\", \"Maturity\"])[\"Error\"].apply(\n",
        "            lambda x: np.sqrt(np.mean(x**2)) * 100.0 # bps\n",
        "        ).reset_index()\n",
        "        rmsfe.rename(columns={\"Error\": label}, inplace=True)\n",
        "        return rmsfe\n",
        "\n",
        "    rmsfe_multi = calc_rmsfe(df_multi, \"RMSFE_Multi\")\n",
        "    rmsfe_single = calc_rmsfe(df_single, \"RMSFE_Single\")\n",
        "\n",
        "    # Merge comparison\n",
        "    comparison = pd.merge(rmsfe_multi, rmsfe_single, on=[\"Seed\", \"Horizon\", \"Maturity\"])\n",
        "    comparison[\"Diff\"] = comparison[\"RMSFE_Multi\"] - comparison[\"RMSFE_Single\"]\n",
        "\n",
        "    logger.info(\"Benchmark Analysis Completed.\")\n",
        "\n",
        "    return {\n",
        "        \"multi_forecasts\": df_multi,\n",
        "        \"single_forecasts\": df_single,\n",
        "        \"comparison_table\": comparison\n",
        "    }\n"
      ],
      "metadata": {
        "id": "-4JvY_GanN3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 31: Execute the benchmark U.S. Treasury RF robustness analysis (multi-output vs single-maturity)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 31: Execute Benchmark RF Analysis\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 31, Step 1 & 2: Execute Multi and Single RF (via Task 30 Orchestrator)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Note: Task 30 provided the logic in `run_benchmark_rf_analysis`.\n",
        "# Task 31 is about executing it and formatting the specific tables required.\n",
        "# We assume `run_benchmark_rf_analysis` is available.\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 31, Step 3: Produce Comparison Tables\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def format_benchmark_tables(\n",
        "    comparison_df: pd.DataFrame,\n",
        "    canonical_maturities: List[str]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Formats the benchmark analysis results into three tables:\n",
        "    1. Multi-Output RMSFE\n",
        "    2. Single-Maturity RMSFE\n",
        "    3. Difference (Multi - Single)\n",
        "\n",
        "    Args:\n",
        "        comparison_df (pd.DataFrame): Merged results with columns\n",
        "                                      ['Seed', 'Horizon', 'Maturity', 'RMSFE_Multi', 'RMSFE_Single', 'Diff'].\n",
        "        canonical_maturities (List[str]): Ordered list of maturities.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: The three formatted tables.\n",
        "    \"\"\"\n",
        "    # Pivot Helper\n",
        "    def make_pivot(value_col):\n",
        "        # Pivot: Index=[Seed, Horizon], Columns=Maturity\n",
        "        pivot = comparison_df.pivot(index=[\"Seed\", \"Horizon\"], columns=\"Maturity\", values=value_col)\n",
        "\n",
        "        # Reorder columns\n",
        "        available = [m for m in canonical_maturities if m in pivot.columns]\n",
        "        pivot = pivot[available]\n",
        "\n",
        "        return pivot\n",
        "\n",
        "    multi_table = make_pivot(\"RMSFE_Multi\")\n",
        "    single_table = make_pivot(\"RMSFE_Single\")\n",
        "    diff_table = make_pivot(\"Diff\")\n",
        "\n",
        "    return multi_table, single_table, diff_table\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 31, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def execute_benchmark_analysis(\n",
        "    df_benchmark_yields: pd.DataFrame,\n",
        "    df_macro: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute the benchmark RF analysis and produce comparison tables.\n",
        "\n",
        "    Args:\n",
        "        df_benchmark_yields: Benchmark yields.\n",
        "        df_macro: Macro data.\n",
        "        study_config: Frozen configuration.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (Multi RMSFE Table, Single RMSFE Table, Difference Table).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 31: Executing Benchmark RF Analysis\")\n",
        "\n",
        "    # 1. Run Analysis (Task 30 logic)\n",
        "    results = run_benchmark_rf_analysis(df_benchmark_yields, df_macro, study_config)\n",
        "    comparison_df = results[\"comparison_table\"]\n",
        "\n",
        "    # 2. Format Tables\n",
        "    benchmark_mats = study_config[\"Raw_Data_Schemas\"][\"US_Benchmark_Yields\"][\"columns\"]\n",
        "\n",
        "    multi_table, single_table, diff_table = format_benchmark_tables(comparison_df, benchmark_mats)\n",
        "\n",
        "    logger.info(\"Task 31 Completed Successfully.\")\n",
        "    return multi_table, single_table, diff_table\n"
      ],
      "metadata": {
        "id": "rdWPVQxKtNAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 32: Create the TIC augmentation orchestrator function specification (for robustness analysis)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 32: TIC Augmentation Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 32, Step 1 & 2: Define Interface and Sample Restriction\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_tic_analysis_data(\n",
        "    df_benchmark: pd.DataFrame,\n",
        "    df_macro: pd.DataFrame,\n",
        "    df_tic: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, str]:\n",
        "    \"\"\"\n",
        "    Prepares data for TIC robustness analysis by aligning samples.\n",
        "\n",
        "    Args:\n",
        "        df_benchmark: Benchmark yields.\n",
        "        df_macro: Macro data.\n",
        "        df_tic: TIC data.\n",
        "        study_config: Config.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (df_baseline_features, df_augmented_features, eval_start_date).\n",
        "    \"\"\"\n",
        "    # 1. Determine Evaluation Start Date\n",
        "    # TIC starts 2014-09-30.\n",
        "    # We need window_size history before the first forecast.\n",
        "    # But execute_benchmark_rf handles the window lookback.\n",
        "    # We just need to set the evaluation start date to where TIC is available.\n",
        "\n",
        "    tic_start = df_tic.index.min()\n",
        "\n",
        "    # We set evaluation start eval_start must be >= Data_Start + Window.\n",
        "    window_size = study_config[\"Model_Architectures\"][\"Random_Forest\"][\"rolling_window_W\"]\n",
        "\n",
        "    # Calculate safe start date\n",
        "    # We need window_size months of TIC data before the first forecast origin.\n",
        "    # TIC start index + window_size\n",
        "    if len(df_tic) < window_size:\n",
        "        raise ValueError(\"TIC history too short for window size.\")\n",
        "\n",
        "    # Get date at position window_size\n",
        "    safe_start_idx = window_size\n",
        "    if safe_start_idx < len(df_tic):\n",
        "        eval_start_date = df_tic.index[safe_start_idx].strftime('%Y-%m-%d')\n",
        "    else:\n",
        "        raise ValueError(\"TIC history insufficient.\")\n",
        "\n",
        "    logger.info(f\"TIC Analysis Evaluation Start Date: {eval_start_date}\")\n",
        "\n",
        "    # 2. Construct Features\n",
        "    macro_lags = study_config[\"Preprocessing_Params\"][\"Lag_Structure\"][\"macro_lags\"]\n",
        "    yield_lags = study_config[\"Preprocessing_Params\"][\"Lag_Structure\"][\"yield_lags\"]\n",
        "\n",
        "    # Baseline Features\n",
        "    # We use \"multi\" mode (all yields) as the strong benchmark\n",
        "    df_feat_baseline = construct_benchmark_features(\n",
        "        df_macro, df_benchmark, macro_lags, yield_lags, mode=\"multi\"\n",
        "    )\n",
        "\n",
        "    # Augmented Features\n",
        "    # We add TIC to Macro\n",
        "    # TIC is treated as macro (lag 1..60)\n",
        "    # We construct TIC features separately and concat\n",
        "    tic_feat_list = []\n",
        "    for lag in macro_lags:\n",
        "        shifted = df_tic.shift(lag)\n",
        "        shifted.columns = [f\"TIC_{c}_L{lag}\" for c in df_tic.columns]\n",
        "        tic_feat_list.append(shifted)\n",
        "\n",
        "    df_tic_feat = pd.concat(tic_feat_list, axis=1)\n",
        "\n",
        "    df_feat_augmented = pd.concat([df_feat_baseline, df_tic_feat], axis=1)\n",
        "\n",
        "    return df_feat_baseline, df_feat_augmented, eval_start_date\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 32, Step 3: Define Comparison Output\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_tic_improvement(\n",
        "    rmsfe_baseline: pd.DataFrame,\n",
        "    rmsfe_augmented: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the improvement in RMSFE from adding TIC variables.\n",
        "\n",
        "    Args:\n",
        "        rmsfe_baseline: DataFrame with 'RMSFE' column.\n",
        "        rmsfe_augmented: DataFrame with 'RMSFE' column.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with 'Delta_RMSFE' (Augmented - Baseline).\n",
        "                      Negative values indicate improvement.\n",
        "    \"\"\"\n",
        "    # Merge on Seed, Horizon, Maturity\n",
        "    merged = pd.merge(\n",
        "        rmsfe_augmented,\n",
        "        rmsfe_baseline,\n",
        "        on=[\"Seed\", \"Horizon\", \"Maturity\"],\n",
        "        suffixes=(\"_TIC\", \"_Base\")\n",
        "    )\n",
        "\n",
        "    merged[\"Delta_RMSFE\"] = merged[\"RMSFE_TIC\"] - merged[\"RMSFE_Base\"]\n",
        "\n",
        "    return merged\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 32, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_tic_robustness_analysis(\n",
        "    df_benchmark_yields: pd.DataFrame,\n",
        "    df_macro: pd.DataFrame,\n",
        "    df_tic: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrator for TIC Robustness Analysis.\n",
        "\n",
        "    Args:\n",
        "        df_benchmark_yields: Benchmark yields.\n",
        "        df_macro: Macro data.\n",
        "        df_tic: TIC data.\n",
        "        study_config: Config.\n",
        "\n",
        "    Returns:\n",
        "        Dict containing baseline results, augmented results, and improvement table.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting TIC Robustness Analysis\")\n",
        "\n",
        "    seeds = study_config[\"Model_Architectures\"][\"Random_Forest\"][\"rf_seeds_benchmark_reported_in_paper\"]\n",
        "    horizons = study_config[\"Global_Settings\"][\"forecast_horizons\"]\n",
        "\n",
        "    # 1. Prepare Data\n",
        "    df_feat_base, df_feat_aug, eval_start = prepare_tic_analysis_data(\n",
        "        df_benchmark_yields, df_macro, df_tic, study_config\n",
        "    )\n",
        "\n",
        "    # 2. Run Baseline (Restricted Sample)\n",
        "    logger.info(\"Running Baseline (Restricted Sample)...\")\n",
        "    base_results = []\n",
        "    for h in horizons:\n",
        "        res = execute_benchmark_rf(\n",
        "            df_feat_base, df_benchmark_yields, study_config, seeds, h, eval_start\n",
        "        )\n",
        "        base_results.extend(res)\n",
        "\n",
        "    # 3. Run Augmented\n",
        "    logger.info(\"Running TIC-Augmented...\")\n",
        "    aug_results = []\n",
        "    for h in horizons:\n",
        "        res = execute_benchmark_rf(\n",
        "            df_feat_aug, df_benchmark_yields, study_config, seeds, h, eval_start\n",
        "        )\n",
        "        aug_results.extend(res)\n",
        "\n",
        "    # 4. Compute RMSFE\n",
        "    # Helper from Task 31 logic (re-implemented for self-containment or assumed shared)\n",
        "    # We'll implement a local helper\n",
        "    def get_rmsfe(results_list):\n",
        "        df = pd.DataFrame(results_list)\n",
        "        # Reconstruct TargetDate\n",
        "        df[\"TargetDate\"] = df.apply(\n",
        "            lambda row: row[\"OriginDate\"] + DateOffset(months=row[\"Horizon\"]) + MonthEnd(0), axis=1\n",
        "        )\n",
        "        # Merge actuals\n",
        "        actuals = df_benchmark_yields.reset_index().melt(\n",
        "            id_vars=[df_benchmark_yields.index.name or \"index\"], var_name=\"Maturity\", value_name=\"Actual\"\n",
        "        ).rename(columns={df_benchmark_yields.index.name or \"index\": \"TargetDate\"})\n",
        "        actuals[\"TargetDate\"] = pd.to_datetime(actuals[\"TargetDate\"])\n",
        "\n",
        "        merged = pd.merge(df, actuals, on=[\"TargetDate\", \"Maturity\"], how=\"inner\")\n",
        "        merged[\"Error\"] = merged[\"Actual\"] - merged[\"Forecast\"]\n",
        "\n",
        "        rmsfe = merged.groupby([\"Seed\", \"Horizon\", \"Maturity\"])[\"Error\"].apply(\n",
        "            lambda x: np.sqrt(np.mean(x**2)) * 100.0\n",
        "        ).reset_index(name=\"RMSFE\")\n",
        "        return rmsfe\n",
        "\n",
        "    rmsfe_base = get_rmsfe(base_results)\n",
        "    rmsfe_aug = get_rmsfe(aug_results)\n",
        "\n",
        "    # 5. Compare\n",
        "    delta_df = compute_tic_improvement(rmsfe_base, rmsfe_aug)\n",
        "\n",
        "    # Pivot for Heatmap (Average across seeds)\n",
        "    # Rows: Horizon, Cols: Maturity\n",
        "    heatmap_data = delta_df.groupby([\"Horizon\", \"Maturity\"])[\"Delta_RMSFE\"].mean().reset_index()\n",
        "    heatmap_matrix = heatmap_data.pivot(index=\"Horizon\", columns=\"Maturity\", values=\"Delta_RMSFE\")\n",
        "\n",
        "    # Reorder\n",
        "    mats = study_config[\"Raw_Data_Schemas\"][\"US_Benchmark_Yields\"][\"columns\"]\n",
        "    available_mats = [m for m in mats if m in heatmap_matrix.columns]\n",
        "    heatmap_matrix = heatmap_matrix[available_mats]\n",
        "\n",
        "    logger.info(\"TIC Analysis Completed.\")\n",
        "\n",
        "    return {\n",
        "        \"baseline_rmsfe\": rmsfe_base,\n",
        "        \"augmented_rmsfe\": rmsfe_aug,\n",
        "        \"delta_table\": delta_df,\n",
        "        \"heatmap_matrix\": heatmap_matrix\n",
        "    }\n"
      ],
      "metadata": {
        "id": "oS1O1LPFxuOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 33: Execute the TIC augmentation robustness analysis\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 33: Execute TIC Augmentation Analysis\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 33, Step 1 & 2: Execute Baseline and Augmented Runs\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Note: We rely on `execute_benchmark_rf` from Task 30 and `prepare_tic_analysis_data` from Task 32.\n",
        "# We assume they are available in the environment.\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 33, Step 3: Compute Improvement Heatmap\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def calculate_tic_rmsfe_and_delta(\n",
        "    base_results: List[Dict[str, Any]],\n",
        "    aug_results: List[Dict[str, Any]],\n",
        "    df_actuals: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Calculates RMSFE for baseline and augmented models, and the delta.\n",
        "\n",
        "    Args:\n",
        "        base_results: List of baseline forecast records.\n",
        "        aug_results: List of augmented forecast records.\n",
        "        df_actuals: Realized benchmark yields.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (RMSFE_Base, RMSFE_Aug, Delta_Matrix).\n",
        "    \"\"\"\n",
        "    # Helper to calc RMSFE\n",
        "    def get_rmsfe(results_list):\n",
        "        df = pd.DataFrame(results_list)\n",
        "\n",
        "        # Reconstruct TargetDate\n",
        "        df[\"TargetDate\"] = df.apply(\n",
        "            lambda row: row[\"OriginDate\"] + DateOffset(months=row[\"Horizon\"]) + MonthEnd(0), axis=1\n",
        "        )\n",
        "        # Merge actuals\n",
        "        actuals_long = df_actuals.reset_index().melt(\n",
        "            id_vars=[df_actuals.index.name or \"index\"], var_name=\"Maturity\", value_name=\"Actual\"\n",
        "        ).rename(columns={df_actuals.index.name or \"index\": \"TargetDate\"})\n",
        "        actuals_long[\"TargetDate\"] = pd.to_datetime(actuals_long[\"TargetDate\"])\n",
        "\n",
        "        merged = pd.merge(df, actuals_long, on=[\"TargetDate\", \"Maturity\"], how=\"inner\")\n",
        "        merged[\"Error\"] = merged[\"Actual\"] - merged[\"Forecast\"]\n",
        "\n",
        "        # RMSFE per Horizon/Maturity (averaged over seeds)\n",
        "        rmsfe = merged.groupby([\"Horizon\", \"Maturity\"])[\"Error\"].apply(\n",
        "            lambda x: np.sqrt(np.mean(x**2)) * 100.0 # bps\n",
        "        ).reset_index(name=\"RMSFE\")\n",
        "        return rmsfe\n",
        "\n",
        "    rmsfe_base = get_rmsfe(base_results)\n",
        "    rmsfe_aug = get_rmsfe(aug_results)\n",
        "\n",
        "    # Merge\n",
        "    merged = pd.merge(rmsfe_aug, rmsfe_base, on=[\"Horizon\", \"Maturity\"], suffixes=(\"_TIC\", \"_Base\"))\n",
        "    merged[\"Delta\"] = merged[\"RMSFE_TIC\"] - merged[\"RMSFE_Base\"]\n",
        "\n",
        "    # Pivot for Heatmap\n",
        "    # Index: Horizon, Columns: Maturity\n",
        "    heatmap = merged.pivot(index=\"Horizon\", columns=\"Maturity\", values=\"Delta\")\n",
        "\n",
        "    return rmsfe_base, rmsfe_aug, heatmap\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 33, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def execute_tic_analysis(\n",
        "    df_benchmark_yields: pd.DataFrame,\n",
        "    df_macro: pd.DataFrame,\n",
        "    df_tic: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute the TIC robustness analysis.\n",
        "\n",
        "    Args:\n",
        "        df_benchmark_yields: Benchmark yields.\n",
        "        df_macro: Macro data.\n",
        "        df_tic: TIC data.\n",
        "        study_config: Config.\n",
        "\n",
        "    Returns:\n",
        "        Dict containing results.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 33: Executing TIC Analysis\")\n",
        "\n",
        "    # 1. Prepare Data (Task 32 logic)\n",
        "    df_feat_base, df_feat_aug, eval_start = prepare_tic_analysis_data(\n",
        "        df_benchmark_yields, df_macro, df_tic, study_config\n",
        "    )\n",
        "\n",
        "    seeds = study_config[\"Model_Architectures\"][\"Random_Forest\"][\"rf_seeds_benchmark_reported_in_paper\"]\n",
        "    horizons = study_config[\"Global_Settings\"][\"forecast_horizons\"]\n",
        "\n",
        "    # 2. Run Baseline\n",
        "    logger.info(\"Running Baseline...\")\n",
        "    base_results = []\n",
        "    for h in horizons:\n",
        "        res = execute_benchmark_rf(\n",
        "            df_feat_base, df_benchmark_yields, study_config, seeds, h, eval_start\n",
        "        )\n",
        "        base_results.extend(res)\n",
        "\n",
        "    # 3. Run Augmented\n",
        "    logger.info(\"Running Augmented...\")\n",
        "    aug_results = []\n",
        "    for h in horizons:\n",
        "        res = execute_benchmark_rf(\n",
        "            df_feat_aug, df_benchmark_yields, study_config, seeds, h, eval_start\n",
        "        )\n",
        "        aug_results.extend(res)\n",
        "\n",
        "    # 4. Compute Delta\n",
        "    rmsfe_base, rmsfe_aug, heatmap = calculate_tic_rmsfe_and_delta(\n",
        "        base_results, aug_results, df_benchmark_yields\n",
        "    )\n",
        "\n",
        "    # Reorder heatmap columns\n",
        "    mats = study_config[\"Raw_Data_Schemas\"][\"US_Benchmark_Yields\"][\"columns\"]\n",
        "    available = [m for m in mats if m in heatmap.columns]\n",
        "    heatmap = heatmap[available]\n",
        "\n",
        "    logger.info(\"Task 33 Completed Successfully.\")\n",
        "\n",
        "    return {\n",
        "        \"baseline_rmsfe\": rmsfe_base,\n",
        "        \"augmented_rmsfe\": rmsfe_aug,\n",
        "        \"heatmap_matrix\": heatmap\n",
        "    }\n"
      ],
      "metadata": {
        "id": "k2Vr6rxl0BhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 34: Create the global extension orchestrator function specification (for robustness analysis)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 34: Global Extension Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 34, Step 1 & 2: Per-Country Workflow\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def execute_country_rf_analysis(\n",
        "    country_name: str,\n",
        "    df_yield_target: pd.DataFrame, # Single column (10Y)\n",
        "    df_macro_panel: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Executes the RF forecasting pipeline for a single country.\n",
        "\n",
        "    Args:\n",
        "        country_name: Name of the country.\n",
        "        df_yield_target: DataFrame with single column (10Y yield).\n",
        "        df_macro_panel: Macro predictors for this country.\n",
        "        study_config: Config.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Forecasts for this country.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Processing Country: {country_name}\")\n",
        "\n",
        "    # 1. Construct Features\n",
        "    # We need to ensure df_yield_target has a name\n",
        "    if isinstance(df_yield_target, pd.Series):\n",
        "        df_yield_target = df_yield_target.to_frame()\n",
        "\n",
        "    df_macro_feat, dict_yield_feat = construct_rf_features(\n",
        "        df_macro_panel, df_yield_target, study_config\n",
        "    )\n",
        "\n",
        "    # 2. Generate Forecasts\n",
        "    # This handles normalization, CV, training, prediction loop\n",
        "    df_forecasts = generate_rf_forecasts(\n",
        "        df_macro_feat, dict_yield_feat, df_yield_target, study_config\n",
        "    )\n",
        "\n",
        "    # Add Country column\n",
        "    df_forecasts[\"Country\"] = country_name\n",
        "\n",
        "    return df_forecasts\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 34, Step 3: Output Formatting\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def format_global_rmsfe_table(\n",
        "    all_forecasts: pd.DataFrame,\n",
        "    df_global_yields: pd.DataFrame,\n",
        "    multiplier: float = 100.0\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes and formats the global RMSFE table.\n",
        "\n",
        "    Args:\n",
        "        all_forecasts: Combined forecasts for all countries.\n",
        "        df_global_yields: Realized yields (Cols: Country_10Y).\n",
        "        multiplier: Unit conversion.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Table with Mean [Min, Max] RMSFE per country/horizon.\n",
        "    \"\"\"\n",
        "    # Prepare actuals\n",
        "    # Melt\n",
        "    actuals_long = df_global_yields.reset_index().melt(\n",
        "        id_vars=[df_global_yields.index.name or \"index\"], var_name=\"Country_Col\", value_name=\"Actual\"\n",
        "    ).rename(columns={df_global_yields.index.name or \"index\": \"TargetDate\"})\n",
        "    actuals_long[\"TargetDate\"] = pd.to_datetime(actuals_long[\"TargetDate\"])\n",
        "\n",
        "    # Merge\n",
        "    # We merge on TargetDate and Country (assuming we fixed names)\n",
        "    merged = pd.merge(\n",
        "        all_forecasts,\n",
        "        actuals_long,\n",
        "        left_on=[\"TargetDate\", \"Country\"],\n",
        "        right_on=[\"TargetDate\", \"Country_Col\"],\n",
        "        how=\"inner\"\n",
        "    )\n",
        "\n",
        "    merged[\"Error\"] = merged[\"Actual\"] - merged[\"Forecast\"]\n",
        "\n",
        "    # RMSFE per Seed\n",
        "    rmsfe_seed = merged.groupby([\"Country\", \"Horizon\", \"Seed\"])[\"Error\"].apply(\n",
        "        lambda x: np.sqrt(np.mean(x**2)) * multiplier\n",
        "    ).reset_index(name=\"RMSFE\")\n",
        "\n",
        "    # Aggregate across seeds\n",
        "    stats = rmsfe_seed.groupby([\"Country\", \"Horizon\"])[\"RMSFE\"].agg([\"mean\", \"min\", \"max\"])\n",
        "\n",
        "    # Format\n",
        "    stats[\"formatted\"] = stats.apply(\n",
        "        lambda row: f\"{row['mean']:.2f} [{row['min']:.2f}, {row['max']:.2f}]\",\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Pivot\n",
        "    table = stats.reset_index().pivot(index=\"Country\", columns=\"Horizon\", values=\"formatted\")\n",
        "    table.columns = [f\"H{h}\" for h in table.columns]\n",
        "\n",
        "    return table\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 34, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_global_extension_analysis(\n",
        "    df_global_yields: pd.DataFrame,\n",
        "    global_macro_panels: Dict[str, pd.DataFrame],\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrator for Global Sovereign Bond RF Analysis.\n",
        "\n",
        "    Args:\n",
        "        df_global_yields: Global 10Y yields.\n",
        "        global_macro_panels: Dict of macro panels.\n",
        "        study_config: Config.\n",
        "\n",
        "    Returns:\n",
        "        Dict containing forecasts and RMSFE table.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Global Extension Analysis\")\n",
        "\n",
        "    multiplier = study_config[\"Global_Settings\"][\"pct_points_to_bps_multiplier\"]\n",
        "\n",
        "    forecast_list = []\n",
        "\n",
        "    # Iterate countries\n",
        "    for country_code, df_macro in global_macro_panels.items():\n",
        "        # Find target column\n",
        "        target_col = f\"{country_code}_10Y\"\n",
        "        if target_col not in df_global_yields.columns:\n",
        "            logger.warning(f\"Target yield {target_col} not found for macro panel {country_code}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        df_target = df_global_yields[[target_col]]\n",
        "\n",
        "        # Execute\n",
        "        # We pass the column name as the country name to align with actuals later\n",
        "        df_fc = execute_country_rf_analysis(target_col, df_target, df_macro, study_config)\n",
        "        forecast_list.append(df_fc)\n",
        "\n",
        "    all_forecasts = pd.concat(forecast_list, ignore_index=True)\n",
        "\n",
        "    # Format Table\n",
        "    rmsfe_table = format_global_rmsfe_table(all_forecasts, df_global_yields, multiplier)\n",
        "\n",
        "    logger.info(\"Global Analysis Completed.\")\n",
        "\n",
        "    return {\n",
        "        \"forecasts\": all_forecasts,\n",
        "        \"rmsfe_table\": rmsfe_table\n",
        "    }\n"
      ],
      "metadata": {
        "id": "RF5Ud8vc5XuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 35: Execute the global sovereign 10Y RF extension\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 35: Execute Global Sovereign 10Y RF Extension\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 35, Step 1 & 2: Execute Per-Country Workflow\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Note: We rely on `execute_country_rf_analysis` from Task 34.\n",
        "# Task 35 is about executing it for all countries and formatting the specific table required.\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 35, Step 3: Compute RMSFE and Produce Table\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Note: We rely on `format_global_rmsfe_table` from Task 34.\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 35, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def execute_global_rf_extension(\n",
        "    df_global_yields: pd.DataFrame,\n",
        "    global_macro_panels: Dict[str, pd.DataFrame],\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrator to execute the Global Sovereign 10Y RF Extension.\n",
        "\n",
        "    Args:\n",
        "        df_global_yields: Global 10Y yields.\n",
        "        global_macro_panels: Dict of macro panels.\n",
        "        study_config: Config.\n",
        "\n",
        "    Returns:\n",
        "        Dict containing forecasts and RMSFE table.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Global RF Extension Execution\")\n",
        "\n",
        "    # Reuse Task 34 logic\n",
        "    results = run_global_extension_analysis(\n",
        "        df_global_yields, global_macro_panels, study_config\n",
        "    )\n",
        "\n",
        "    logger.info(\"Task 35 Completed Successfully.\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "WLU6iPpE841n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 36: Compute SHAP values for RF models using TreeExplainer\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 36: Compute SHAP values\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 36, Step 1 & 2: Refit and Explain\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_shap_for_last_window(\n",
        "    df_macro_features: pd.DataFrame,\n",
        "    dict_yield_features: Dict[str, pd.DataFrame],\n",
        "    df_yields: pd.DataFrame,\n",
        "    study_config: Dict[str, Any],\n",
        "    seeds: List[int]\n",
        ") -> Dict[Tuple[str, int, int], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Computes SHAP values for the Random Forest models trained on the LAST rolling window.\n",
        "\n",
        "    This function rigorously reconstructs the training environment for the final forecast origin,\n",
        "    trains the optimal Random Forest model using Randomized Cross-Validation (matching the\n",
        "    forecasting methodology), and computes SHAP values to interpret global feature importance.\n",
        "\n",
        "    Methodology:\n",
        "        1. Identify the last valid forecast origin t.\n",
        "        2. Construct the training set D_{t,h} = {(X_s, y_{s+h})} for s in [t-w+1, t].\n",
        "        3. Normalize features and targets to [0,1].\n",
        "        4. For each seed, perform RandomizedSearchCV to find theta*.\n",
        "        5. Fit the optimal model f_theta*.\n",
        "        6. Compute SHAP values phi_j(X_s) for all s in the training set using TreeExplainer.\n",
        "           The training set itself serves as the background distribution.\n",
        "\n",
        "    Args:\n",
        "        df_macro_features (pd.DataFrame): DataFrame of macro feature lags.\n",
        "        dict_yield_features (Dict[str, pd.DataFrame]): Dictionary mapping maturity to yield feature lags.\n",
        "        df_yields (pd.DataFrame): DataFrame of realized yields (targets).\n",
        "        study_config (Dict[str, Any]): Frozen configuration dictionary containing model and data parameters.\n",
        "        seeds (List[int]): List of random seeds to evaluate.\n",
        "\n",
        "    Returns:\n",
        "        Dict: A dictionary mapping (maturity, horizon, seed) -> result_dict.\n",
        "              result_dict contains:\n",
        "              - \"shap_values\": np.ndarray of shape (n_samples, n_features)\n",
        "              - \"feature_names\": List[str] of feature names\n",
        "              - \"base_value\": float (expected value of the model output)\n",
        "    \"\"\"\n",
        "    # Extract configuration parameters\n",
        "    window_size = study_config[\"Model_Architectures\"][\"Random_Forest\"][\"rolling_window_W\"]\n",
        "    horizons = study_config[\"Global_Settings\"][\"forecast_horizons\"]\n",
        "    maturities = study_config[\"Global_Settings\"][\"us_zero_maturities\"]\n",
        "\n",
        "    # Identify the last feasible forecast origin\n",
        "    # The feature index defines the available timeline.\n",
        "    last_origin_idx = len(df_macro_features) - 1\n",
        "    last_origin_date = df_macro_features.index[last_origin_idx]\n",
        "\n",
        "    logger.info(f\"Computing SHAP analysis for models trained at origin: {last_origin_date}\")\n",
        "\n",
        "    shap_results = {}\n",
        "\n",
        "    # Define Training Window Indices (inclusive)\n",
        "    # We use integer indexing for speed and precision\n",
        "    train_start_idx = last_origin_idx - window_size + 1\n",
        "    train_end_idx = last_origin_idx\n",
        "\n",
        "    # Ensure sufficient history exists\n",
        "    if train_start_idx < 0:\n",
        "        raise ValueError(f\"Insufficient history for SHAP analysis at {last_origin_date}. Need {window_size} lags.\")\n",
        "\n",
        "    # Iterate over each maturity to explain\n",
        "    for maturity in maturities:\n",
        "        # 1. Construct Feature Matrix X (Raw)\n",
        "        # Slice macro features for the window\n",
        "        X_macro_train = df_macro_features.iloc[train_start_idx : train_end_idx + 1].values\n",
        "\n",
        "        # Slice yield features for the specific maturity\n",
        "        df_yf = dict_yield_features[maturity]\n",
        "        X_yield_train = df_yf.iloc[train_start_idx : train_end_idx + 1].values\n",
        "\n",
        "        # Concatenate to form full X matrix\n",
        "        X_train_raw = np.hstack([X_macro_train, X_yield_train])\n",
        "\n",
        "        # Construct feature names list for interpretability\n",
        "        feature_names = list(df_macro_features.columns) + list(df_yf.columns)\n",
        "\n",
        "        # Iterate over forecast horizons\n",
        "        for h in horizons:\n",
        "            # 2. Construct Target Vector y (Raw)\n",
        "            # The target for feature vector at time s is Yield_{s+h}\n",
        "            # Get the dates corresponding to the training features\n",
        "            train_dates = df_macro_features.index[train_start_idx : train_end_idx + 1]\n",
        "\n",
        "            # Calculate target dates: s + h months\n",
        "            target_dates = train_dates + DateOffset(months=h) + MonthEnd(0)\n",
        "\n",
        "            # Retrieve realized yields for these target dates\n",
        "            y_train_raw = df_yields[maturity].reindex(target_dates).values\n",
        "\n",
        "            # 3. Handle Missing Data\n",
        "            # Drop rows where the target is missing (e.g., if h extends beyond available data)\n",
        "            valid_mask = np.isfinite(y_train_raw)\n",
        "\n",
        "            # Enforce minimum data requirement\n",
        "            if np.sum(valid_mask) < window_size * 0.9:\n",
        "                logger.warning(f\"Skipping SHAP for {maturity} H{h}: Insufficient valid targets ({np.sum(valid_mask)}/{window_size}).\")\n",
        "                continue\n",
        "\n",
        "            X_train_clean = X_train_raw[valid_mask]\n",
        "            y_train_clean = y_train_raw[valid_mask]\n",
        "\n",
        "            # 4. Normalize Data (Min-Max)\n",
        "            # We must replicate the exact normalization used in the forecasting pipeline\n",
        "\n",
        "            # Normalize X\n",
        "            x_min = X_train_clean.min(axis=0)\n",
        "            x_max = X_train_clean.max(axis=0)\n",
        "            x_range = x_max - x_min\n",
        "            # Handle constant features to avoid division by zero\n",
        "            x_range[x_range == 0] = 1.0\n",
        "            X_train_norm = (X_train_clean - x_min) / x_range\n",
        "\n",
        "            # Normalize y\n",
        "            y_min = y_train_clean.min()\n",
        "            y_max = y_train_clean.max()\n",
        "            y_range = y_max - y_min if y_max != y_min else 1.0\n",
        "            y_train_norm = (y_train_clean - y_min) / y_range\n",
        "\n",
        "            # Iterate over random seeds\n",
        "            for seed in seeds:\n",
        "                # 5. Rigorous Model Training\n",
        "                logger.debug(f\"Training optimal RF for SHAP: {maturity} H{h} Seed {seed}\")\n",
        "\n",
        "                best_model = train_rf_with_randomized_cv(\n",
        "                    X_train_norm,\n",
        "                    y_train_norm,\n",
        "                    study_config,\n",
        "                    seed\n",
        "                )\n",
        "\n",
        "                # 6. Compute SHAP Values\n",
        "                # We use TreeExplainer to explain the model's predictions on the training set.\n",
        "                # This provides the \"global\" feature importance for this specific model instance.\n",
        "\n",
        "                # We explicitly pass the background dataset (X_train_norm) to ensure\n",
        "                # the expected value is calculated over the training distribution.\n",
        "                # check_additivity=False is often needed for RF due to precision issues,\n",
        "                # but we aim for exactness where possible.\n",
        "                try:\n",
        "                    explainer = shap.TreeExplainer(best_model, data=X_train_norm)\n",
        "                    shap_values = explainer.shap_values(X_train_norm, check_additivity=False)\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"SHAP computation failed for {maturity} H{h} Seed {seed}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                # 7. Store Results\n",
        "                key = (maturity, h, seed)\n",
        "                shap_results[key] = {\n",
        "                    \"shap_values\": shap_values,      # Shape: (n_samples, n_features)\n",
        "                    \"feature_names\": feature_names,  # List of feature names\n",
        "                    \"base_value\": explainer.expected_value # Scalar expected value\n",
        "                }\n",
        "\n",
        "    return shap_results\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 36, Step 3: Validate SHAP\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_shap_additivity(shap_results: Dict[Tuple[str, int, int], Dict[str, Any]]) -> None:\n",
        "    \"\"\"\n",
        "    Validates the additive property of SHAP values for a sample.\n",
        "\n",
        "    Args:\n",
        "        shap_results: Dictionary of SHAP results.\n",
        "    \"\"\"\n",
        "    # We just check one entry\n",
        "    if not shap_results:\n",
        "        return\n",
        "\n",
        "    first_key = list(shap_results.keys())[0]\n",
        "    data = shap_results[first_key]\n",
        "    vals = data[\"shap_values\"]\n",
        "    base = data[\"base_value\"]\n",
        "\n",
        "    # Sum of SHAP + Base should equal prediction\n",
        "    # We don't have the prediction stored here easily without re-predicting.\n",
        "    # But TreeExplainer guarantees this property (within tolerance).\n",
        "    # We'll log the shape.\n",
        "\n",
        "    logger.info(f\"SHAP validation: Matrix shape {vals.shape}. Features: {len(data['feature_names'])}.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 36, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_shap_analysis(\n",
        "    df_macro_features: pd.DataFrame,\n",
        "    dict_yield_features: Dict[str, pd.DataFrame],\n",
        "    df_yields: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[Tuple[str, int, int], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrator to compute SHAP values.\n",
        "\n",
        "    Args:\n",
        "        df_macro_features: Macro features.\n",
        "        dict_yield_features: Yield features.\n",
        "        df_yields: Realized yields.\n",
        "        study_config: Config.\n",
        "\n",
        "    Returns:\n",
        "        Dict containing SHAP results.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting SHAP Analysis\")\n",
        "\n",
        "    seeds = study_config[\"Model_Architectures\"][\"Random_Forest\"][\"rf_seeds_main_required_for_exact_replication\"]\n",
        "    # Limit seeds for SHAP to 1 or 2 if computational cost is high, but manuscript implies robustness across seeds.\n",
        "    # We'll use the first 2 seeds for demonstration/speed if list is long.\n",
        "    seeds_subset = seeds[:2]\n",
        "\n",
        "    shap_data = compute_shap_for_last_window(\n",
        "        df_macro_features, dict_yield_features, df_yields, study_config, seeds_subset\n",
        "    )\n",
        "\n",
        "    validate_shap_additivity(shap_data)\n",
        "\n",
        "    logger.info(\"SHAP Analysis Completed.\")\n",
        "    return shap_data\n"
      ],
      "metadata": {
        "id": "PA7kKgUI_ty-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 37: Aggregate SHAP values to GlobalSHAP and produce the interpretability figure\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 37: Aggregate SHAP values\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 37, Step 1 & 2: Aggregate and Rank\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def aggregate_shap_to_global(\n",
        "    shap_results: Dict[Tuple[str, int, int], Dict[str, Any]]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates SHAP values across maturities to compute GlobalSHAP.\n",
        "\n",
        "    Equation:\n",
        "        GlobalSHAP_j(h, s) = Mean_{tau} ( Mean_{samples} ( |phi_j(tau, h, s)| ) )\n",
        "\n",
        "    Feature Mapping:\n",
        "        - Macro features are common: \"Macro_CPI_L1\" -> \"Macro_CPI_L1\"\n",
        "        - Yield features are specific: \"Yield_3M_L1\" -> \"Yield_Self_L1\"\n",
        "        This allows averaging importance of \"Lag 1 Yield\" across maturities.\n",
        "\n",
        "    Args:\n",
        "        shap_results: Dictionary from Task 36.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with columns ['Horizon', 'Seed', 'Feature', 'GlobalSHAP'].\n",
        "    \"\"\"\n",
        "    aggregated_data = []\n",
        "\n",
        "    # Group keys by (Horizon, Seed)\n",
        "    # We iterate the dictionary\n",
        "\n",
        "    # Intermediate storage: (Horizon, Seed) -> Dict[GenericFeature -> List[Importance]]\n",
        "    grouped_importance = {}\n",
        "\n",
        "    for key, result in shap_results.items():\n",
        "        maturity, h, seed = key\n",
        "\n",
        "        # 1. Compute Mean Absolute SHAP per feature for this model\n",
        "        # shap_values: (N, F)\n",
        "        vals = result[\"shap_values\"]\n",
        "        names = result[\"feature_names\"]\n",
        "\n",
        "        # Mean abs\n",
        "        mean_abs = np.mean(np.abs(vals), axis=0) # (F,)\n",
        "\n",
        "        # 2. Map to Generic Names\n",
        "        for feat_name, importance in zip(names, mean_abs):\n",
        "            if \"Yield_\" in feat_name:\n",
        "                # Example: Yield_3M_L1 -> Yield_Self_L1\n",
        "                # We assume format \"Yield_{mat}_L{lag}\"\n",
        "                parts = feat_name.split(\"_\")\n",
        "\n",
        "                # Reconstruct: Yield_Self_L{lag}\n",
        "                # parts[-1] is L{lag}\n",
        "                generic_name = f\"Yield_Self_{parts[-1]}\"\n",
        "            else:\n",
        "                generic_name = feat_name\n",
        "\n",
        "            # Store\n",
        "            group_key = (h, seed)\n",
        "            if group_key not in grouped_importance:\n",
        "                grouped_importance[group_key] = {}\n",
        "\n",
        "            if generic_name not in grouped_importance[group_key]:\n",
        "                grouped_importance[group_key][generic_name] = []\n",
        "\n",
        "            grouped_importance[group_key][generic_name].append(importance)\n",
        "\n",
        "    # 3. Average across maturities\n",
        "    for (h, seed), feat_dict in grouped_importance.items():\n",
        "        for feat, imp_list in feat_dict.items():\n",
        "            # Average over the list (which contains one entry per maturity)\n",
        "            global_shap = np.mean(imp_list)\n",
        "\n",
        "            aggregated_data.append({\n",
        "                \"Horizon\": h,\n",
        "                \"Seed\": seed,\n",
        "                \"Feature\": feat,\n",
        "                \"GlobalSHAP\": global_shap\n",
        "            })\n",
        "\n",
        "    df_global = pd.DataFrame(aggregated_data)\n",
        "    return df_global\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 37, Step 3: Produce Plot Data\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_shap_plot_data(\n",
        "    df_global_shap: pd.DataFrame,\n",
        "    top_k: int = 20\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Prepares data for the Global SHAP summary plot.\n",
        "\n",
        "    Aggregates across seeds (mean) and selects top K features per horizon.\n",
        "\n",
        "    Args:\n",
        "        df_global_shap: DataFrame from Step 1.\n",
        "        top_k: Number of top features to retain.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Top K features per horizon with mean GlobalSHAP.\n",
        "    \"\"\"\n",
        "    # Average across seeds\n",
        "    df_mean = df_global_shap.groupby([\"Horizon\", \"Feature\"])[\"GlobalSHAP\"].mean().reset_index()\n",
        "\n",
        "    # Rank per horizon\n",
        "    df_mean[\"Rank\"] = df_mean.groupby(\"Horizon\")[\"GlobalSHAP\"].rank(ascending=False, method=\"first\")\n",
        "\n",
        "    # Filter Top K\n",
        "    df_plot = df_mean[df_mean[\"Rank\"] <= top_k].copy()\n",
        "\n",
        "    # Sort\n",
        "    df_plot = df_plot.sort_values([\"Horizon\", \"Rank\"])\n",
        "\n",
        "    return df_plot\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 37, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def aggregate_shap_results(\n",
        "    shap_results: Dict[Tuple[str, int, int], Dict[str, Any]]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrator to aggregate SHAP results.\n",
        "\n",
        "    Args:\n",
        "        shap_results: Raw SHAP results.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (Full GlobalSHAP DataFrame, Top-K Plot Data).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting SHAP Aggregation\")\n",
        "\n",
        "    # 1. Aggregate\n",
        "    df_global = aggregate_shap_to_global(shap_results)\n",
        "\n",
        "    # 2. Prepare Plot Data\n",
        "    df_plot = prepare_shap_plot_data(df_global, top_k=20)\n",
        "\n",
        "    logger.info(\"SHAP Aggregation Completed.\")\n",
        "    return df_global, df_plot\n"
      ],
      "metadata": {
        "id": "ZyeF5ufWDpi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 38: Produce the weight dynamics figures for DRO combination schemes\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 38: Produce weight dynamics data\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 38, Step 1 & 2: Extract and Aggregate\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def aggregate_weights_by_group(\n",
        "    df_weights: pd.DataFrame,\n",
        "    target_methods: List[str] = [\"FC-DRMV\", \"FC-DRO-ES\", \"FC-DRO-MIX\"]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Aggregates weights into RF and FADNS groups for specified methods.\n",
        "\n",
        "    Args:\n",
        "        df_weights: Wide weights DataFrame (Cols: Method, Model).\n",
        "        target_methods: List of methods to process.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: Mapping Method -> DataFrame(Index=Time, Cols=['RF', 'FADNS']).\n",
        "    \"\"\"\n",
        "    aggregated_data = {}\n",
        "\n",
        "    for method in target_methods:\n",
        "        if method not in df_weights.columns.get_level_values(0):\n",
        "            logger.warning(f\"Method {method} not found in weights.\")\n",
        "            continue\n",
        "\n",
        "        # Extract weights for method\n",
        "        w_method = df_weights[method]\n",
        "\n",
        "        # Identify groups\n",
        "        rf_cols = [c for c in w_method.columns if c.startswith(\"RF\")]\n",
        "        fadns_cols = [c for c in w_method.columns if c.startswith(\"FADNS\")]\n",
        "\n",
        "        # Sum\n",
        "        w_rf = w_method[rf_cols].sum(axis=1)\n",
        "        w_fadns = w_method[fadns_cols].sum(axis=1)\n",
        "\n",
        "        # Combine\n",
        "        df_agg = pd.DataFrame({\n",
        "            \"RF\": w_rf,\n",
        "            \"FADNS\": w_fadns\n",
        "        })\n",
        "\n",
        "        aggregated_data[method] = df_agg\n",
        "\n",
        "    return aggregated_data\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 38, Step 3: Prepare Plot Data\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# The aggregated data IS the plot data.\n",
        "# We just ensure it's ready for the artifact bundle.\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 38, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_weight_dynamics_data(\n",
        "    df_weights: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrator to prepare weight dynamics data.\n",
        "\n",
        "    Args:\n",
        "        df_weights: Weights DataFrame.\n",
        "        study_config: Config.\n",
        "\n",
        "    Returns:\n",
        "        Dict mapping Method -> Aggregated Weights DataFrame.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Weight Dynamics Data Preparation\")\n",
        "\n",
        "    # We focus on DRO methods as per task\n",
        "    target_methods = [\"FC-DRMV\", \"FC-DRO-ES\", \"FC-DRO-MIX\"]\n",
        "\n",
        "    plot_data = aggregate_weights_by_group(df_weights, target_methods)\n",
        "\n",
        "    logger.info(\"Weight Dynamics Data Prepared.\")\n",
        "    return plot_data\n"
      ],
      "metadata": {
        "id": "G33nzAAQWKml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 39: Produce the hybrid error dynamics figure (one-month horizon)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 39: Produce hybrid error dynamics data\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 39, Step 1 & 2: Compute Errors and Assign Families\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_and_categorize_errors(\n",
        "    df_combined: pd.DataFrame,\n",
        "    df_actuals: pd.DataFrame,\n",
        "    horizon: int = 1\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes forecast errors for a specific horizon and categorizes them by method family.\n",
        "\n",
        "    Args:\n",
        "        df_combined: Combined forecasts (Long format).\n",
        "        df_actuals: Realized yields.\n",
        "        horizon: Forecast horizon to filter (default 1).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with columns [TargetDate, Maturity, Method, Family, Error].\n",
        "    \"\"\"\n",
        "    # Filter Horizon\n",
        "    df_subset = df_combined[df_combined[\"Horizon\"] == horizon].copy()\n",
        "\n",
        "    # Prepare Actuals\n",
        "    actuals_long = df_actuals.reset_index().melt(\n",
        "        id_vars=[df_actuals.index.name or \"index\"], var_name=\"Maturity\", value_name=\"Actual\"\n",
        "    ).rename(columns={df_actuals.index.name or \"index\": \"TargetDate\"})\n",
        "    actuals_long[\"TargetDate\"] = pd.to_datetime(actuals_long[\"TargetDate\"])\n",
        "\n",
        "    # Merge\n",
        "    merged = pd.merge(\n",
        "        df_subset,\n",
        "        actuals_long,\n",
        "        on=[\"TargetDate\", \"Maturity\"],\n",
        "        how=\"inner\"\n",
        "    )\n",
        "\n",
        "    # Compute Error\n",
        "    merged[\"Error\"] = merged[\"Actual\"] - merged[\"Forecast\"]\n",
        "\n",
        "    # Define Families\n",
        "    family_map = {\n",
        "        \"FC-EW\": \"Classic\", \"FC-RANK\": \"Classic\", \"FC-RMSE\": \"Classic\",\n",
        "        \"FC-MSE\": \"Classic\", \"FC-OLS\": \"Classic\",\n",
        "        \"FC-MV\": \"Var/Risk\", \"FC-STACK\": \"Var/Risk\", \"FC-LAD\": \"Var/Risk\",\n",
        "        \"AFTER-Rolling\": \"AFTER\", \"AFTER-EWMA\": \"AFTER\", \"AFTER-Simplified\": \"AFTER\",\n",
        "        \"FC-DRO-ES\": \"DRO\", \"FC-DRO-MIX\": \"DRO\", \"FC-DRMV\": \"DRO\"\n",
        "    }\n",
        "\n",
        "    merged[\"Family\"] = merged[\"Method\"].map(family_map)\n",
        "\n",
        "    # Validate mapping\n",
        "    if merged[\"Family\"].isna().any():\n",
        "        missing = merged[merged[\"Family\"].isna()][\"Method\"].unique()\n",
        "        logger.warning(f\"Methods missing family mapping: {missing}\")\n",
        "        merged[\"Family\"] = merged[\"Family\"].fillna(\"Other\")\n",
        "\n",
        "    return merged[[\"TargetDate\", \"Maturity\", \"Method\", \"Family\", \"Error\"]]\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 39, Step 3: Prepare Plot Data\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def format_error_dynamics_data(\n",
        "    df_errors: pd.DataFrame\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Formats error data for plotting.\n",
        "    Returns a dictionary mapping Maturity -> DataFrame(Index=Date, Columns=Method).\n",
        "\n",
        "    Args:\n",
        "        df_errors: DataFrame from Step 1.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: Plotting data per maturity.\n",
        "    \"\"\"\n",
        "    plot_data = {}\n",
        "\n",
        "    for maturity in df_errors[\"Maturity\"].unique():\n",
        "        subset = df_errors[df_errors[\"Maturity\"] == maturity]\n",
        "\n",
        "        # Pivot: Index=TargetDate, Columns=Method, Values=Error\n",
        "        pivot = subset.pivot(index=\"TargetDate\", columns=\"Method\", values=\"Error\")\n",
        "\n",
        "        plot_data[maturity] = pivot\n",
        "\n",
        "    return plot_data\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 39, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_error_dynamics_data(\n",
        "    df_combined: pd.DataFrame,\n",
        "    df_yields: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrator to prepare error dynamics data.\n",
        "\n",
        "    Args:\n",
        "        df_combined: Combined forecasts.\n",
        "        df_yields: Realized yields.\n",
        "        study_config: Config.\n",
        "\n",
        "    Returns:\n",
        "        Dict mapping Maturity -> Error Time Series DataFrame.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Error Dynamics Data Preparation\")\n",
        "\n",
        "    # 1. Compute Errors\n",
        "    df_errors = compute_and_categorize_errors(df_combined, df_yields, horizon=1)\n",
        "\n",
        "    # 2. Format\n",
        "    plot_data = format_error_dynamics_data(df_errors)\n",
        "\n",
        "    logger.info(\"Error Dynamics Data Prepared.\")\n",
        "    return plot_data\n"
      ],
      "metadata": {
        "id": "5ltHwtAXYemp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 40: Package all results into the final reproduction artifact bundle\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 40: Package Reproduction Artifacts\n",
        "# ==============================================================================\n",
        "\n",
        "def package_reproduction_artifacts(\n",
        "    main_results: Dict[str, Any],\n",
        "    benchmark_results: Optional[Dict[str, Any]] = None,\n",
        "    tic_results: Optional[Dict[str, Any]] = None,\n",
        "    global_results: Optional[Dict[str, Any]] = None,\n",
        "    shap_results: Optional[Dict[str, Any]] = None,\n",
        "    weight_dynamics: Optional[Dict[str, pd.DataFrame]] = None,\n",
        "    error_dynamics: Optional[Dict[str, pd.DataFrame]] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Packages all study results into a structured artifact bundle for reproduction.\n",
        "\n",
        "    This function aggregates outputs from all pipeline stages into a unified structure\n",
        "    suitable for serialization, reporting, and auditing. It ensures that all critical\n",
        "    tables and figure data required by the manuscript are present.\n",
        "\n",
        "    Structure:\n",
        "        - Tables:\n",
        "            - DNS_RMSFE\n",
        "            - FADNS_RMSFE_Tables (Dict by horizon)\n",
        "            - FADNS_Best_k\n",
        "            - RF_RMSFE_Summary\n",
        "            - Combination_RMSFE_H1\n",
        "            - Benchmark_Comparison\n",
        "            - TIC_Improvement\n",
        "            - Global_RMSFE\n",
        "            - Breakpoints\n",
        "        - Figures (Data):\n",
        "            - TIC_Heatmap_Data\n",
        "            - Global_SHAP_Data\n",
        "            - Error_Dynamics_Data\n",
        "            - Weight_Dynamics_Data\n",
        "        - Audit:\n",
        "            - Config_Hash\n",
        "            - Frozen_Config\n",
        "            - Metadata\n",
        "\n",
        "    Args:\n",
        "        main_results: Output from run_main_study_pipeline.\n",
        "        benchmark_results: Output from run_benchmark_rf_analysis.\n",
        "        tic_results: Output from run_tic_robustness_analysis.\n",
        "        global_results: Output from run_global_extension_analysis.\n",
        "        shap_results: Output from aggregate_shap_results (tuple: df_global, df_plot).\n",
        "        weight_dynamics: Output from prepare_weight_dynamics_data.\n",
        "        error_dynamics: Output from prepare_error_dynamics_data.\n",
        "\n",
        "    Returns:\n",
        "        Dict: The final artifact bundle.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 40: Packaging Reproduction Artifacts\")\n",
        "\n",
        "    tables = {}\n",
        "    figures = {}\n",
        "    audit = {}\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # 1. Compile Tables\n",
        "    # --------------------------------------------------------------------------\n",
        "\n",
        "    # Main Study Tables\n",
        "    if 'dns_rmsfe' in main_results:\n",
        "        tables['DNS_RMSFE'] = main_results['dns_rmsfe']\n",
        "    else:\n",
        "        logger.warning(\"DNS RMSFE table missing from main results.\")\n",
        "\n",
        "    if 'fadns_results' in main_results:\n",
        "        fadns_res = main_results['fadns_results']\n",
        "        if 'rmsfe_tables' in fadns_res:\n",
        "            tables['FADNS_RMSFE_Tables'] = fadns_res['rmsfe_tables']\n",
        "        if 'best_k_table' in fadns_res:\n",
        "            tables['FADNS_Best_k'] = fadns_res['best_k_table']\n",
        "\n",
        "    if 'rf_results' in main_results:\n",
        "        rf_res = main_results['rf_results']\n",
        "        if 'rmsfe_summary' in rf_res:\n",
        "            tables['RF_RMSFE_Summary'] = rf_res['rmsfe_summary']\n",
        "\n",
        "    if 'combination_results' in main_results:\n",
        "        combo_res = main_results['combination_results']\n",
        "        if 'rmsfe_table_h1' in combo_res:\n",
        "            tables['Combination_RMSFE_H1'] = combo_res['rmsfe_table_h1']\n",
        "\n",
        "    if 'breakpoints' in main_results:\n",
        "        bp_res = main_results['breakpoints']\n",
        "        if 'pelt_table' in bp_res:\n",
        "            tables['Breakpoints'] = bp_res['pelt_table']\n",
        "\n",
        "    # Robustness Tables\n",
        "    if benchmark_results and 'comparison_table' in benchmark_results:\n",
        "        tables['Benchmark_Comparison'] = benchmark_results['comparison_table']\n",
        "\n",
        "    if tic_results and 'delta_table' in tic_results:\n",
        "        tables['TIC_Improvement'] = tic_results['delta_table']\n",
        "\n",
        "    if global_results and 'rmsfe_table' in global_results:\n",
        "        tables['Global_RMSFE'] = global_results['rmsfe_table']\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # 2. Compile Figure Data\n",
        "    # --------------------------------------------------------------------------\n",
        "\n",
        "    # TIC Heatmap\n",
        "    if tic_results and 'heatmap_matrix' in tic_results:\n",
        "        figures['TIC_Heatmap_Data'] = tic_results['heatmap_matrix']\n",
        "\n",
        "    # Global SHAP\n",
        "    # shap_results is expected to be the tuple (df_global, df_plot) from Task 37\n",
        "    if shap_results:\n",
        "        # Check if it's a tuple or dict (depending on how it was passed)\n",
        "        # Task 37 returns a tuple.\n",
        "        if isinstance(shap_results, tuple) and len(shap_results) == 2:\n",
        "            figures['Global_SHAP_Data'] = shap_results[1] # df_plot\n",
        "        elif isinstance(shap_results, dict) and 'plot_data' in shap_results:\n",
        "             figures['Global_SHAP_Data'] = shap_results['plot_data']\n",
        "\n",
        "    # Weight Dynamics\n",
        "    if weight_dynamics:\n",
        "        figures['Weight_Dynamics_Data'] = weight_dynamics\n",
        "\n",
        "    # Error Dynamics\n",
        "    if error_dynamics:\n",
        "        figures['Error_Dynamics_Data'] = error_dynamics\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # 3. Compile Audit Info\n",
        "    # --------------------------------------------------------------------------\n",
        "\n",
        "    if 'frozen_config' in main_results:\n",
        "        audit['Frozen_Config'] = main_results['frozen_config']\n",
        "    if 'config_hash' in main_results:\n",
        "        audit['Config_Hash'] = main_results['config_hash']\n",
        "\n",
        "    # Construct Final Bundle\n",
        "    final_bundle = {\n",
        "        \"Tables\": tables,\n",
        "        \"Figures\": figures,\n",
        "        \"Audit\": audit\n",
        "    }\n",
        "\n",
        "    # Validation of Bundle Integrity\n",
        "    required_tables = [\n",
        "        'DNS_RMSFE', 'FADNS_Best_k', 'RF_RMSFE_Summary', 'Combination_RMSFE_H1', 'Breakpoints'\n",
        "    ]\n",
        "    missing_tables = [t for t in required_tables if t not in tables]\n",
        "\n",
        "    if missing_tables:\n",
        "        logger.error(f\"Artifact bundle incomplete. Missing tables: {missing_tables}\")\n",
        "    else:\n",
        "        logger.info(\"Artifact bundle contains all critical main study tables.\")\n",
        "\n",
        "    logger.info(\"Task 40 Completed Successfully. Artifact bundle created.\")\n",
        "    return final_bundle\n"
      ],
      "metadata": {
        "id": "mFQuqJLhbrru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator\n",
        "\n",
        "# ==============================================================================\n",
        "# Task: Top-Level Orchestrator (Variant)\n",
        "# ==============================================================================\n",
        "\n",
        "def run_main_study_pipeline_variant(\n",
        "    df_us_yields_raw: pd.DataFrame,\n",
        "    df_us_macro_raw: pd.DataFrame,\n",
        "    df_us_benchmark_yields_raw: pd.DataFrame,\n",
        "    df_us_tic_raw: pd.DataFrame,\n",
        "    df_global_yields_raw: pd.DataFrame,\n",
        "    global_macro_panels_raw: Dict[str, pd.DataFrame],\n",
        "    raw_study_config: Dict[str, Any],\n",
        "    study_metadata: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete end-to-end pipeline for the U.S. Treasury yield forecasting study,\n",
        "    including all robustness checks, extensions, and interpretability analyses.\n",
        "\n",
        "    This variant executes Tasks 1 through 40 sequentially, ensuring strict data dependency management\n",
        "    and fidelity to the replication protocol.\n",
        "\n",
        "    Sequence:\n",
        "        1.  Validation (Tasks 1-3)\n",
        "        2.  Data Cleansing & Alignment (Tasks 4-6)\n",
        "        3.  Structural Break Detection (Tasks 7-8)\n",
        "        4.  DNS Parametric Modeling (Tasks 9-13)\n",
        "        5.  FADNS Factor-Augmented Modeling (Tasks 14-20)\n",
        "        6.  Random Forest Modeling (Tasks 21-25)\n",
        "        7.  Forecast Combination (Tasks 26-28)\n",
        "        8.  Benchmark Robustness Analysis (Tasks 30-31)\n",
        "        9.  TIC Augmentation Analysis (Tasks 32-33)\n",
        "        10. Global Extension Analysis (Tasks 34-35)\n",
        "        11. SHAP Interpretability (Tasks 36-37)\n",
        "        12. Visualization Data Preparation (Tasks 38-39)\n",
        "        13. Artifact Packaging (Task 40)\n",
        "\n",
        "    Args:\n",
        "        df_us_yields_raw (pd.DataFrame): Raw U.S. zero-coupon yields.\n",
        "        df_us_macro_raw (pd.DataFrame): Raw U.S. macro predictors.\n",
        "        df_us_benchmark_yields_raw (pd.DataFrame): Raw U.S. benchmark yields.\n",
        "        df_us_tic_raw (pd.DataFrame): Raw U.S. TIC data.\n",
        "        df_global_yields_raw (pd.DataFrame): Raw Global 10Y yields.\n",
        "        global_macro_panels_raw (Dict[str, pd.DataFrame]): Dictionary of raw Global macro panels.\n",
        "        raw_study_config (Dict[str, Any]): Initial configuration dictionary.\n",
        "        study_metadata (Dict[str, Any]): Metadata dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: The final artifact bundle containing all tables, figures, and audit logs.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Comprehensive Study Pipeline (Variant)\")\n",
        "\n",
        "    try:\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 1: Validation and Configuration (Tasks 1-3)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 1: Validation and Configuration ---\")\n",
        "\n",
        "        # Task 1: Schema Validation\n",
        "        validate_all_input_schemas(\n",
        "            df_us_yields_raw, df_us_macro_raw, df_us_benchmark_yields_raw,\n",
        "            df_us_tic_raw, df_global_yields_raw, global_macro_panels_raw,\n",
        "            raw_study_config, study_metadata\n",
        "        )\n",
        "\n",
        "        # Task 2: Metadata Validation\n",
        "        us_macro_cols = df_us_macro_raw.columns.tolist()\n",
        "        validate_metadata_bundle(study_metadata, raw_study_config, us_macro_cols)\n",
        "\n",
        "        # Task 3: Config Resolution\n",
        "        frozen_config, config_hash = resolve_and_freeze_config(raw_study_config)\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 2: Data Cleansing and Alignment (Tasks 4-6)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 2: Data Cleansing and Alignment ---\")\n",
        "\n",
        "        # Task 4: Index Alignment\n",
        "        (df_us_yields_aligned, df_us_macro_aligned, df_benchmark_aligned,\n",
        "         df_tic_aligned, df_global_yields_aligned, global_macro_panels_aligned) = \\\n",
        "            cleanse_and_align_indices(\n",
        "                df_us_yields_raw, df_us_macro_raw, df_us_benchmark_yields_raw,\n",
        "                df_us_tic_raw, df_global_yields_raw, global_macro_panels_raw,\n",
        "                frozen_config\n",
        "            )\n",
        "\n",
        "        # Task 5: Yield Cleansing\n",
        "        df_us_yields_clean = cleanse_yield_panel(\n",
        "            df_us_yields_aligned, frozen_config, study_metadata\n",
        "        )\n",
        "\n",
        "        # Task 6: Macro Interpolation\n",
        "        df_us_macro_clean, global_macro_panels_clean = interpolate_macro_panels(\n",
        "            df_us_macro_aligned, global_macro_panels_aligned, study_metadata, frozen_config\n",
        "        )\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 3: Structural Break Detection (Tasks 7-8)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 3: Structural Break Detection ---\")\n",
        "\n",
        "        # Task 7: CUSUM\n",
        "        cusum_results = run_cusum_tests(df_us_yields_clean, frozen_config)\n",
        "\n",
        "        # Task 8: PELT\n",
        "        pelt_table, pelt_raw = run_pelt_detection(df_us_yields_clean, frozen_config)\n",
        "\n",
        "        breakpoints_artifacts = {\n",
        "            'cusum_table': cusum_results,\n",
        "            'pelt_table': pelt_table\n",
        "        }\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 4: DNS Parametric Modeling (Tasks 9-13)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 4: DNS Modeling ---\")\n",
        "\n",
        "        # Task 9: Loading Matrix\n",
        "        L, maturities = construct_dns_loading_matrix(frozen_config, study_metadata)\n",
        "\n",
        "        # Task 10: Factor Extraction\n",
        "        df_dns_factors = extract_dns_factors(df_us_yields_clean, L, maturities)\n",
        "\n",
        "        # Task 11: Rolling VAR\n",
        "        dns_var_params = estimate_rolling_dns_var(df_dns_factors, frozen_config)\n",
        "\n",
        "        # Task 12: Forecasting\n",
        "        df_dns_forecasts = generate_dns_forecasts(dns_var_params, df_dns_factors, L, frozen_config)\n",
        "\n",
        "        # Task 13: RMSFE\n",
        "        dns_rmsfe_table = compute_dns_rmsfe(df_dns_forecasts, df_us_yields_clean, frozen_config)\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 5: FADNS Factor-Augmented Modeling (Tasks 14-20)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 5: FADNS Modeling ---\")\n",
        "\n",
        "        # Task 14: Macro Preprocessing (Lagging)\n",
        "        macro_blocks = preprocess_fadns_macro(df_us_macro_clean, frozen_config, study_metadata)\n",
        "\n",
        "        # Task 15: Stationarity (ADF)\n",
        "        macro_blocks_diff = apply_rolling_adf_filtering(macro_blocks, frozen_config)\n",
        "\n",
        "        # Task 16: Standardization\n",
        "        macro_blocks_std = standardize_rolling_macro_blocks(macro_blocks_diff, frozen_config)\n",
        "\n",
        "        # Task 17: PCA Factors\n",
        "        df_macro_factors = construct_rolling_pca_factors(macro_blocks_std, frozen_config)\n",
        "\n",
        "        # Task 18: Estimation & Forecasting\n",
        "        df_fadns_beta_forecasts = run_fadns_estimation_and_forecast(\n",
        "            df_dns_factors, df_macro_factors, frozen_config\n",
        "        )\n",
        "\n",
        "        # Task 19: Yield Mapping & RMSFE\n",
        "        fadns_rmsfe_tables, df_fadns_yield_forecasts = compute_fadns_rmsfe(\n",
        "            df_fadns_beta_forecasts, df_us_yields_clean, L, frozen_config\n",
        "        )\n",
        "\n",
        "        # Task 20: Best-k Selection\n",
        "        df_best_k, df_best_rmsfe = select_best_fadns_k(fadns_rmsfe_tables, frozen_config)\n",
        "\n",
        "        fadns_artifacts = {\n",
        "            'rmsfe_tables': fadns_rmsfe_tables,\n",
        "            'best_k_table': df_best_k\n",
        "        }\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 6: Random Forest Modeling (Tasks 21-25)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 6: Random Forest Modeling ---\")\n",
        "\n",
        "        # Task 21: Feature Construction\n",
        "        df_rf_macro_feat, dict_rf_yield_feat = construct_rf_features(\n",
        "            df_us_macro_clean, df_us_yields_clean, frozen_config\n",
        "        )\n",
        "\n",
        "        # Task 24: Forecasting (includes Normalization Task 22 & Training Task 23)\n",
        "        df_rf_forecasts = generate_rf_forecasts(\n",
        "            df_rf_macro_feat, dict_rf_yield_feat, df_us_yields_clean, frozen_config\n",
        "        )\n",
        "\n",
        "        # Task 25: RMSFE Summary\n",
        "        rf_summary_table, rf_numeric_stats = compute_rf_rmsfe_summary(\n",
        "            df_rf_forecasts, df_us_yields_clean, frozen_config\n",
        "        )\n",
        "\n",
        "        rf_artifacts = {\n",
        "            'rmsfe_summary': rf_summary_table\n",
        "        }\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 7: Forecast Combination (Tasks 26-28)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 7: Forecast Combination ---\")\n",
        "\n",
        "        # Task 26: Pool Construction\n",
        "        df_pool, df_pool_errors = prepare_forecast_combination_data(\n",
        "            df_fadns_yield_forecasts, df_rf_forecasts, df_us_yields_clean, frozen_config\n",
        "        )\n",
        "\n",
        "        # Task 27: Weight Estimation\n",
        "        df_weights = compute_forecast_weights(df_pool_errors, frozen_config)\n",
        "\n",
        "        # Task 28: Combination Results\n",
        "        combo_rmsfe_table, df_combined_forecasts = compute_combination_results(\n",
        "            df_pool, df_weights, df_us_yields_clean, frozen_config\n",
        "        )\n",
        "\n",
        "        combination_artifacts = {\n",
        "            'rmsfe_table_h1': combo_rmsfe_table\n",
        "        }\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 8: Benchmark Robustness Analysis (Tasks 30-31)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 8: Benchmark Robustness Analysis ---\")\n",
        "\n",
        "        # Task 30 & 31: Benchmark Analysis\n",
        "        multi_table, single_table, diff_table = execute_benchmark_analysis(\n",
        "            df_benchmark_aligned, df_us_macro_clean, frozen_config\n",
        "        )\n",
        "\n",
        "        benchmark_results = {\n",
        "            'comparison_table': diff_table,\n",
        "            'multi_rmsfe': multi_table,\n",
        "            'single_rmsfe': single_table\n",
        "        }\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 9: TIC Augmentation Analysis (Tasks 32-33)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 9: TIC Augmentation Analysis ---\")\n",
        "\n",
        "        # Task 32 & 33: TIC Analysis\n",
        "        tic_results_dict = execute_tic_analysis(\n",
        "            df_benchmark_aligned, df_us_macro_clean, df_tic_aligned, frozen_config\n",
        "        )\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 10: Global Extension Analysis (Tasks 34-35)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 10: Global Extension Analysis ---\")\n",
        "\n",
        "        # Task 34 & 35: Global Analysis\n",
        "        global_results_dict = execute_global_rf_extension(\n",
        "            df_global_yields_aligned, global_macro_panels_clean, frozen_config\n",
        "        )\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 11: SHAP Interpretability (Tasks 36-37)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 11: SHAP Interpretability ---\")\n",
        "\n",
        "        # Task 36: SHAP Computation\n",
        "        shap_raw_results = compute_shap_analysis(\n",
        "            df_rf_macro_feat, dict_rf_yield_feat, df_us_yields_clean, frozen_config\n",
        "        )\n",
        "\n",
        "        # Task 37: SHAP Aggregation\n",
        "        df_global_shap, df_shap_plot = aggregate_shap_results(shap_raw_results)\n",
        "\n",
        "        shap_artifacts = {\n",
        "            'plot_data': df_shap_plot,\n",
        "            'global_shap_table': df_global_shap\n",
        "        }\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 12: Visualization Data Preparation (Tasks 38-39)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 12: Visualization Data Preparation ---\")\n",
        "\n",
        "        # Task 38: Weight Dynamics\n",
        "        weight_dynamics_data = prepare_weight_dynamics_data(df_weights, frozen_config)\n",
        "\n",
        "        # Task 39: Error Dynamics\n",
        "        error_dynamics_data = prepare_error_dynamics_data(df_combined_forecasts, df_us_yields_clean, frozen_config)\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 13: Artifact Packaging (Task 40)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\"--- Phase 13: Packaging ---\")\n",
        "\n",
        "        # Aggregate main results for packaging\n",
        "        main_results = {\n",
        "            'frozen_config': frozen_config,\n",
        "            'config_hash': config_hash,\n",
        "            'dns_rmsfe': dns_rmsfe_table,\n",
        "            'fadns_results': fadns_artifacts,\n",
        "            'rf_results': rf_artifacts,\n",
        "            'combination_results': combination_artifacts,\n",
        "            'breakpoints': breakpoints_artifacts\n",
        "        }\n",
        "\n",
        "        # Task 40: Packaging\n",
        "        final_bundle = package_reproduction_artifacts(\n",
        "            main_results=main_results,\n",
        "            benchmark_results=benchmark_results,\n",
        "            tic_results=tic_results_dict,\n",
        "            global_results=global_results_dict,\n",
        "            shap_results=shap_artifacts,\n",
        "            weight_dynamics=weight_dynamics_data,\n",
        "            error_dynamics=error_dynamics_data\n",
        "        )\n",
        "\n",
        "        logger.info(\"Comprehensive Study Pipeline Completed Successfully.\")\n",
        "        return final_bundle\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Pipeline Failed: {str(e)}\", exc_info=True)\n",
        "        raise\n"
      ],
      "metadata": {
        "id": "YWtcSNXhmcFC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}